{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accfc3ce",
   "metadata": {},
   "source": [
    "# Screening Task: Semantic NLP Filtering for Identifying Deep Learning Papers in Virology and Epidemiology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8962f2f",
   "metadata": {},
   "source": [
    "### Install Required Libraries\n",
    "\n",
    "In this step, we install essential libraries that will be used throughout our analysis:\n",
    "\n",
    "- **`transformers`**: Provides tools for using pre-trained transformer models, which are essential for embedding generation.\n",
    "- **`pandas`**: Allows for efficient data manipulation and analysis in DataFrame format.\n",
    "- **`scikit-learn`**: Includes tools for machine learning and similarity calculations.\n",
    "- **`torch`**: Powers the transformer models and provides deep learning functionalities.\n",
    "- **`tqdm`**: Enables progress bars to track the progress of various operations, making it easier to follow batch processing steps.\n",
    "\n",
    "```python\n",
    "# Install necessary libraries\n",
    "!pip install transformers pandas scikit-learn torch tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b3891c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /home/paritala/anaconda3/lib/python3.11/site-packages (4.46.1)\n",
      "Requirement already satisfied: pandas in /home/paritala/anaconda3/lib/python3.11/site-packages (2.0.3)\n",
      "Requirement already satisfied: scikit-learn in /home/paritala/anaconda3/lib/python3.11/site-packages (1.3.0)\n",
      "Requirement already satisfied: torch in /home/paritala/anaconda3/lib/python3.11/site-packages (2.5.1)\n",
      "Requirement already satisfied: tqdm in /home/paritala/anaconda3/lib/python3.11/site-packages (4.65.0)\n",
      "Requirement already satisfied: filelock in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /home/paritala/anaconda3/lib/python3.11/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/paritala/anaconda3/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.11.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (2024.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/paritala/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/paritala/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/paritala/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/paritala/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/paritala/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/paritala/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2023.7.22)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install transformers pandas scikit-learn torch tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e08203",
   "metadata": {},
   "source": [
    "### Data Loading and Initial Filtering\n",
    "\n",
    "In this step, we load the dataset and perform initial filtering to ensure data quality for further analysis. Hereâ€™s a breakdown of the process:\n",
    "\n",
    "1. **Load Dataset**: The dataset, which includes research paper titles and abstracts, is loaded into a DataFrame.\n",
    "\n",
    "2. **Identify Missing Values**:\n",
    "   - We check for rows where either the \"Title\" or \"Abstract\" column is missing.\n",
    "   - These records are saved separately in a file named `missing_title_or_abstract.csv` for reference.\n",
    "\n",
    "3. **Filter Non-Empty Abstracts**:\n",
    "   - We remove rows where the \"Abstract\" column is empty or contains only whitespace, as these entries are not suitable for semantic analysis.\n",
    "   - After filtering, we display the number of records remaining.\n",
    "\n",
    "4. **Confirm Data Quality**:\n",
    "   - We display summary information about the filtered DataFrame to confirm that only records with both a title and a non-empty abstract remain.\n",
    "   - Additionally, a preview of the cleaned DataFrame is provided to verify the data structure before moving to the next stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac1921d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records before filtering: 11450\n",
      "Records with missing Title or Abstract: 213\n",
      "Records with missing Title or Abstract saved as 'missing_title_or_abstract.csv'.\n",
      "Total records after filtering for non-empty Abstract: 11237\n",
      "\n",
      "Main DataFrame after filtering out rows with both Title and Abstract missing (for analysis):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 11237 entries, 1 to 11449\n",
      "Data columns (total 12 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   PMID              11237 non-null  int64 \n",
      " 1   Title             11237 non-null  object\n",
      " 2   Authors           11237 non-null  object\n",
      " 3   Citation          11237 non-null  object\n",
      " 4   First Author      11237 non-null  object\n",
      " 5   Journal/Book      11237 non-null  object\n",
      " 6   Publication Year  11237 non-null  int64 \n",
      " 7   Create Date       11237 non-null  object\n",
      " 8   PMCID             6387 non-null   object\n",
      " 9   NIHMS ID          946 non-null    object\n",
      " 10  DOI               10768 non-null  object\n",
      " 11  Abstract          11237 non-null  object\n",
      "dtypes: int64(2), object(10)\n",
      "memory usage: 1.1+ MB\n",
      "None\n",
      "\n",
      "Cleaned DataFrame preview for analysis:\n",
      "       PMID                                              Title  \\\n",
      "1  39398866  Characterization of arteriosclerosis based on ...   \n",
      "2  39390053  Multi-scale input layers and dense decoder agg...   \n",
      "3  39367648  An initial game-theoretic assessment of enhanc...   \n",
      "4  39363262  Truncated M13 phage for smart detection of E. ...   \n",
      "5  39287522  AI for Multistructure Incidental Findings and ...   \n",
      "\n",
      "                                             Authors  \\\n",
      "1  Zhou J, Li X, Demeke D, Dinh TA, Yang Y, Janow...   \n",
      "2                                      Lan X, Jin W.   \n",
      "3  Fatemi MY, Lu Y, Diallo AB, Srinivasan G, Azhe...   \n",
      "4  Yuan J, Zhu H, Li S, Thierry B, Yang CT, Zhang...   \n",
      "5  Marcinkiewicz AM, Buchwald M, Shanbhag A, Bedn...   \n",
      "\n",
      "                                            Citation      First Author  \\\n",
      "1  J Med Imaging (Bellingham). 2024 Sep;11(5):057...            Zhou J   \n",
      "2  Sci Rep. 2024 Oct 10;14(1):23729. doi: 10.1038...             Lan X   \n",
      "3  Brief Bioinform. 2024 Sep 23;25(6):bbae476. do...         Fatemi MY   \n",
      "4  J Nanobiotechnology. 2024 Oct 3;22(1):599. doi...            Yuan J   \n",
      "5  Radiology. 2024 Sep;312(3):e240541. doi: 10.11...  Marcinkiewicz AM   \n",
      "\n",
      "                 Journal/Book  Publication Year Create Date        PMCID  \\\n",
      "1  J Med Imaging (Bellingham)              2024  2024/10/14  PMC11466048   \n",
      "2                     Sci Rep              2024  2024/10/10  PMC11467340   \n",
      "3             Brief Bioinform              2024  2024/10/05  PMC11452536   \n",
      "4         J Nanobiotechnology              2024  2024/10/04  PMC11451008   \n",
      "5                   Radiology              2024  2024/09/17  PMC11427857   \n",
      "\n",
      "  NIHMS ID                         DOI  \\\n",
      "1      NaN   10.1117/1.JMI.11.5.057501   \n",
      "2      NaN  10.1038/s41598-024-74701-0   \n",
      "3      NaN         10.1093/bib/bbae476   \n",
      "4      NaN  10.1186/s12951-024-02881-y   \n",
      "5      NaN       10.1148/radiol.240541   \n",
      "\n",
      "                                            Abstract  \n",
      "1  PURPOSE: Our purpose is to develop a computer ...  \n",
      "2  Accurate segmentation of COVID-19 lesions from...  \n",
      "3  The application of deep learning to spatial tr...  \n",
      "4  BACKGROUND: The urgent need for affordable and...  \n",
      "5  Background Incidental extrapulmonary findings ...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file into a DataFrame\n",
    "df = pd.read_csv('collection_with_abstracts.csv')\n",
    "\n",
    "print(\"Total records before filtering:\", len(df))\n",
    "\n",
    "# Identify rows where either Title or Abstract (or both) are missing\n",
    "missing_either_df = df[df['Title'].isnull() | df['Abstract'].isnull()]\n",
    "print(\"Records with missing Title or Abstract:\", len(missing_either_df))\n",
    "\n",
    "# Save this DataFrame for tracking purposes\n",
    "missing_either_df.to_csv('missing_title_or_abstract.csv', index=False)\n",
    "print(\"Records with missing Title or Abstract saved as 'missing_title_or_abstract.csv'.\")\n",
    "\n",
    "# Remove rows where Abstract is NaN or an empty string\n",
    "df_for_analysis = df.dropna(subset=['Abstract'])  # First, remove rows where Abstract is NaN\n",
    "df_for_analysis = df_for_analysis[df_for_analysis['Abstract'].str.strip() != '']  # Then, remove rows where Abstract is an empty string\n",
    "\n",
    "print(\"Total records after filtering for non-empty Abstract:\", len(df_for_analysis))\n",
    "\n",
    "# Display info to confirm filtering\n",
    "print(\"\\nMain DataFrame after filtering out rows with both Title and Abstract missing (for analysis):\")\n",
    "print(df_for_analysis.info())\n",
    "\n",
    "# Optional: Display the first few rows of the cleaned DataFrame for analysis\n",
    "print(\"\\nCleaned DataFrame preview for analysis:\")\n",
    "print(df_for_analysis.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2188d56",
   "metadata": {},
   "source": [
    "### Generate Target Embeddings for Virology/Epidemiology and Deep Learning\n",
    "\n",
    "In this step, we create target embeddings based on phrases relevant to two primary focus areas in our analysis: **virology/epidemiology** and **deep learning**. These target embeddings will serve as reference points to identify papers that align with the topics of interest.\n",
    "\n",
    "1. **Define Target Phrases**:\n",
    "   - A list of phrases is defined for each focus area:\n",
    "     - **Virology/Epidemiology Phrases**: These phrases capture specific keywords related to infectious diseases, viral interactions, public health surveillance, and similar topics.\n",
    "     - **Deep Learning Phrases**: These phrases cover keywords relevant to neural networks, machine learning models, and advanced computational methods used in virology and epidemiology.\n",
    "\n",
    "\n",
    "2. **Load Model and Tokenizer**:\n",
    "   - We load a pretrained transformer model and its tokenizer, which are used to generate embeddings. This model will provide the vector representation for each phrase.\n",
    "\n",
    "\n",
    "3. **Embedding Generation Function**:\n",
    "   - A function, `get_embedding`, is defined to generate an embedding for each phrase. This function:\n",
    "     - Tokenizes the input text and applies truncation and padding.\n",
    "     - Uses the transformer model to create an embedding from the mean of the last hidden states, ensuring each phrase is represented by a dense vector.\n",
    "\n",
    "\n",
    "4. **Calculate Average Embeddings**:\n",
    "   - **Virology/Epidemiology Embedding**: Each phrase in this category is converted to an embedding, and the average of these embeddings is calculated to create a unified target embedding.\n",
    "   - **Deep Learning Embedding**: Similarly, the average embedding for all phrases related to deep learning is calculated.\n",
    "\n",
    "\n",
    "5. **Display Results**:\n",
    "   - Once the embeddings are generated, we print a confirmation message for each focus area, indicating that the average embeddings for virology/epidemiology and deep learning are ready for use in the analysis.\n",
    "\n",
    "These embeddings will enable us to assess the relevance of each research paper by comparing its embedding to these target embeddings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e347e5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Virology/Epidemiology Embedding generated.\n",
      "Average Deep Learning Embedding generated.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Define phrases for each target area\n",
    "virology_epidemiology_relevance_phrases = [\n",
    "    \"deep learning in virology and epidemiology\",\n",
    "    \"neural network applications in infectious diseases\",\n",
    "    \"AI models for viral infection analysis\",\n",
    "    \"machine learning in public health research\",\n",
    "    \"deep learning solutions for disease spread analysis\",\n",
    "    \"infectious diseases\", \"virus detection\", \"disease spread modeling\",\n",
    "    \"viral disease outbreaks\", \"virus genomics and sequencing\", \n",
    "    \"viral infection mechanisms\", \"antiviral drug resistance\", \n",
    "    \"virus-host interactions\", \"epidemic response to viral infections\", \n",
    "    \"pandemic virology research\", \"epidemiological models\", \n",
    "    \"public health surveillance\", \"disease transmission patterns\", \n",
    "    \"infection rate prediction\", \"epidemic spread modeling\", \n",
    "    \"population-level health impact\", \"disease incidence and prevalence\", \n",
    "    \"contact tracing in disease outbreaks\", \"infectious disease modeling\", \n",
    "    \"pathogen tracking and monitoring\", \"viral pathogen analysis\", \n",
    "    \"health risk assessment for infectious diseases\", \n",
    "    \"genomic epidemiology of viruses\", \"disease outbreak prediction\"\n",
    "]\n",
    "\n",
    "deep_learning_phrases = [\n",
    "    \"neural network\", \"artificial neural network\", \"machine learning model\", \n",
    "    \"feedforward neural network\", \"neural net algorithm\", \"multilayer perceptron\", \n",
    "    \"convolutional neural network\", \"recurrent neural network\", \"long short-term memory network\", \n",
    "    \"CNN\", \"GRNN\", \"RNN\", \"LSTM\", \"deep learning\", \"deep neural networks\", \n",
    "    \"computer vision\", \"vision model\", \"image processing\", \"vision algorithms\", \n",
    "    \"computer graphics and vision\", \"object recognition\", \"scene understanding\", \n",
    "    \"natural language processing\", \"text mining\", \"NLP\", \"computational linguistics\", \n",
    "    \"language processing\", \"text analytics\", \"textual data analysis\", \"text data analysis\", \n",
    "    \"text analysis\", \"speech and language technology\", \"language modeling\", \n",
    "    \"computational semantics\", \"generative artificial intelligence\", \"generative AI\", \n",
    "    \"generative deep learning\", \"generative models\", \"transformer models\", \n",
    "    \"self-attention models\", \"transformer architecture\", \"transformer\", \n",
    "    \"attention-based neural networks\", \"transformer networks\", \"sequence-to-sequence models\", \n",
    "    \"large language model\", \"LLM\", \"transformer-based model\", \"pretrained language model\", \n",
    "    \"generative language model\", \"foundation model\", \"state-of-the-art language model\", \n",
    "    \"multimodal model\", \"multimodal neural network\", \"vision transformer\", \n",
    "    \"diffusion model\", \"generative diffusion model\", \"diffusion-based generative model\", \n",
    "    \"continuous diffusion model\"\n",
    "]\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = 'HuggingFaceTB/SmolLM2-1.7B'  # Replace with preferred model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Set padding token for the tokenizer\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Function to generate embeddings for a given text\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    # Use the mean of the last hidden state as the embedding\n",
    "    return outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "\n",
    "# Generate embeddings for virology/epidemiology relevance phrases and calculate the average embedding\n",
    "virology_embeddings = [get_embedding(phrase) for phrase in virology_epidemiology_relevance_phrases]\n",
    "average_virology_embedding = np.mean(virology_embeddings, axis=0)\n",
    "\n",
    "# Generate embeddings for deep learning phrases and calculate the average embedding\n",
    "deep_learning_embeddings = [get_embedding(phrase) for phrase in deep_learning_phrases]\n",
    "average_deep_learning_embedding = np.mean(deep_learning_embeddings, axis=0)\n",
    "\n",
    "# Display the results\n",
    "print(\"Average Virology/Epidemiology Embedding generated.\")\n",
    "print(\"Average Deep Learning Embedding generated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea3c65",
   "metadata": {},
   "source": [
    "### Step 4: Generate Embeddings for Each Paper and Calculate Similarity with Target Embeddings\n",
    "\n",
    "In this step, we prepare the dataset by generating embeddings for each paper in the analysis dataset. These embeddings are crucial as they allow us to measure the semantic similarity between the content of each paper and the predefined target embeddings.\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "1. **Concatenate Title and Abstract**:\n",
    "   - We combine the `Title` and `Abstract` columns for each paper to create a comprehensive text representation (`Title_Abstract`). This combined text will serve as the input for embedding generation, ensuring we capture the full context of each paper.\n",
    "\n",
    "2. **Generate Embeddings**:\n",
    "   - For each paperâ€™s combined title and abstract, we generate an embedding using the `get_embedding` function.\n",
    "   - To track progress and manage potentially large datasets, we use `tqdm` to provide a progress bar, helping us monitor the embedding generation process.\n",
    "\n",
    "3. **Progress Tracking**:\n",
    "   - The `tqdm.pandas()` function is applied to `pandas`' `apply` method, allowing us to view progress updates as embeddings are generated for each paper.\n",
    "\n",
    "The result is a new column, `Paper_Embedding`, in the `df_for_analysis` DataFrame. This column contains the embeddings for each paper, which we will use in the next steps to calculate similarity to the target embeddings for **virology/epidemiology** and **deep learning** relevance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f2675fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11237/11237 [13:46:57<00:00,  4.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Enable tqdm for pandas apply\n",
    "tqdm.pandas()\n",
    "\n",
    "# Combine Title and Abstract text for each paper\n",
    "df_for_analysis['Title_Abstract'] = df_for_analysis['Title'].fillna('') + \" \" + df_for_analysis['Abstract'].fillna('')\n",
    "\n",
    "# Generate embeddings for each paper's combined Title and Abstract with progress tracking\n",
    "df_for_analysis['Paper_Embedding'] = df_for_analysis['Title_Abstract'].progress_apply(get_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d62994",
   "metadata": {},
   "source": [
    "### Step 4.1: Calculate Similarity to Virology/Epidemiology Embedding and Apply Threshold Filtering\n",
    "\n",
    "In this step, we calculate the similarity between each paper's embedding and the **average embedding for virology/epidemiology relevance**. Using a defined threshold, we then filter papers that are most relevant to our focus on virology and epidemiology.\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "1. **Calculate Similarity with Virology/Epidemiology Embedding**:\n",
    "   - We use the `cosine_similarity` function to compute the similarity between each paper's embedding (`Paper_Embedding`) and the `average_virology_embedding`.\n",
    "   - The resulting similarity score is stored in a new column, `Virology_Similarity`, in `df_for_analysis`.\n",
    "\n",
    "2. **Set and Apply Threshold for Filtering**:\n",
    "   - We define a similarity threshold (`virology_similarity_threshold = 0.90`), which serves as the cutoff to determine if a paper is relevant to virology and epidemiology.\n",
    "   - Papers with a similarity score above or equal to this threshold are considered relevant and are filtered into a new DataFrame, `virology_filtered_df`.\n",
    "   - This threshold value is critical for ensuring that we retain only those papers that closely match the virology/epidemiology criteria.\n",
    "\n",
    "3. **Preview and Save Filtered Results**:\n",
    "   - A preview of the filtered data is displayed to validate the filtering.\n",
    "   - Finally, the filtered DataFrame `virology_filtered_df` is saved to a CSV file, `virology_relevant_papers.csv`, which contains only the virology-relevant papers for further analysis.\n",
    "\n",
    "The similarity threshold ensures we focus on papers that meet a high degree of relevance to virology and epidemiology, making this step key in narrowing down the dataset to our specific area of interest.\n",
    "\n",
    "**Additionally, based on specific requirements or the results observed, this threshold can be adjusted to either broaden or restrict the range of papers filtered, allowing for flexibility in refining the selection to match analysis needs.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "49b89636",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11237/11237 [00:01<00:00, 5899.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virology/Epidemiology similarity scores calculated. Sample data:\n",
      "       PMID                                              Title  \\\n",
      "1  39398866  Characterization of arteriosclerosis based on ...   \n",
      "2  39390053  Multi-scale input layers and dense decoder agg...   \n",
      "3  39367648  An initial game-theoretic assessment of enhanc...   \n",
      "4  39363262  Truncated M13 phage for smart detection of E. ...   \n",
      "5  39287522  AI for Multistructure Incidental Findings and ...   \n",
      "\n",
      "                                            Abstract  Virology_Similarity  \n",
      "1  PURPOSE: Our purpose is to develop a computer ...             0.844549  \n",
      "2  Accurate segmentation of COVID-19 lesions from...             0.858944  \n",
      "3  The application of deep learning to spatial tr...             0.878694  \n",
      "4  BACKGROUND: The urgent need for affordable and...             0.857912  \n",
      "5  Background Incidental extrapulmonary findings ...             0.870465  \n",
      "Papers with Virology/Epidemiology Similarity >= 0.9:\n",
      "        PMID                                              Title  \\\n",
      "9   39269702  Health Warnings on Instagram Advertisements fo...   \n",
      "30  39013794  Deep Learning - Methods to Amplify Epidemiolog...   \n",
      "45  38885231  Classification of white blood cells (leucocyte...   \n",
      "52  38829905  Automatic mapping of high-risk urban areas for...   \n",
      "55  38800990  Prediction of prognosis using artificial intel...   \n",
      "\n",
      "                                             Abstract  Virology_Similarity  \n",
      "9   IMPORTANCE: Synthetic nicotine is increasingly...             0.912689  \n",
      "30  Deep learning is a subfield of artificial inte...             0.914811  \n",
      "45  Machine learning (ML) and deep learning (DL) m...             0.912759  \n",
      "52  BACKGROUND: Dengue, Zika, and chikungunya, who...             0.903492  \n",
      "55  BACKGROUND: Prompt histopathological diagnosis...             0.908390  \n",
      "Total virology-relevant papers after thresholding: 2009\n",
      "Filtered virology-relevant papers saved as 'virology_relevant_papers.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Step 4.1: Calculate similarity with virology/epidemiology embedding\n",
    "df_for_analysis['Virology_Similarity'] = df_for_analysis['Paper_Embedding'].progress_apply(\n",
    "    lambda emb: cosine_similarity([emb], [average_virology_embedding])[0][0]\n",
    ")\n",
    "\n",
    "# Display sample of the virology/epidemiology similarity scores\n",
    "print(\"Virology/Epidemiology similarity scores calculated. Sample data:\")\n",
    "print(df_for_analysis[['PMID', 'Title', 'Abstract', 'Virology_Similarity']].head())\n",
    "\n",
    "# Define similarity threshold for Virology/Epidemiology relevance\n",
    "virology_similarity_threshold = 0.90\n",
    "\n",
    "# Filter the DataFrame based on Virology/Epidemiology similarity threshold\n",
    "virology_filtered_df = df_for_analysis[df_for_analysis['Virology_Similarity'] >= virology_similarity_threshold]\n",
    "\n",
    "# Display filtered results for virology\n",
    "print(f\"Papers with Virology/Epidemiology Similarity >= {virology_similarity_threshold}:\")\n",
    "print(virology_filtered_df[['PMID', 'Title', 'Abstract', 'Virology_Similarity']].head())\n",
    "print(\"Total virology-relevant papers after thresholding:\", len(virology_filtered_df))\n",
    "\n",
    "# Save the filtered DataFrame to a CSV file\n",
    "virology_filtered_df.to_csv('virology_relevant_papers.csv', index=False)\n",
    "\n",
    "print(\"Filtered virology-relevant papers saved as 'virology_relevant_papers.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c34f95",
   "metadata": {},
   "source": [
    "## Download Output File\n",
    "\n",
    "The following links allow you to download the filtered and processed datasets directly from the notebook:\n",
    "\n",
    "  - Contains papers filtered based on their relevance to virology and epidemiology.\n",
    "\n",
    "You can click on link to download the respective CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f009e6de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='virology_relevant_papers.csv' target='_blank'>virology_relevant_papers.csv</a><br>"
      ],
      "text/plain": [
       "/home/paritala/Desktop/NLP task/virology_relevant_papers.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Display download link for the final CSV\n",
    "display(FileLink('virology_relevant_papers.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df973e6f",
   "metadata": {},
   "source": [
    "### Step 4.2: Calculate Similarity with Deep Learning Embedding and Apply Threshold Filtering\n",
    "\n",
    "In this step, we extend our filtering process to identify papers that meet both the virology/epidemiology and deep learning relevance criteria. By calculating the similarity of each paperâ€™s embedding to a deep learning embedding, we can further refine the dataset to include papers that intersect with deep learning applications in virology and epidemiology.\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "1. **Calculate Similarity with Deep Learning Embedding**:\n",
    "   - We use the `cosine_similarity` function to compute the similarity between each paperâ€™s embedding (`Paper_Embedding`) and the `average_deep_learning_embedding`.\n",
    "   - The resulting similarity scores are stored in a new column, `Deep_Learning_Similarity`, within the `virology_filtered_df` DataFrame, which contains papers previously filtered for virology relevance.\n",
    "\n",
    "2. **Set and Apply Threshold for Filtering**:\n",
    "   - We define a threshold (`deep_learning_similarity_threshold = 0.80`) to assess deep learning relevance.\n",
    "   - Papers with a similarity score equal to or above this threshold are retained as deep learning-relevant and filtered into a new DataFrame, `final_filtered_df`.\n",
    "   - This threshold value helps ensure that the filtered dataset contains papers with significant overlap in both deep learning and virology/epidemiology themes.\n",
    "\n",
    "3. **Preview and Save Final Filtered Results**:\n",
    "   - A preview of the final filtered data is displayed to confirm that only papers meeting both criteria are included.\n",
    "   - The final DataFrame, `final_filtered_df`, is saved as `deep_learning_virology_relevant_papers.csv` for future reference and detailed analysis.\n",
    "\n",
    "The two-stage threshold filtering approach allows us to hone in on papers that not only focus on virology/epidemiology but also incorporate substantial deep learning methodologies.\n",
    "\n",
    "**As with the virology threshold, the deep learning similarity threshold can be adjusted to tailor the selection. Increasing or lowering the threshold can help further refine the dataset based on the specific analysis needs and project objectives.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9720ef7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2009/2009 [00:00<00:00, 6038.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papers with Deep Learning Similarity >= 0.8:\n",
      "         PMID                                              Title  \\\n",
      "30   39013794  Deep Learning - Methods to Amplify Epidemiolog...   \n",
      "127  38006509  Quantitation of Oncologic Image Features for R...   \n",
      "228  36189512  Visual ergonomics for changing work environmen...   \n",
      "402  34253822  Deep learning for COVID-19 detection based on ...   \n",
      "462  35782182  Computer Audition for Fighting the SARS-CoV-2 ...   \n",
      "\n",
      "                                              Abstract  \\\n",
      "30   Deep learning is a subfield of artificial inte...   \n",
      "127  Radiomics is an emerging and exciting field of...   \n",
      "228  BACKGROUND: The coronavirus 2019 (COVID-19) pa...   \n",
      "402  COVID-19 has tremendously impacted patients an...   \n",
      "462  Computer audition (CA) has experienced a fast ...   \n",
      "\n",
      "     Deep_Learning_Similarity  \n",
      "30                   0.816207  \n",
      "127                  0.801202  \n",
      "228                  0.810959  \n",
      "402                  0.803380  \n",
      "462                  0.806091  \n",
      "Total deep learning-relevant papers after thresholding: 154\n",
      "Final filtered deep learning and virology-relevant papers saved as 'deep_learning_virology_relevant_papers.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/tmp/ipykernel_24044/3222731302.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  virology_filtered_df['Deep_Learning_Similarity'] = virology_filtered_df['Paper_Embedding'].progress_apply(\n"
     ]
    }
   ],
   "source": [
    "# Step 4.2: Calculate similarity with deep learning embedding\n",
    "virology_filtered_df['Deep_Learning_Similarity'] = virology_filtered_df['Paper_Embedding'].progress_apply(\n",
    "    lambda emb: cosine_similarity([emb], [average_deep_learning_embedding])[0][0]\n",
    ")\n",
    "\n",
    "# Define deep learning similarity threshold\n",
    "deep_learning_similarity_threshold = 0.80\n",
    "\n",
    "# Filter DataFrame based on Deep Learning similarity threshold\n",
    "final_filtered_df = virology_filtered_df[virology_filtered_df['Deep_Learning_Similarity'] >= deep_learning_similarity_threshold]\n",
    "\n",
    "# Display filtered results for deep learning\n",
    "print(f\"Papers with Deep Learning Similarity >= {deep_learning_similarity_threshold}:\")\n",
    "print(final_filtered_df[['PMID', 'Title', 'Abstract', 'Deep_Learning_Similarity']].head())\n",
    "print(\"Total deep learning-relevant papers after thresholding:\", len(final_filtered_df))\n",
    "\n",
    "# Save the final filtered DataFrame to a CSV file\n",
    "final_filtered_df.to_csv('deep_learning_virology_relevant_papers.csv', index=False)\n",
    "\n",
    "print(\"Final filtered deep learning and virology-relevant papers saved as 'deep_learning_virology_relevant_papers.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dd094",
   "metadata": {},
   "source": [
    "## Download Output File\n",
    "\n",
    "The following links allow you to download the filtered and processed datasets directly from the notebook:\n",
    "\n",
    "  - Contains papers relevant to both virology/epidemiology and deep learning criteria.\n",
    "\n",
    "You can click on link to download the respective CSV file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0332591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='deep_learning_virology_relevant_papers.csv' target='_blank'>deep_learning_virology_relevant_papers.csv</a><br>"
      ],
      "text/plain": [
       "/home/paritala/Desktop/NLP task/deep_learning_virology_relevant_papers.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display download link for the final CSV\n",
    "display(FileLink('deep_learning_virology_relevant_papers.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8733fffe",
   "metadata": {},
   "source": [
    "### Step 5: Classify Method Type Based on Semantic Similarity for Text Mining and Computer Vision\n",
    "\n",
    "In this step, we further refine our analysis by categorizing each paper into core method typesâ€”**Text Mining** and **Computer Vision**â€”based on the semantic similarity of the paperâ€™s content. This classification allows us to identify which deep learning techniques are being used in virology and epidemiology research, facilitating a targeted review of relevant methodologies.\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "1. **Define Target Phrases for Core Method Types**:\n",
    "   - We define target phrases that represent the key method types of interest:\n",
    "     - **Text Mining**: Captures natural language processing, text mining, and language model applications.\n",
    "     - **Computer Vision**: Focuses on medical image processing, computer vision, and visual data analysis related to infectious disease research.\n",
    "\n",
    "2. **Generate Embeddings for Each Method Type**:\n",
    "   - Using the `get_embedding` function, we create embeddings for each target phrase, representing the core method types. These embeddings serve as a basis for comparing each paperâ€™s content.\n",
    "\n",
    "3. **Classify Papers by Method Type**:\n",
    "   - We define a `classify_method_semantically` function that computes the similarity of each paperâ€™s embedding to both the **Text Mining** and **Computer Vision** embeddings using `cosine_similarity`.\n",
    "   - A similarity threshold (`threshold = 0.90`) is used to determine if a paper falls under a specific method type:\n",
    "     - If a paper meets the threshold for both method types, it is classified as **Both**.\n",
    "     - If only one threshold is met, the paper is classified as **Text Mining** or **Computer Vision** accordingly.\n",
    "     - Papers that do not meet the threshold for either method type are classified as **Other**.\n",
    "\n",
    "4. **Apply Classification Function**:\n",
    "   - The classification function is applied to each paper in the `final_filtered_df`, resulting in a new column, `Method_Type`, which indicates the method category for each paper.\n",
    "\n",
    "5. **Preview and Save Final Data with Method Classification**:\n",
    "   - A preview of the classified data is displayed to confirm accuracy.\n",
    "   - The final DataFrame, `final_filtered_df`, containing the method classifications, is saved as `deep_learning_virology_method_classification.csv` for future reference.\n",
    "\n",
    "By classifying each paper based on the method type, this step enriches our analysis, allowing for a more organized view of deep learning applications in virology and epidemiology. The threshold value can be adjusted if necessary, depending on initial results and specific analysis requirements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "848dff04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "Semantic method type classification completed with two main phrases.\n",
      "         PMID                                              Title  \\\n",
      "30   39013794  Deep Learning - Methods to Amplify Epidemiolog...   \n",
      "127  38006509  Quantitation of Oncologic Image Features for R...   \n",
      "228  36189512  Visual ergonomics for changing work environmen...   \n",
      "402  34253822  Deep learning for COVID-19 detection based on ...   \n",
      "462  35782182  Computer Audition for Fighting the SARS-CoV-2 ...   \n",
      "\n",
      "                                              Abstract      Method_Type  \n",
      "30   Deep learning is a subfield of artificial inte...             Both  \n",
      "127  Radiomics is an emerging and exciting field of...  Computer Vision  \n",
      "228  BACKGROUND: The coronavirus 2019 (COVID-19) pa...            Other  \n",
      "402  COVID-19 has tremendously impacted patients an...  Computer Vision  \n",
      "462  Computer audition (CA) has experienced a fast ...             Both  \n",
      "Final DataFrame with method classification saved as 'deep_learning_virology_method_classification.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24044/542493873.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_filtered_df['Method_Type'] = final_filtered_df['Paper_Embedding'].apply(classify_method_semantically)\n"
     ]
    }
   ],
   "source": [
    "# Define target phrases for the two core method types\n",
    "method_type_phrases = {\n",
    "    \"Text Mining\": \"natural language processing, text mining, and language model applications in virology and epidemiology research\",\n",
    "    \"Computer Vision\": \"medical image processing, computer vision, and visual data analysis in infectious disease research\"\n",
    "}\n",
    "\n",
    "# Generate embeddings for each method type\n",
    "method_type_embeddings = {method: get_embedding(phrase) for method, phrase in method_type_phrases.items()}\n",
    "\n",
    "# Function to classify each paper based on semantic similarity\n",
    "def classify_method_semantically(paper_embedding):\n",
    "    # Calculate similarity to each core method type\n",
    "    similarities = {method: cosine_similarity([paper_embedding], [emb])[0][0] for method, emb in method_type_embeddings.items()}\n",
    "    text_mining_sim = similarities[\"Text Mining\"]\n",
    "    computer_vision_sim = similarities[\"Computer Vision\"]\n",
    "    \n",
    "    # Define threshold\n",
    "    threshold = 0.90  # Adjust based on initial results\n",
    "\n",
    "    # Classify based on similarity scores\n",
    "    if text_mining_sim >= threshold and computer_vision_sim >= threshold:\n",
    "        return \"Both\"\n",
    "    elif text_mining_sim >= threshold:\n",
    "        return \"Text Mining\"\n",
    "    elif computer_vision_sim >= threshold:\n",
    "        return \"Computer Vision\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "# Apply the classification function to each paper in the final_filtered_df\n",
    "final_filtered_df['Method_Type'] = final_filtered_df['Paper_Embedding'].apply(classify_method_semantically)\n",
    "\n",
    "print(len(final_filtered_df))\n",
    "\n",
    "print(\"Semantic method type classification completed with two main phrases.\")\n",
    "print(final_filtered_df[['PMID', 'Title', 'Abstract', 'Method_Type']].head())\n",
    "\n",
    "# Save the final filtered DataFrame with Method_Type to a CSV file\n",
    "final_filtered_df.to_csv('deep_learning_virology_method_classification.csv', index=False)\n",
    "\n",
    "print(\"Final DataFrame with method classification saved as 'deep_learning_virology_method_classification.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a1321d",
   "metadata": {},
   "source": [
    "## Download Output File\n",
    "\n",
    "The following links allow you to download the filtered and processed datasets directly from the notebook:\n",
    "\n",
    "  - Includes papers classified by method type, indicating their focus on Text Mining, Computer Vision, or both.\n",
    "\n",
    "You can click on link to download the respective CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9ec226fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='deep_learning_virology_method_classification.csv' target='_blank'>deep_learning_virology_method_classification.csv</a><br>"
      ],
      "text/plain": [
       "/home/paritala/Desktop/NLP task/deep_learning_virology_method_classification.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "# Display download link for the final CSV\n",
    "display(FileLink('deep_learning_virology_method_classification.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07975cc5",
   "metadata": {},
   "source": [
    "### Step 6: Extract Specific Deep Learning Methods from Titles and Abstracts\n",
    "\n",
    "In this step, we extract specific deep learning methods mentioned in each paper based on a comprehensive list of relevant keywords. This helps to provide a detailed overview of the deep learning techniques applied in each paper, facilitating further analysis.\n",
    "\n",
    "#### Step-by-Step Explanation:\n",
    "\n",
    "1. **Define List of Target Keywords**:\n",
    "   - We compile a list of keywords representing various deep learning and machine learning methods, architectures, and models (e.g., \"neural network,\" \"transformer,\" \"convolutional neural network\").\n",
    "   - These keywords cover a broad range of techniques and terms related to both text and image analysis methods.\n",
    "\n",
    "2. **Extract Methods by Matching Keywords in Title and Abstract**:\n",
    "   - The function `extract_methods_from_text` is defined to check each paperâ€™s combined `Title_Abstract` field.\n",
    "   - For each entry, it searches for occurrences of the keywords in the title and abstract. If any keywords are found, they are recorded as the methods used in that paper. If no relevant keywords are detected, the method is labeled as **Not Specified**.\n",
    "\n",
    "3. **Apply Extraction Function and Save Results**:\n",
    "   - The extraction function is applied to each row in the `final_filtered_df`, creating a new column, `Methods_Used`, which lists the identified methods.\n",
    "   - A summary of entries with \"Not Specified\" in `Methods_Used` is also displayed to track papers where no relevant method keywords were found.\n",
    "\n",
    "4. **Save the Final Data with Extracted Methods**:\n",
    "   - The final DataFrame, including the extracted methods for each paper, is saved as `deep_learning_virology_methods_extracted.csv`. This output serves as the culmination of our analysis, providing a comprehensive view of the specific deep learning methods applied in the virology and epidemiology context.\n",
    "\n",
    "By capturing specific methods mentioned in the papers, this step enriches the dataset, making it easier to analyze the prevalence and application of various techniques within the field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1784922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154\n",
      "length\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 154 entries, 30 to 11283\n",
      "Data columns (total 18 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   PMID                      154 non-null    int64  \n",
      " 1   Title                     154 non-null    object \n",
      " 2   Authors                   154 non-null    object \n",
      " 3   Citation                  154 non-null    object \n",
      " 4   First Author              154 non-null    object \n",
      " 5   Journal/Book              154 non-null    object \n",
      " 6   Publication Year          154 non-null    int64  \n",
      " 7   Create Date               154 non-null    object \n",
      " 8   PMCID                     113 non-null    object \n",
      " 9   NIHMS ID                  6 non-null      object \n",
      " 10  DOI                       149 non-null    object \n",
      " 11  Abstract                  154 non-null    object \n",
      " 12  Title_Abstract            154 non-null    object \n",
      " 13  Paper_Embedding           154 non-null    object \n",
      " 14  Virology_Similarity       154 non-null    float32\n",
      " 15  Deep_Learning_Similarity  154 non-null    float32\n",
      " 16  Method_Type               154 non-null    object \n",
      " 17  Methods_Used              154 non-null    object \n",
      "dtypes: float32(2), int64(2), object(14)\n",
      "memory usage: 21.7+ KB\n",
      "None\n",
      "Method extraction based on keywords completed.\n",
      "         PMID                                              Title  \\\n",
      "30   39013794  Deep Learning - Methods to Amplify Epidemiolog...   \n",
      "127  38006509  Quantitation of Oncologic Image Features for R...   \n",
      "228  36189512  Visual ergonomics for changing work environmen...   \n",
      "402  34253822  Deep learning for COVID-19 detection based on ...   \n",
      "462  35782182  Computer Audition for Fighting the SARS-CoV-2 ...   \n",
      "\n",
      "                                              Abstract  \\\n",
      "30   Deep learning is a subfield of artificial inte...   \n",
      "127  Radiomics is an emerging and exciting field of...   \n",
      "228  BACKGROUND: The coronavirus 2019 (COVID-19) pa...   \n",
      "402  COVID-19 has tremendously impacted patients an...   \n",
      "462  Computer audition (CA) has experienced a fast ...   \n",
      "\n",
      "                                          Methods_Used  \n",
      "30                       neural network, deep learning  \n",
      "127                                      Not Specified  \n",
      "228                                    computer vision  \n",
      "402  neural network, convolutional neural network, ...  \n",
      "462                     deep learning, computer vision  \n",
      "Number of 'Not Specified' entries in Methods_Used: 15\n",
      "Final DataFrame with extracted methods saved as 'deep_learning_virology_methods_extracted.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of specific methods based on provided query phrases\n",
    "method_keywords = [\n",
    "    \"neural network\", \"artificial neural network\", \"machine learning model\", \n",
    "    \"feedforward neural network\", \"neural net algorithm\", \"multilayer perceptron\", \n",
    "    \"convolutional neural network\", \"recurrent neural network\", \"long short-term memory network\", \n",
    "    \"CNN\", \"GRNN\", \"RNN\", \"LSTM\", \"deep learning\", \"deep neural networks\", \n",
    "    \"computer vision\", \"vision model\", \"image processing\", \"vision algorithms\", \n",
    "    \"computer graphics and vision\", \"object recognition\", \"scene understanding\", \n",
    "    \"natural language processing\", \"text mining\", \"NLP\", \"computational linguistics\", \n",
    "    \"language processing\", \"text analytics\", \"textual data analysis\", \"text analysis\", \n",
    "    \"speech and language technology\", \"language modeling\", \"computational semantics\", \n",
    "    \"generative artificial intelligence\", \"generative AI\", \"generative deep learning\", \n",
    "    \"generative models\", \"transformer models\", \"self-attention models\", \n",
    "    \"transformer architecture\", \"transformer\", \"attention-based neural networks\", \n",
    "    \"transformer networks\", \"sequence-to-sequence models\", \"large language model\", \n",
    "    \"LLM\", \"transformer-based model\", \"pretrained language model\", \"generative language model\", \n",
    "    \"foundation model\", \"state-of-the-art language model\", \"multimodal model\", \n",
    "    \"multimodal neural network\", \"vision transformer\", \"diffusion model\", \n",
    "    \"generative diffusion model\", \"diffusion-based generative model\", \"continuous diffusion model\"\n",
    "]\n",
    "\n",
    "# Function to extract methods by matching keywords in the title and abstract\n",
    "def extract_methods_from_text(text):\n",
    "    if not isinstance(text, str):  # Check if text is a valid string\n",
    "        return \"Not Specified\"\n",
    "    \n",
    "    text = text.lower()  # Convert to lowercase for consistent matching\n",
    "    found_methods = [method for method in method_keywords if method.lower() in text]\n",
    "    return \", \".join(found_methods) if found_methods else \"Not Specified\"\n",
    "\n",
    "# Display DataFrame information to confirm the current state\n",
    "print(len(final_filtered_df))\n",
    "print('length')\n",
    "\n",
    "# Combine Title and Abstract into a comprehensive text field for analysis\n",
    "final_filtered_df['Title_Abstract'] = final_filtered_df['Title'].fillna('') + \" \" + final_filtered_df['Abstract'].fillna('')\n",
    "\n",
    "# Apply the method extraction function\n",
    "final_filtered_df['Methods_Used'] = final_filtered_df['Title_Abstract'].apply(extract_methods_from_text)\n",
    "\n",
    "print(final_filtered_df.info())\n",
    "\n",
    "# Display a sample of the results to check extracted methods\n",
    "print(\"Method extraction based on keywords completed.\")\n",
    "print(final_filtered_df[['PMID', 'Title', 'Abstract', 'Methods_Used']].head())\n",
    "\n",
    "# Count entries with \"Not Specified\" in 'Methods_Used'\n",
    "not_specified_count = final_filtered_df['Methods_Used'].value_counts().get(\"Not Specified\", 0)\n",
    "print(f\"Number of 'Not Specified' entries in Methods_Used: {not_specified_count}\")\n",
    "\n",
    "# Save the final filtered DataFrame with Methods_Used to a CSV file\n",
    "final_filtered_df.to_csv('deep_learning_virology_methods_extracted.csv', index=False)\n",
    "\n",
    "print(\"Final DataFrame with extracted methods saved as 'deep_learning_virology_methods_extracted.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd9c0ce",
   "metadata": {},
   "source": [
    "## Download Output File\n",
    "\n",
    "The following links allow you to download the filtered and processed datasets directly from the notebook:\n",
    "\n",
    "    - Contains papers with extracted method details based on keyword matching for deep learning techniques.\n",
    "\n",
    "You can click on link to download the respective CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8c82a96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='deep_learning_virology_methods_extracted.csv' target='_blank'>deep_learning_virology_methods_extracted.csv</a><br>"
      ],
      "text/plain": [
       "/home/paritala/Desktop/NLP task/deep_learning_virology_methods_extracted.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display download link for the final CSV\n",
    "display(FileLink('deep_learning_virology_methods_extracted.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
