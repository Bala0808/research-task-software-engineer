PMID,Title,Authors,Citation,First Author,Journal/Book,Publication Year,Create Date,PMCID,NIHMS ID,DOI,Abstract,Title_Abstract,Paper_Embedding,Virology_Similarity,Deep_Learning_Similarity,Method_Type,Methods_Used
39013794,Deep Learning - Methods to Amplify Epidemiological Data Collection and Analyses,"Alex Quistberg D, Mooney SJ, Tasdizen T, Arbelaez P, Nguyen QC.",Am J Epidemiol. 2024 Jul 16:kwae215. doi: 10.1093/aje/kwae215. Online ahead of print.,Alex Quistberg D,Am J Epidemiol,2024,2024/07/16,,,10.1093/aje/kwae215,"Deep learning is a subfield of artificial intelligence and machine learning based mostly on neural networks and often combined with attention algorithms that has been used to detect and identify objects in text, audio, images, and video. Serghiou and Rough (Am J Epidemiol. 0000;000(00):0000-0000) present a primer for epidemiologists on deep learning models. These models provide substantial opportunities for epidemiologists to expand and amplify their research in both data collection and analyses by increasing the geographic reach of studies, including more research subjects, and working with large or high dimensional data. The tools for implementing deep learning methods are not quite yet as straightforward or ubiquitous for epidemiologists as traditional regression methods found in standard statistical software, but there are exciting opportunities for interdisciplinary collaboration with deep learning experts, just as epidemiologists have with statisticians, healthcare providers, urban planners, and other professionals. Despite the novelty of these methods, epidemiological principles of assessing bias, study design, interpretation and others still apply when implementing deep learning methods or assessing the findings of studies that have used them.","Deep Learning - Methods to Amplify Epidemiological Data Collection and Analyses Deep learning is a subfield of artificial intelligence and machine learning based mostly on neural networks and often combined with attention algorithms that has been used to detect and identify objects in text, audio, images, and video. Serghiou and Rough (Am J Epidemiol. 0000;000(00):0000-0000) present a primer for epidemiologists on deep learning models. These models provide substantial opportunities for epidemiologists to expand and amplify their research in both data collection and analyses by increasing the geographic reach of studies, including more research subjects, and working with large or high dimensional data. The tools for implementing deep learning methods are not quite yet as straightforward or ubiquitous for epidemiologists as traditional regression methods found in standard statistical software, but there are exciting opportunities for interdisciplinary collaboration with deep learning experts, just as epidemiologists have with statisticians, healthcare providers, urban planners, and other professionals. Despite the novelty of these methods, epidemiological principles of assessing bias, study design, interpretation and others still apply when implementing deep learning methods or assessing the findings of studies that have used them.","[ 0.05850248  0.2632981   0.33853316 ...  0.48994902 -0.36432013
 -0.3297251 ]",0.9148114,0.8162069,Both,"neural network, deep learning"
38006509,Quantitation of Oncologic Image Features for Radiomic Analyses in PET,"Williams TL, Gonen M, Wray R, Do RKG, Simpson AL.",Methods Mol Biol. 2024;2729:409-421. doi: 10.1007/978-1-0716-3499-8_23.,Williams TL,Methods Mol Biol,2024,2023/11/25,,,10.1007/978-1-0716-3499-8_23,"Radiomics is an emerging and exciting field of study involving the extraction of many quantitative features from radiographic images. Positron emission tomography (PET) images are used in cancer diagnosis and staging. Utilizing radiomics on PET images can better quantify the spatial relationships between image voxels and generate more consistent and accurate results for diagnosis, prognosis, treatment, etc. This chapter gives the general steps a researcher would take to extract PET radiomic features from medical images and properly develop models to implement.","Quantitation of Oncologic Image Features for Radiomic Analyses in PET Radiomics is an emerging and exciting field of study involving the extraction of many quantitative features from radiographic images. Positron emission tomography (PET) images are used in cancer diagnosis and staging. Utilizing radiomics on PET images can better quantify the spatial relationships between image voxels and generate more consistent and accurate results for diagnosis, prognosis, treatment, etc. This chapter gives the general steps a researcher would take to extract PET radiomic features from medical images and properly develop models to implement.","[-0.20012559  0.6479427  -0.29164964 ...  0.79803896 -0.02133496
 -0.61350334]",0.90134776,0.8012022,Computer Vision,Not Specified
36189512,Visual ergonomics for changing work environments in the COVID-19 pandemic,"Khanwalkar P, Dabir N.",Work. 2022;73(s1):S169-S176. doi: 10.3233/WOR-211130.,Khanwalkar P,Work,2022,2022/10/03,,,10.3233/WOR-211130,"BACKGROUND: The coronavirus 2019 (COVID-19) pandemic has brought about change in the work environment, increasing remote and hybrid mode of work, presenting a compelling need to study visual ergonomics in this new work environment.
OBJECTIVE: To assess computer vision symptoms and visual ergonomics in remote and hybrid work settings during the COVID-19 pandemic with a focus on eye to screen relationship.
METHODS: The computer-vision symptom scale (CVSS17) questionnaire and questions about human factors and ergonomics were included in the survey conducted in September 2021. Sixty-six working professionals (mean age 37 years±5), working from home (n = 44) or in hybrid mode (n = 22) were included in the study. Cramer's V was used for the correlation coefficient between two categorical variables for assessing eye health in changing work environments.
RESULTS: Compared to our previous study, the correlation between computer vision syndrome (CVS) symptoms is markedly higher. The population working in hybrid mode experienced eye heaviness with strain to see well (V = 0.6872, p = 0.002) and dryness in the eyes (V = 0.5912, p = 0.0179). The population working from home who are bothered by surrounding lights also report dryness in the eyes (V = 0.3846, p = 0.0005). Screen use hours are higher in work from home situations (43% work more than 9 hrs) than those in hybrid mode of work (4% work more than 9 hrs).
CONCLUSION: A definite increase in CVS in most of the population working remotely or in hybrid environments is established through this study. User-friendly strategies for raising awareness of applied visual ergonomics can prevent rampant onset of CVS in the working population.","Visual ergonomics for changing work environments in the COVID-19 pandemic BACKGROUND: The coronavirus 2019 (COVID-19) pandemic has brought about change in the work environment, increasing remote and hybrid mode of work, presenting a compelling need to study visual ergonomics in this new work environment.
OBJECTIVE: To assess computer vision symptoms and visual ergonomics in remote and hybrid work settings during the COVID-19 pandemic with a focus on eye to screen relationship.
METHODS: The computer-vision symptom scale (CVSS17) questionnaire and questions about human factors and ergonomics were included in the survey conducted in September 2021. Sixty-six working professionals (mean age 37 years±5), working from home (n = 44) or in hybrid mode (n = 22) were included in the study. Cramer's V was used for the correlation coefficient between two categorical variables for assessing eye health in changing work environments.
RESULTS: Compared to our previous study, the correlation between computer vision syndrome (CVS) symptoms is markedly higher. The population working in hybrid mode experienced eye heaviness with strain to see well (V = 0.6872, p = 0.002) and dryness in the eyes (V = 0.5912, p = 0.0179). The population working from home who are bothered by surrounding lights also report dryness in the eyes (V = 0.3846, p = 0.0005). Screen use hours are higher in work from home situations (43% work more than 9 hrs) than those in hybrid mode of work (4% work more than 9 hrs).
CONCLUSION: A definite increase in CVS in most of the population working remotely or in hybrid environments is established through this study. User-friendly strategies for raising awareness of applied visual ergonomics can prevent rampant onset of CVS in the working population.",[0.23437095 0.23635566 0.13041244 ... 0.31070343 0.2849217  0.5748013 ],0.9076988,0.8109585,Other,computer vision
34253822,Deep learning for COVID-19 detection based on CT images,"Zhao W, Jiang W, Qiu X.",Sci Rep. 2021 Jul 12;11(1):14353. doi: 10.1038/s41598-021-93832-2.,Zhao W,Sci Rep,2021,2021/07/13,PMC8275612,,10.1038/s41598-021-93832-2,"COVID-19 has tremendously impacted patients and medical systems globally. Computed tomography images can effectively complement the reverse transcription-polymerase chain reaction testing. This study adopted a convolutional neural network for COVID-19 testing. We examined the performance of different pre-trained models on CT testing and identified that larger, out-of-field datasets boost the testing power of the models. This suggests that a priori knowledge of the models from out-of-field training is also applicable to CT images. The proposed transfer learning approach proves to be more successful than the current approaches described in literature. We believe that our approach has achieved the state-of-the-art performance in identification thus far. Based on experiments with randomly sampled training datasets, the results reveal a satisfactory performance by our model. We investigated the relevant visual characteristics of the CT images used by the model; these may assist clinical doctors in manual screening.","Deep learning for COVID-19 detection based on CT images COVID-19 has tremendously impacted patients and medical systems globally. Computed tomography images can effectively complement the reverse transcription-polymerase chain reaction testing. This study adopted a convolutional neural network for COVID-19 testing. We examined the performance of different pre-trained models on CT testing and identified that larger, out-of-field datasets boost the testing power of the models. This suggests that a priori knowledge of the models from out-of-field training is also applicable to CT images. The proposed transfer learning approach proves to be more successful than the current approaches described in literature. We believe that our approach has achieved the state-of-the-art performance in identification thus far. Based on experiments with randomly sampled training datasets, the results reveal a satisfactory performance by our model. We investigated the relevant visual characteristics of the CT images used by the model; these may assist clinical doctors in manual screening.","[ 0.04248524  0.45259807  0.38525787 ...  0.77214056 -0.19657789
 -0.5761338 ]",0.9033046,0.80338025,Computer Vision,"neural network, convolutional neural network, deep learning"
35782182,Computer Audition for Fighting the SARS-CoV-2 Corona Crisis-Introducing the Multitask Speech Corpus for COVID-19,"Qian K, Schmitt M, Zheng H, Koike T, Han J, Liu J, Ji W, Duan J, Song M, Yang Z, Ren Z, Liu S, Zhang Z, Yamamoto Y, Schuller BW.",IEEE Internet Things J. 2021 Mar 22;8(21):16035-16046. doi: 10.1109/JIOT.2021.3067605. eCollection 2021 Nov 1.,Qian K,IEEE Internet Things J,2021,2022/07/05,PMC8768988,,10.1109/JIOT.2021.3067605,"Computer audition (CA) has experienced a fast development in the past decades by leveraging advanced signal processing and machine learning techniques. In particular, for its noninvasive and ubiquitous character by nature, CA-based applications in healthcare have increasingly attracted attention in recent years. During the tough time of the global crisis caused by the coronavirus disease 2019 (COVID-19), scientists and engineers in data science have collaborated to think of novel ways in prevention, diagnosis, treatment, tracking, and management of this global pandemic. On the one hand, we have witnessed the power of 5G, Internet of Things, big data, computer vision, and artificial intelligence in applications of epidemiology modeling, drug and/or vaccine finding and designing, fast CT screening, and quarantine management. On the other hand, relevant studies in exploring the capacity of CA are extremely lacking and underestimated. To this end, we propose a novel multitask speech corpus for COVID-19 research usage. We collected 51 confirmed COVID-19 patients' in-the-wild speech data in Wuhan city, China. We define three main tasks in this corpus, i.e., three-category classification tasks for evaluating the physical and/or mental status of patients, i.e., sleep quality, fatigue, and anxiety. The benchmarks are given by using both classic machine learning methods and state-of-the-art deep learning techniques. We believe this study and corpus cannot only facilitate the ongoing research on using data science to fight against COVID-19, but also the monitoring of contagious diseases for general purpose.","Computer Audition for Fighting the SARS-CoV-2 Corona Crisis-Introducing the Multitask Speech Corpus for COVID-19 Computer audition (CA) has experienced a fast development in the past decades by leveraging advanced signal processing and machine learning techniques. In particular, for its noninvasive and ubiquitous character by nature, CA-based applications in healthcare have increasingly attracted attention in recent years. During the tough time of the global crisis caused by the coronavirus disease 2019 (COVID-19), scientists and engineers in data science have collaborated to think of novel ways in prevention, diagnosis, treatment, tracking, and management of this global pandemic. On the one hand, we have witnessed the power of 5G, Internet of Things, big data, computer vision, and artificial intelligence in applications of epidemiology modeling, drug and/or vaccine finding and designing, fast CT screening, and quarantine management. On the other hand, relevant studies in exploring the capacity of CA are extremely lacking and underestimated. To this end, we propose a novel multitask speech corpus for COVID-19 research usage. We collected 51 confirmed COVID-19 patients' in-the-wild speech data in Wuhan city, China. We define three main tasks in this corpus, i.e., three-category classification tasks for evaluating the physical and/or mental status of patients, i.e., sleep quality, fatigue, and anxiety. The benchmarks are given by using both classic machine learning methods and state-of-the-art deep learning techniques. We believe this study and corpus cannot only facilitate the ongoing research on using data science to fight against COVID-19, but also the monitoring of contagious diseases for general purpose.","[ 0.21117201  0.38244686  0.17962985 ...  0.40724194  0.13399228
 -0.32729015]",0.906755,0.80609107,Both,"deep learning, computer vision"
33231160,Deep Transfer Learning for COVID-19 Prediction: Case Study for Limited Data Problems,"Albahli S, Albattah W.",Curr Med Imaging. 2021;17(8):973-980. doi: 10.2174/1573405616666201123120417.,Albahli S,Curr Med Imaging,2021,2020/11/24,PMC8653418,,10.2174/1573405616666201123120417,"OBJECTIVE: Automatic prediction of COVID-19 using deep convolution neural networks based pre-trained transfer models and Chest X-ray images.
METHODS: This research employs the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of the disease. Using Deep Learning models, the research aims at evaluating the effectiveness and accuracy of different convolutional neural networks models in the automatic diagnosis of COVID-19 from X-ray images as compared to diagnosis performed by experts in the medical community.
RESULTS: Due to the fact that the dataset available for COVID-19 is still limited, the best model to use is the InceptionNetV3. Performance results show that the InceptionNetV3 model yielded the highest accuracy of 98.63% (with data augmentation) and 98.90% (without data augmentation) among the three models designed. However, as the dataset gets bigger, the Inception ResNetV2 and NASNetlarge will do a better job of classification. All the performed networks tend to over-fit when data augmentation is not used, this is due to the small amount of data used for training and validation.
CONCLUSION: A deep transfer learning is proposed to detecting the COVID-19 automatically from chest X-ray by training it with X-ray images gotten from both COVID-19 patients and people with normal chest X-rays. The study is aimed at helping doctors in making decisions in their clinical practice due its high performance and effectiveness, the study also gives an insight to how transfer learning was used to automatically detect the COVID-19.","Deep Transfer Learning for COVID-19 Prediction: Case Study for Limited Data Problems OBJECTIVE: Automatic prediction of COVID-19 using deep convolution neural networks based pre-trained transfer models and Chest X-ray images.
METHODS: This research employs the advantages of computer vision and medical image analysis to develop an automated model that has the clinical potential for early detection of the disease. Using Deep Learning models, the research aims at evaluating the effectiveness and accuracy of different convolutional neural networks models in the automatic diagnosis of COVID-19 from X-ray images as compared to diagnosis performed by experts in the medical community.
RESULTS: Due to the fact that the dataset available for COVID-19 is still limited, the best model to use is the InceptionNetV3. Performance results show that the InceptionNetV3 model yielded the highest accuracy of 98.63% (with data augmentation) and 98.90% (without data augmentation) among the three models designed. However, as the dataset gets bigger, the Inception ResNetV2 and NASNetlarge will do a better job of classification. All the performed networks tend to over-fit when data augmentation is not used, this is due to the small amount of data used for training and validation.
CONCLUSION: A deep transfer learning is proposed to detecting the COVID-19 automatically from chest X-ray by training it with X-ray images gotten from both COVID-19 patients and people with normal chest X-rays. The study is aimed at helping doctors in making decisions in their clinical practice due its high performance and effectiveness, the study also gives an insight to how transfer learning was used to automatically detect the COVID-19.","[ 0.04152548  0.59138346  0.4078377  ...  0.7413976   0.06226723
 -0.40561208]",0.90687823,0.8067089,Computer Vision,"neural network, convolutional neural network, deep learning, computer vision"
33094700,Implementation of convolutional neural network approach for COVID-19 disease detection,Irmak E.,Physiol Genomics. 2020 Dec 1;52(12):590-601. doi: 10.1152/physiolgenomics.00084.2020. Epub 2020 Oct 23.,Irmak E,Physiol Genomics,2020,2020/10/23,PMC7774002,,10.1152/physiolgenomics.00084.2020,"In this paper, two novel, powerful, and robust convolutional neural network (CNN) architectures are designed and proposed for two different classification tasks using publicly available data sets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 versus normal versus pneumonia) with 98.27% average accuracy. The hyperparameters of both CNN models are automatically determined using Grid Search. Experimental results on large clinical data sets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome the disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue, which is a great improvement of available automatic methods in the literature. To the best of the author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN, whose hyperparameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN-based COVID-19 chest X-ray image classification study that uses the largest possible clinical data set. A total of 1,524 COVID-19, 1,527 pneumonia, and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.","Implementation of convolutional neural network approach for COVID-19 disease detection In this paper, two novel, powerful, and robust convolutional neural network (CNN) architectures are designed and proposed for two different classification tasks using publicly available data sets. The first architecture is able to decide whether a given chest X-ray image of a patient contains COVID-19 or not with 98.92% average accuracy. The second CNN architecture is able to divide a given chest X-ray image of a patient into three classes (COVID-19 versus normal versus pneumonia) with 98.27% average accuracy. The hyperparameters of both CNN models are automatically determined using Grid Search. Experimental results on large clinical data sets show the effectiveness of the proposed architectures and demonstrate that the proposed algorithms can overcome the disadvantages mentioned above. Moreover, the proposed CNN models are fully automatic in terms of not requiring the extraction of diseased tissue, which is a great improvement of available automatic methods in the literature. To the best of the author's knowledge, this study is the first study to detect COVID-19 disease from given chest X-ray images, using CNN, whose hyperparameters are automatically determined by the Grid Search. Another important contribution of this study is that it is the first CNN-based COVID-19 chest X-ray image classification study that uses the largest possible clinical data set. A total of 1,524 COVID-19, 1,527 pneumonia, and 1524 normal X-ray images are collected. It is aimed to collect the largest number of COVID-19 X-ray images that exist in the literature until the writing of this research paper.","[ 0.07707462  0.47931933  0.18715423 ...  0.7693102  -0.08013391
 -0.5855329 ]",0.9076934,0.8097199,Computer Vision,"neural network, convolutional neural network, CNN"
33025386,Issues associated with deploying CNN transfer learning to detect COVID-19 from chest X-rays,"Majeed T, Rashid R, Ali D, Asaad A.",Phys Eng Sci Med. 2020 Dec;43(4):1289-1303. doi: 10.1007/s13246-020-00934-8. Epub 2020 Oct 6.,Majeed T,Phys Eng Sci Med,2020,2020/10/07,PMC7537970,,10.1007/s13246-020-00934-8,"Covid-19 first occurred in Wuhan, China in December 2019. Subsequently, the virus spread throughout the world and as of June 2020 the total number of confirmed cases are above 4.7 million with over 315,000 deaths. Machine learning algorithms built on radiography images can be used as a decision support mechanism to aid radiologists to speed up the diagnostic process. The aim of this work is to conduct a critical analysis to investigate the applicability of convolutional neural networks (CNNs) for the purpose of COVID-19 detection in chest X-ray images and highlight the issues of using CNN directly on the whole image. To accomplish this task, we use 12-off-the-shelf CNN architectures in transfer learning mode on 3 publicly available chest X-ray databases together with proposing a shallow CNN architecture in which we train it from scratch. Chest X-ray images are fed into CNN models without any preprocessing to replicate researches used chest X-rays in this manner. Then a qualitative investigation performed to inspect the decisions made by CNNs using a technique known as class activation maps (CAM). Using CAMs, one can map the activations contributed to the decision of CNNs back to the original image to visualize the most discriminating region(s) on the input image. We conclude that CNN decisions should not be taken into consideration, despite their high classification accuracy, until clinicians can visually inspect and approve the region(s) of the input image used by CNNs that lead to its prediction.","Issues associated with deploying CNN transfer learning to detect COVID-19 from chest X-rays Covid-19 first occurred in Wuhan, China in December 2019. Subsequently, the virus spread throughout the world and as of June 2020 the total number of confirmed cases are above 4.7 million with over 315,000 deaths. Machine learning algorithms built on radiography images can be used as a decision support mechanism to aid radiologists to speed up the diagnostic process. The aim of this work is to conduct a critical analysis to investigate the applicability of convolutional neural networks (CNNs) for the purpose of COVID-19 detection in chest X-ray images and highlight the issues of using CNN directly on the whole image. To accomplish this task, we use 12-off-the-shelf CNN architectures in transfer learning mode on 3 publicly available chest X-ray databases together with proposing a shallow CNN architecture in which we train it from scratch. Chest X-ray images are fed into CNN models without any preprocessing to replicate researches used chest X-rays in this manner. Then a qualitative investigation performed to inspect the decisions made by CNNs using a technique known as class activation maps (CAM). Using CAMs, one can map the activations contributed to the decision of CNNs back to the original image to visualize the most discriminating region(s) on the input image. We conclude that CNN decisions should not be taken into consideration, despite their high classification accuracy, until clinicians can visually inspect and approve the region(s) of the input image used by CNNs that lead to its prediction.","[-0.04376448  0.42415762  0.30821756 ...  0.87319493 -0.06560979
 -0.4942979 ]",0.9144239,0.8053302,Both,"neural network, convolutional neural network, CNN"
39448760,Cough2COVID-19 detection using an enhanced multi layer ensemble deep learning framework and CoughFeatureRanker,"Husssain S, Ayoub M, Wahid JA, Khan A, Alabrah A, Amran GA.",Sci Rep. 2024 Oct 24;14(1):25207. doi: 10.1038/s41598-024-76639-9.,Husssain S,Sci Rep,2024,2024/10/25,PMC11502923,,10.1038/s41598-024-76639-9,"In response to the pressing requirement for precise and easily accessible COVID-19 detection methods, we present the Cough2COVID-19 framework, which is cost-effective, non-intrusive, and widely accessible. The conventional diagnostic methods, notably the PCR test, are encumbered by limitations such as cost and invasiveness. Consequently, the exploration of alternative solutions has gained momentum. Our innovative approach employs a multi-layer ensemble deep learning (MLEDL) framework that capitalizes on cough audio signals to achieve heightened efficiency in COVID-19 detection. This study introduces the Cough2COVID-19 framework, effectively addressing these challenges through AI-driven analysis. Additionally, this study proposed the CoughFeatureRanker algorithm, which delves into the robustness of pivotal features embedded within cough audios. The CoughFeatureRanker algorithm selects the most prominent features based on their optimal discriminatory performance from 15 features to detect COVID-19. The effectiveness of the CoughFeatureRanker algorithm within the ensemble framework is scrutinized, confirming its favorable influence on the accuracy of COVID-19 detection. The Cough2COVID-19 (MLEDL) framework achieves remarkable outcomes in COVID-19 detection through cough audio signals, boasting a specificity of 98%, sensitivity of 97%, accuracy of 98%, and an AUC score of 0.981. Our framework asserts its supremacy in precise non-invasive screening through an exhaustive comparison with cutting-edge methodologies. This groundbreaking innovation holds the potential to enhance urban resilience by transforming disease diagnosis, offering a significant approach to curtailing transmission risks and facilitating timely interventions in the ongoing battle against the pandemic.","Cough2COVID-19 detection using an enhanced multi layer ensemble deep learning framework and CoughFeatureRanker In response to the pressing requirement for precise and easily accessible COVID-19 detection methods, we present the Cough2COVID-19 framework, which is cost-effective, non-intrusive, and widely accessible. The conventional diagnostic methods, notably the PCR test, are encumbered by limitations such as cost and invasiveness. Consequently, the exploration of alternative solutions has gained momentum. Our innovative approach employs a multi-layer ensemble deep learning (MLEDL) framework that capitalizes on cough audio signals to achieve heightened efficiency in COVID-19 detection. This study introduces the Cough2COVID-19 framework, effectively addressing these challenges through AI-driven analysis. Additionally, this study proposed the CoughFeatureRanker algorithm, which delves into the robustness of pivotal features embedded within cough audios. The CoughFeatureRanker algorithm selects the most prominent features based on their optimal discriminatory performance from 15 features to detect COVID-19. The effectiveness of the CoughFeatureRanker algorithm within the ensemble framework is scrutinized, confirming its favorable influence on the accuracy of COVID-19 detection. The Cough2COVID-19 (MLEDL) framework achieves remarkable outcomes in COVID-19 detection through cough audio signals, boasting a specificity of 98%, sensitivity of 97%, accuracy of 98%, and an AUC score of 0.981. Our framework asserts its supremacy in precise non-invasive screening through an exhaustive comparison with cutting-edge methodologies. This groundbreaking innovation holds the potential to enhance urban resilience by transforming disease diagnosis, offering a significant approach to curtailing transmission risks and facilitating timely interventions in the ongoing battle against the pandemic.","[ 0.05991062  0.5922822   0.12700188 ...  0.70413166 -0.04269189
 -0.6738444 ]",0.9213201,0.82126266,Computer Vision,deep learning
39384938,Prediction of adverse drug reactions using demographic and non-clinical drug characteristics in FAERS data,"Farnoush A, Sedighi-Maman Z, Rasoolian B, Heath JJ, Fallah B.",Sci Rep. 2024 Oct 9;14(1):23636. doi: 10.1038/s41598-024-74505-2.,Farnoush A,Sci Rep,2024,2024/10/09,PMC11464664,,10.1038/s41598-024-74505-2,"The presence of adverse drug reactions (ADRs) is an ongoing public health concern. While traditional methods to discover ADRs are very costly and limited, it is prudent to predict ADRs through non-invasive methods such as machine learning based on existing data. Although various studies exist regarding ADR prediction using non-clinical data, a process that leverages both demographic and non-clinical data for ADR prediction is missing. In addition, the importance of individual features in ADR prediction has yet to be fully explored. This study aims to develop an ADR prediction model based on demographic and non-clinical data, where we identify the highest contributing factors. We focus our efforts on 30 common and severe ADRs reported to the Food and Drug Administration (FDA) between 2012 and 2023. We have developed a random forest (RF) and deep learning (DL) machine learning model that ingests demographic data (e.g., Age and Gender of patients) and non-clinical data, which includes chemical, molecular, and biological drug characteristics. We successfully unified both demographic and non-clinical data sources within a complete dataset regarding ADR prediction. Model performances were assessed via the area under the receiver operating characteristic curve (AUC) and the mean average precision (MAP). We demonstrated that our parsimonious models, which include only the top 20 most important features comprising 5 demographic features and 15 non-clinical features (13 molecular and 2 biological), achieve ADR prediction performance comparable to a less practical, feature-rich model consisting of all 2,315 features. Specifically, our models achieved an AUC of 0.611 and 0.674 for RF and DL algorithms, respectively. We hope our research provides researchers and clinicians with valuable insights and facilitates future research designs by identifying top ADR predictors (including demographic information) and practical parsimonious models.","Prediction of adverse drug reactions using demographic and non-clinical drug characteristics in FAERS data The presence of adverse drug reactions (ADRs) is an ongoing public health concern. While traditional methods to discover ADRs are very costly and limited, it is prudent to predict ADRs through non-invasive methods such as machine learning based on existing data. Although various studies exist regarding ADR prediction using non-clinical data, a process that leverages both demographic and non-clinical data for ADR prediction is missing. In addition, the importance of individual features in ADR prediction has yet to be fully explored. This study aims to develop an ADR prediction model based on demographic and non-clinical data, where we identify the highest contributing factors. We focus our efforts on 30 common and severe ADRs reported to the Food and Drug Administration (FDA) between 2012 and 2023. We have developed a random forest (RF) and deep learning (DL) machine learning model that ingests demographic data (e.g., Age and Gender of patients) and non-clinical data, which includes chemical, molecular, and biological drug characteristics. We successfully unified both demographic and non-clinical data sources within a complete dataset regarding ADR prediction. Model performances were assessed via the area under the receiver operating characteristic curve (AUC) and the mean average precision (MAP). We demonstrated that our parsimonious models, which include only the top 20 most important features comprising 5 demographic features and 15 non-clinical features (13 molecular and 2 biological), achieve ADR prediction performance comparable to a less practical, feature-rich model consisting of all 2,315 features. Specifically, our models achieved an AUC of 0.611 and 0.674 for RF and DL algorithms, respectively. We hope our research provides researchers and clinicians with valuable insights and facilitates future research designs by identifying top ADR predictors (including demographic information) and practical parsimonious models.","[-0.39600068  0.7980129   0.4261959  ...  0.48613295 -0.21117698
 -0.28353256]",0.9123186,0.805267,Other,"machine learning model, deep learning"
39266748,An open-source framework for end-to-end analysis of electronic health record data,"Heumos L, Ehmele P, Treis T, Upmeier Zu Belzen J, Roellin E, May L, Namsaraeva A, Horlava N, Shitov VA, Zhang X, Zappia L, Knoll R, Lang NJ, Hetzel L, Virshup I, Sikkema L, Curion F, Eils R, Schiller HB, Hilgendorff A, Theis FJ.",Nat Med. 2024 Sep 12. doi: 10.1038/s41591-024-03214-0. Online ahead of print.,Heumos L,Nat Med,2024,2024/09/12,,,10.1038/s41591-024-03214-0,"With progressive digitalization of healthcare systems worldwide, large-scale collection of electronic health records (EHRs) has become commonplace. However, an extensible framework for comprehensive exploratory analysis that accounts for data heterogeneity is missing. Here we introduce ehrapy, a modular open-source Python framework designed for exploratory analysis of heterogeneous epidemiology and EHR data. ehrapy incorporates a series of analytical steps, from data extraction and quality control to the generation of low-dimensional representations. Complemented by rich statistical modules, ehrapy facilitates associating patients with disease states, differential comparison between patient clusters, survival analysis, trajectory inference, causal inference and more. Leveraging ontologies, ehrapy further enables data sharing and training EHR deep learning models, paving the way for foundational models in biomedical research. We demonstrate ehrapy's features in six distinct examples. We applied ehrapy to stratify patients affected by unspecified pneumonia into finer-grained phenotypes. Furthermore, we reveal biomarkers for significant differences in survival among these groups. Additionally, we quantify medication-class effects of pneumonia medications on length of stay. We further leveraged ehrapy to analyze cardiovascular risks across different data modalities. We reconstructed disease state trajectories in patients with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) based on imaging data. Finally, we conducted a case study to demonstrate how ehrapy can detect and mitigate biases in EHR data. ehrapy, thus, provides a framework that we envision will standardize analysis pipelines on EHR data and serve as a cornerstone for the community.","An open-source framework for end-to-end analysis of electronic health record data With progressive digitalization of healthcare systems worldwide, large-scale collection of electronic health records (EHRs) has become commonplace. However, an extensible framework for comprehensive exploratory analysis that accounts for data heterogeneity is missing. Here we introduce ehrapy, a modular open-source Python framework designed for exploratory analysis of heterogeneous epidemiology and EHR data. ehrapy incorporates a series of analytical steps, from data extraction and quality control to the generation of low-dimensional representations. Complemented by rich statistical modules, ehrapy facilitates associating patients with disease states, differential comparison between patient clusters, survival analysis, trajectory inference, causal inference and more. Leveraging ontologies, ehrapy further enables data sharing and training EHR deep learning models, paving the way for foundational models in biomedical research. We demonstrate ehrapy's features in six distinct examples. We applied ehrapy to stratify patients affected by unspecified pneumonia into finer-grained phenotypes. Furthermore, we reveal biomarkers for significant differences in survival among these groups. Additionally, we quantify medication-class effects of pneumonia medications on length of stay. We further leveraged ehrapy to analyze cardiovascular risks across different data modalities. We reconstructed disease state trajectories in patients with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) based on imaging data. Finally, we conducted a case study to demonstrate how ehrapy can detect and mitigate biases in EHR data. ehrapy, thus, provides a framework that we envision will standardize analysis pipelines on EHR data and serve as a cornerstone for the community.","[-0.01695872  0.8197423  -0.00338541 ...  0.54727435 -0.2747945
 -0.58389497]",0.9106115,0.80819535,Both,deep learning
39232164,A pathology foundation model for cancer diagnosis and prognosis prediction,"Wang X, Zhao J, Marostica E, Yuan W, Jin J, Zhang J, Li R, Tang H, Wang K, Li Y, Wang F, Peng Y, Zhu J, Zhang J, Jackson CR, Zhang J, Dillon D, Lin NU, Sholl L, Denize T, Meredith D, Ligon KL, Signoretti S, Ogino S, Golden JA, Nasrallah MP, Han X, Yang S, Yu KH.",Nature. 2024 Oct;634(8035):970-978. doi: 10.1038/s41586-024-07894-z. Epub 2024 Sep 4.,Wang X,Nature,2024,2024/09/04,,,10.1038/s41586-024-07894-z,"Histopathology image evaluation is indispensable for cancer diagnoses and subtype classification. Standard artificial intelligence methods for histopathology image analyses have focused on optimizing specialized models for each diagnostic task1,2. Although such methods have achieved some success, they often have limited generalizability to images generated by different digitization protocols or samples collected from different populations3. Here, to address this challenge, we devised the Clinical Histopathology Imaging Evaluation Foundation (CHIEF) model, a general-purpose weakly supervised machine learning framework to extract pathology imaging features for systematic cancer evaluation. CHIEF leverages two complementary pretraining methods to extract diverse pathology representations: unsupervised pretraining for tile-level feature identification and weakly supervised pretraining for whole-slide pattern recognition. We developed CHIEF using 60,530 whole-slide images spanning 19 anatomical sites. Through pretraining on 44 terabytes of high-resolution pathology imaging datasets, CHIEF extracted microscopic representations useful for cancer cell detection, tumour origin identification, molecular profile characterization and prognostic prediction. We successfully validated CHIEF using 19,491 whole-slide images from 32 independent slide sets collected from 24 hospitals and cohorts internationally. Overall, CHIEF outperformed the state-of-the-art deep learning methods by up to 36.1%, showing its ability to address domain shifts observed in samples from diverse populations and processed by different slide preparation methods. CHIEF provides a generalizable foundation for efficient digital pathology evaluation for patients with cancer.","A pathology foundation model for cancer diagnosis and prognosis prediction Histopathology image evaluation is indispensable for cancer diagnoses and subtype classification. Standard artificial intelligence methods for histopathology image analyses have focused on optimizing specialized models for each diagnostic task1,2. Although such methods have achieved some success, they often have limited generalizability to images generated by different digitization protocols or samples collected from different populations3. Here, to address this challenge, we devised the Clinical Histopathology Imaging Evaluation Foundation (CHIEF) model, a general-purpose weakly supervised machine learning framework to extract pathology imaging features for systematic cancer evaluation. CHIEF leverages two complementary pretraining methods to extract diverse pathology representations: unsupervised pretraining for tile-level feature identification and weakly supervised pretraining for whole-slide pattern recognition. We developed CHIEF using 60,530 whole-slide images spanning 19 anatomical sites. Through pretraining on 44 terabytes of high-resolution pathology imaging datasets, CHIEF extracted microscopic representations useful for cancer cell detection, tumour origin identification, molecular profile characterization and prognostic prediction. We successfully validated CHIEF using 19,491 whole-slide images from 32 independent slide sets collected from 24 hospitals and cohorts internationally. Overall, CHIEF outperformed the state-of-the-art deep learning methods by up to 36.1%, showing its ability to address domain shifts observed in samples from diverse populations and processed by different slide preparation methods. CHIEF provides a generalizable foundation for efficient digital pathology evaluation for patients with cancer.","[-0.04111103  0.62654024  0.07170822 ...  0.6522523  -0.34563443
 -0.72623825]",0.90356344,0.8198054,Both,"deep learning, foundation model"
39090144,Forecasting rheumatoid arthritis patient arrivals by including meteorological factors and air pollutants,"Ye Z, Ye B, Ming Z, Shu J, Xia C, Xu L, Wan Y, Wei Z.",Sci Rep. 2024 Aug 1;14(1):17840. doi: 10.1038/s41598-024-67694-3.,Ye Z,Sci Rep,2024,2024/08/01,PMC11294361,,10.1038/s41598-024-67694-3,"The burden of rheumatoid arthritis (RA) has gradually elevated, increasing the need for medical resource redistribution. Forecasting RA patient arrivals can be helpful in managing medical resources. However, no relevant studies have been conducted yet. This study aims to construct a long short-term memory (LSTM) model, a deep learning model recently developed for novel data processing, to forecast RA patient arrivals considering meteorological factors and air pollutants and compares this model with traditional methods. Data on RA patients, meteorological factors and air pollutants from 2015 to 2022 were collected and normalized to construct moving average (MA)- and autoregressive (AR)-based and LSTM models. After data normalization, the root mean square error (RMSE) was adopted to evaluate models' forecast ability. A total of 2422 individuals were enrolled. Not using the environmental data, the RMSEs of the MA- and AR-based models' test sets are 0.131, 0.132, and 0.117 when the training set: test set ratio is 2:1, 3:1, and 7:1, while they are 0.110, 0.130, and 0.112 for the univariate LSTM models. Considering meteorological factors and air pollutants, the RMSEs of the MA- and AR-based model test sets were 0.142, 0.303, and 0.164 when the training set: test set ratio is 2:1, 3:1, and 7:1, while they were 0.108, 0.119, and 0.109 for the multivariable LSTM models. Our study demonstrated that LSTM models can forecast RA patient arrivals more accurately than MA- and AR-based models for datasets of all three sizes. Considering the meteorological factors and air pollutants can further improve the forecasting ability of the LSTM models. This novel method provides valuable information for medical management, the optimization of medical resource redistribution, and the alleviation of resource shortages.","Forecasting rheumatoid arthritis patient arrivals by including meteorological factors and air pollutants The burden of rheumatoid arthritis (RA) has gradually elevated, increasing the need for medical resource redistribution. Forecasting RA patient arrivals can be helpful in managing medical resources. However, no relevant studies have been conducted yet. This study aims to construct a long short-term memory (LSTM) model, a deep learning model recently developed for novel data processing, to forecast RA patient arrivals considering meteorological factors and air pollutants and compares this model with traditional methods. Data on RA patients, meteorological factors and air pollutants from 2015 to 2022 were collected and normalized to construct moving average (MA)- and autoregressive (AR)-based and LSTM models. After data normalization, the root mean square error (RMSE) was adopted to evaluate models' forecast ability. A total of 2422 individuals were enrolled. Not using the environmental data, the RMSEs of the MA- and AR-based models' test sets are 0.131, 0.132, and 0.117 when the training set: test set ratio is 2:1, 3:1, and 7:1, while they are 0.110, 0.130, and 0.112 for the univariate LSTM models. Considering meteorological factors and air pollutants, the RMSEs of the MA- and AR-based model test sets were 0.142, 0.303, and 0.164 when the training set: test set ratio is 2:1, 3:1, and 7:1, while they were 0.108, 0.119, and 0.109 for the multivariable LSTM models. Our study demonstrated that LSTM models can forecast RA patient arrivals more accurately than MA- and AR-based models for datasets of all three sizes. Considering the meteorological factors and air pollutants can further improve the forecasting ability of the LSTM models. This novel method provides valuable information for medical management, the optimization of medical resource redistribution, and the alleviation of resource shortages.","[-0.4576666   0.3314539   0.19657816 ...  0.49307337  0.11283729
 -0.559822  ]",0.90867215,0.80050635,Other,"LSTM, deep learning"
38773161,COVID‑19 detection from chest X-ray images using transfer learning,El Houby EMF.,Sci Rep. 2024 May 21;14(1):11639. doi: 10.1038/s41598-024-61693-0.,El Houby EMF,Sci Rep,2024,2024/05/21,PMC11109273,,10.1038/s41598-024-61693-0,"COVID-19 is a kind of coronavirus that appeared in China in the Province of Wuhan in December 2019. The most significant influence of this virus is its very highly contagious characteristic which may lead to death. The standard diagnosis of COVID-19 is based on swabs from the throat and nose, their sensitivity is not high enough and so they are prone to errors. Early diagnosis of COVID-19 disease is important to provide the chance of quick isolation of the suspected cases and to decrease the opportunity of infection in healthy people. In this research, a framework for chest X-ray image classification tasks based on deep learning is proposed to help in early diagnosis of COVID-19. The proposed framework contains two phases which are the pre-processing phase and classification phase which uses pre-trained convolution neural network models based on transfer learning. In the pre-processing phase, different image enhancements have been applied to full and segmented X-ray images to improve the classification performance of the CNN models. Two CNN pre-trained models have been used for classification which are VGG19 and EfficientNetB0. From experimental results, the best model achieved a sensitivity of 0.96, specificity of 0.94, precision of 0.9412, F1 score of 0.9505 and accuracy of 0.95 using enhanced full X-ray images for binary classification of chest X-ray images into COVID-19 or normal with VGG19. The proposed framework is promising and achieved a classification accuracy of 0.935 for 4-class classification.","COVID‑19 detection from chest X-ray images using transfer learning COVID-19 is a kind of coronavirus that appeared in China in the Province of Wuhan in December 2019. The most significant influence of this virus is its very highly contagious characteristic which may lead to death. The standard diagnosis of COVID-19 is based on swabs from the throat and nose, their sensitivity is not high enough and so they are prone to errors. Early diagnosis of COVID-19 disease is important to provide the chance of quick isolation of the suspected cases and to decrease the opportunity of infection in healthy people. In this research, a framework for chest X-ray image classification tasks based on deep learning is proposed to help in early diagnosis of COVID-19. The proposed framework contains two phases which are the pre-processing phase and classification phase which uses pre-trained convolution neural network models based on transfer learning. In the pre-processing phase, different image enhancements have been applied to full and segmented X-ray images to improve the classification performance of the CNN models. Two CNN pre-trained models have been used for classification which are VGG19 and EfficientNetB0. From experimental results, the best model achieved a sensitivity of 0.96, specificity of 0.94, precision of 0.9412, F1 score of 0.9505 and accuracy of 0.95 using enhanced full X-ray images for binary classification of chest X-ray images into COVID-19 or normal with VGG19. The proposed framework is promising and achieved a classification accuracy of 0.935 for 4-class classification.","[ 0.14895314  0.4945422   0.22903885 ...  0.55919015 -0.02239565
 -0.31080204]",0.92205554,0.8113415,Computer Vision,"neural network, CNN, deep learning"
38726471,Predicting the Utility of Scientific Articles for Emerging Pandemics Using Their Titles and Natural Language Processing,"Dobolyi K, Hussain S, McPeak G.",Disaster Med Public Health Prep. 2024 May 10;18:e103. doi: 10.1017/dmp.2024.109.,Dobolyi K,Disaster Med Public Health Prep,2024,2024/05/10,,,10.1017/dmp.2024.109,"OBJECTIVE: Not all scientific publications are equally useful to policy-makers tasked with mitigating the spread and impact of diseases, especially at the start of novel epidemics and pandemics. The urgent need for actionable, evidence-based information is paramount, but the nature of preprint and peer-reviewed articles published during these times is often at odds with such goals. For example, a lack of novel results and a focus on opinions rather than evidence were common in coronavirus disease (COVID-19) publications at the start of the pandemic in 2019. In this work, we seek to automatically judge the utility of these scientific articles, from a public health policy making persepctive, using only their titles.
METHODS: Deep learning natural language processing (NLP) models were trained on scientific COVID-19 publication titles from the CORD-19 dataset and evaluated against expert-curated COVID-19 evidence to measure their real-world feasibility at screening these scientific publications in an automated manner.
RESULTS: This work demonstrates that it is possible to judge the utility of COVID-19 scientific articles, from a public health policy-making perspective, based on their title alone, using deep natural language processing (NLP) models.
CONCLUSIONS: NLP models can be successfully trained on scienticic articles and used by public health experts to triage and filter the hundreds of new daily publications on novel diseases such as COVID-19 at the start of pandemics.","Predicting the Utility of Scientific Articles for Emerging Pandemics Using Their Titles and Natural Language Processing OBJECTIVE: Not all scientific publications are equally useful to policy-makers tasked with mitigating the spread and impact of diseases, especially at the start of novel epidemics and pandemics. The urgent need for actionable, evidence-based information is paramount, but the nature of preprint and peer-reviewed articles published during these times is often at odds with such goals. For example, a lack of novel results and a focus on opinions rather than evidence were common in coronavirus disease (COVID-19) publications at the start of the pandemic in 2019. In this work, we seek to automatically judge the utility of these scientific articles, from a public health policy making persepctive, using only their titles.
METHODS: Deep learning natural language processing (NLP) models were trained on scientific COVID-19 publication titles from the CORD-19 dataset and evaluated against expert-curated COVID-19 evidence to measure their real-world feasibility at screening these scientific publications in an automated manner.
RESULTS: This work demonstrates that it is possible to judge the utility of COVID-19 scientific articles, from a public health policy-making perspective, based on their title alone, using deep natural language processing (NLP) models.
CONCLUSIONS: NLP models can be successfully trained on scienticic articles and used by public health experts to triage and filter the hundreds of new daily publications on novel diseases such as COVID-19 at the start of pandemics.","[ 0.01682331  0.42317817  0.10808963 ...  0.7079448  -0.23018193
 -0.24513261]",0.9187313,0.8077242,Both,"deep learning, natural language processing, NLP, language processing"
38711448,Mpox-AISM: AI-mediated super monitoring for mpox and like-mpox,"Yue Y, Jiang M, Zhang X, Xu J, Ye H, Zhang F, Li Z, Li Y.",iScience. 2024 Apr 17;27(5):109766. doi: 10.1016/j.isci.2024.109766. eCollection 2024 May 17.,Yue Y,iScience,2024,2024/05/07,PMC11070687,,10.1016/j.isci.2024.109766,"Swift and accurate diagnosis for earlier-stage monkeypox (mpox) patients is crucial to avoiding its spread. However, the similarities between common skin disorders and mpox and the need for professional diagnosis unavoidably impaired the diagnosis of earlier-stage mpox patients and contributed to mpox outbreak. To address the challenge, we proposed ""Super Monitoring"", a real-time visualization technique employing artificial intelligence (AI) and Internet technology to diagnose earlier-stage mpox cheaply, conveniently, and quickly. Concretely, AI-mediated ""Super Monitoring"" (mpox-AISM) integrates deep learning models, data augmentation, self-supervised learning, and cloud services. According to publicly accessible datasets, mpox-AISM's Precision, Recall, Specificity, and F1-score in diagnosing mpox reach 99.3%, 94.1%, 99.9%, and 96.6%, respectively, and it achieves 94.51% accuracy in diagnosing mpox, six like-mpox skin disorders, and normal skin. With the Internet and communication terminal, mpox-AISM has the potential to perform real-time and accurate diagnosis for earlier-stage mpox in real-world scenarios, thereby preventing mpox outbreak.","Mpox-AISM: AI-mediated super monitoring for mpox and like-mpox Swift and accurate diagnosis for earlier-stage monkeypox (mpox) patients is crucial to avoiding its spread. However, the similarities between common skin disorders and mpox and the need for professional diagnosis unavoidably impaired the diagnosis of earlier-stage mpox patients and contributed to mpox outbreak. To address the challenge, we proposed ""Super Monitoring"", a real-time visualization technique employing artificial intelligence (AI) and Internet technology to diagnose earlier-stage mpox cheaply, conveniently, and quickly. Concretely, AI-mediated ""Super Monitoring"" (mpox-AISM) integrates deep learning models, data augmentation, self-supervised learning, and cloud services. According to publicly accessible datasets, mpox-AISM's Precision, Recall, Specificity, and F1-score in diagnosing mpox reach 99.3%, 94.1%, 99.9%, and 96.6%, respectively, and it achieves 94.51% accuracy in diagnosing mpox, six like-mpox skin disorders, and normal skin. With the Internet and communication terminal, mpox-AISM has the potential to perform real-time and accurate diagnosis for earlier-stage mpox in real-world scenarios, thereby preventing mpox outbreak.","[ 0.14380975  0.6461689   0.3712745  ...  0.45082736  0.05502928
 -0.40965173]",0.93729377,0.81986845,Computer Vision,deep learning
38710671,DeepARV: ensemble deep learning to predict drug-drug interaction of clinical relevance with antiretroviral therapy,"Pham T, Ghafoor M, Grañana-Castillo S, Marzolini C, Gibbons S, Khoo S, Chiong J, Wang D, Siccardi M.",NPJ Syst Biol Appl. 2024 May 6;10(1):48. doi: 10.1038/s41540-024-00374-0.,Pham T,NPJ Syst Biol Appl,2024,2024/05/06,PMC11074332,,10.1038/s41540-024-00374-0,"Drug-drug interaction (DDI) may result in clinical toxicity or treatment failure of antiretroviral therapy (ARV) or comedications. Despite the high number of possible drug combinations, only a limited number of clinical DDI studies are conducted. Computational prediction of DDIs could provide key evidence for the rational management of complex therapies. Our study aimed to assess the potential of deep learning approaches to predict DDIs of clinical relevance between ARVs and comedications. DDI severity grading between 30,142 drug pairs was extracted from the Liverpool HIV Drug Interaction database. Two feature construction techniques were employed: 1) drug similarity profiles by comparing Morgan fingerprints, and 2) embeddings from SMILES of each drug via ChemBERTa, a transformer-based model. We developed DeepARV-Sim and DeepARV-ChemBERTa to predict four categories of DDI: i) Red: drugs should not be co-administered, ii) Amber: interaction of potential clinical relevance manageable by monitoring/dose adjustment, iii) Yellow: interaction of weak relevance and iv) Green: no expected interaction. The imbalance in the distribution of DDI severity grades was addressed by undersampling and applying ensemble learning. DeepARV-Sim and DeepARV-ChemBERTa predicted clinically relevant DDI between ARVs and comedications with a weighted mean balanced accuracy of 0.729 ± 0.012 and 0.776 ± 0.011, respectively. DeepARV-Sim and DeepARV-ChemBERTa have the potential to leverage molecular structures associated with DDI risks and reduce DDI class imbalance, effectively increasing the predictive ability on clinically relevant DDIs. This approach could be developed for identifying high-risk pairing of drugs, enhancing the screening process, and targeting DDIs to study in clinical drug development.","DeepARV: ensemble deep learning to predict drug-drug interaction of clinical relevance with antiretroviral therapy Drug-drug interaction (DDI) may result in clinical toxicity or treatment failure of antiretroviral therapy (ARV) or comedications. Despite the high number of possible drug combinations, only a limited number of clinical DDI studies are conducted. Computational prediction of DDIs could provide key evidence for the rational management of complex therapies. Our study aimed to assess the potential of deep learning approaches to predict DDIs of clinical relevance between ARVs and comedications. DDI severity grading between 30,142 drug pairs was extracted from the Liverpool HIV Drug Interaction database. Two feature construction techniques were employed: 1) drug similarity profiles by comparing Morgan fingerprints, and 2) embeddings from SMILES of each drug via ChemBERTa, a transformer-based model. We developed DeepARV-Sim and DeepARV-ChemBERTa to predict four categories of DDI: i) Red: drugs should not be co-administered, ii) Amber: interaction of potential clinical relevance manageable by monitoring/dose adjustment, iii) Yellow: interaction of weak relevance and iv) Green: no expected interaction. The imbalance in the distribution of DDI severity grades was addressed by undersampling and applying ensemble learning. DeepARV-Sim and DeepARV-ChemBERTa predicted clinically relevant DDI between ARVs and comedications with a weighted mean balanced accuracy of 0.729 ± 0.012 and 0.776 ± 0.011, respectively. DeepARV-Sim and DeepARV-ChemBERTa have the potential to leverage molecular structures associated with DDI risks and reduce DDI class imbalance, effectively increasing the predictive ability on clinically relevant DDIs. This approach could be developed for identifying high-risk pairing of drugs, enhancing the screening process, and targeting DDIs to study in clinical drug development.","[-0.20369144  0.59963197  0.41451612 ...  0.35409606 -0.21789874
 -0.12139209]",0.9099737,0.80980486,Other,"deep learning, transformer, transformer-based model"
38676257,COVID-19 Hierarchical Classification Using a Deep Learning Multi-Modal,"Althenayan AS, AlSalamah SA, Aly S, Nouh T, Mahboub B, Salameh L, Alkubeyyer M, Mirza A.",Sensors (Basel). 2024 Apr 20;24(8):2641. doi: 10.3390/s24082641.,Althenayan AS,Sensors (Basel),2024,2024/04/27,PMC11053684,,10.3390/s24082641,"Coronavirus disease 2019 (COVID-19), originating in China, has rapidly spread worldwide. Physicians must examine infected patients and make timely decisions to isolate them. However, completing these processes is difficult due to limited time and availability of expert radiologists, as well as limitations of the reverse-transcription polymerase chain reaction (RT-PCR) method. Deep learning, a sophisticated machine learning technique, leverages radiological imaging modalities for disease diagnosis and image classification tasks. Previous research on COVID-19 classification has encountered several limitations, including binary classification methods, single-feature modalities, small public datasets, and reliance on CT diagnostic processes. Additionally, studies have often utilized a flat structure, disregarding the hierarchical structure of pneumonia classification. This study aims to overcome these limitations by identifying pneumonia caused by COVID-19, distinguishing it from other types of pneumonia and healthy lungs using chest X-ray (CXR) images and related tabular medical data, and demonstrate the value of incorporating tabular medical data in achieving more accurate diagnoses. Resnet-based and VGG-based pre-trained convolutional neural network (CNN) models were employed to extract features, which were then combined using early fusion for the classification of eight distinct classes. We leveraged the hierarchal structure of pneumonia classification within our approach to achieve improved classification outcomes. Since an imbalanced dataset is common in this field, a variety of versions of generative adversarial networks (GANs) were used to generate synthetic data. The proposed approach tested in our private datasets of 4523 patients achieved a macro-avg F1-score of 95.9% and an F1-score of 87.5% for COVID-19 identification using a Resnet-based structure. In conclusion, in this study, we were able to create an accurate deep learning multi-modal to diagnose COVID-19 and differentiate it from other kinds of pneumonia and normal lungs, which will enhance the radiological diagnostic process.","COVID-19 Hierarchical Classification Using a Deep Learning Multi-Modal Coronavirus disease 2019 (COVID-19), originating in China, has rapidly spread worldwide. Physicians must examine infected patients and make timely decisions to isolate them. However, completing these processes is difficult due to limited time and availability of expert radiologists, as well as limitations of the reverse-transcription polymerase chain reaction (RT-PCR) method. Deep learning, a sophisticated machine learning technique, leverages radiological imaging modalities for disease diagnosis and image classification tasks. Previous research on COVID-19 classification has encountered several limitations, including binary classification methods, single-feature modalities, small public datasets, and reliance on CT diagnostic processes. Additionally, studies have often utilized a flat structure, disregarding the hierarchical structure of pneumonia classification. This study aims to overcome these limitations by identifying pneumonia caused by COVID-19, distinguishing it from other types of pneumonia and healthy lungs using chest X-ray (CXR) images and related tabular medical data, and demonstrate the value of incorporating tabular medical data in achieving more accurate diagnoses. Resnet-based and VGG-based pre-trained convolutional neural network (CNN) models were employed to extract features, which were then combined using early fusion for the classification of eight distinct classes. We leveraged the hierarchal structure of pneumonia classification within our approach to achieve improved classification outcomes. Since an imbalanced dataset is common in this field, a variety of versions of generative adversarial networks (GANs) were used to generate synthetic data. The proposed approach tested in our private datasets of 4523 patients achieved a macro-avg F1-score of 95.9% and an F1-score of 87.5% for COVID-19 identification using a Resnet-based structure. In conclusion, in this study, we were able to create an accurate deep learning multi-modal to diagnose COVID-19 and differentiate it from other kinds of pneumonia and normal lungs, which will enhance the radiological diagnostic process.","[ 0.04294083  0.53382623  0.13866958 ...  0.69190633 -0.18227224
 -0.639333  ]",0.9106127,0.8021985,Computer Vision,"neural network, convolutional neural network, CNN, deep learning"
38598563,DeepDynaForecast: Phylogenetic-informed graph deep learning for epidemic transmission dynamic prediction,"Sun C, Fang R, Salemi M, Prosperi M, Rife Magalis B.",PLoS Comput Biol. 2024 Apr 10;20(4):e1011351. doi: 10.1371/journal.pcbi.1011351. eCollection 2024 Apr.,Sun C,PLoS Comput Biol,2024,2024/04/10,PMC11034642,,10.1371/journal.pcbi.1011351,"In the midst of an outbreak or sustained epidemic, reliable prediction of transmission risks and patterns of spread is critical to inform public health programs. Projections of transmission growth or decline among specific risk groups can aid in optimizing interventions, particularly when resources are limited. Phylogenetic trees have been widely used in the detection of transmission chains and high-risk populations. Moreover, tree topology and the incorporation of population parameters (phylodynamics) can be useful in reconstructing the evolutionary dynamics of an epidemic across space and time among individuals. We now demonstrate the utility of phylodynamic trees for transmission modeling and forecasting, developing a phylogeny-based deep learning system, referred to as DeepDynaForecast. Our approach leverages a primal-dual graph learning structure with shortcut multi-layer aggregation, which is suited for the early identification and prediction of transmission dynamics in emerging high-risk groups. We demonstrate the accuracy of DeepDynaForecast using simulated outbreak data and the utility of the learned model using empirical, large-scale data from the human immunodeficiency virus epidemic in Florida between 2012 and 2020. Our framework is available as open-source software (MIT license) at github.com/lab-smile/DeepDynaForcast.","DeepDynaForecast: Phylogenetic-informed graph deep learning for epidemic transmission dynamic prediction In the midst of an outbreak or sustained epidemic, reliable prediction of transmission risks and patterns of spread is critical to inform public health programs. Projections of transmission growth or decline among specific risk groups can aid in optimizing interventions, particularly when resources are limited. Phylogenetic trees have been widely used in the detection of transmission chains and high-risk populations. Moreover, tree topology and the incorporation of population parameters (phylodynamics) can be useful in reconstructing the evolutionary dynamics of an epidemic across space and time among individuals. We now demonstrate the utility of phylodynamic trees for transmission modeling and forecasting, developing a phylogeny-based deep learning system, referred to as DeepDynaForecast. Our approach leverages a primal-dual graph learning structure with shortcut multi-layer aggregation, which is suited for the early identification and prediction of transmission dynamics in emerging high-risk groups. We demonstrate the accuracy of DeepDynaForecast using simulated outbreak data and the utility of the learned model using empirical, large-scale data from the human immunodeficiency virus epidemic in Florida between 2012 and 2020. Our framework is available as open-source software (MIT license) at github.com/lab-smile/DeepDynaForcast.","[-0.00450633  0.49702057  0.22749266 ...  0.9677708  -0.3594464
 -0.43439847]",0.9111571,0.8049661,Other,deep learning
38456661,Demeter - a Risk Mitigation Tool for Agriculture Workers,"Martin J, Seward T, Mintas D, Wanke R.",J Agromedicine. 2024 Jul;29(3):508-510. doi: 10.1080/1059924X.2024.2326556. Epub 2024 Mar 8.,Martin J,J Agromedicine,2024,2024/03/08,,,10.1080/1059924X.2024.2326556,"The agriculture industry lacks novel techniques for analyzing risks facing its workers. Although injuries are common in this field, existing datasets and tools are insufficient for risk assessment and mitigation for two primary reasons: they provide neither immediate nor long-term risk mitigation advice, and they do not account for hazards which fluctuate daily. The purpose of Demeter is to collect safety data about hazards on farms and produce risk analysis and mitigation reports. This application uses a combination of formula-based risk calculations and state-of-the-art graph neural networks (GNNs) to perform risk analysis and reduction. The formula-based risk calculations had a mean absolute error (MAE) of 0.2110, and the GNN had an accuracy of 94.9%, a precision of 0.3521, and a recall of 0.8333. Demeter has the potential to reduce the number of injuries and fatalities among agriculture workers by alerting them to risks present in their daily workflow and suggesting safety precautions.","Demeter - a Risk Mitigation Tool for Agriculture Workers The agriculture industry lacks novel techniques for analyzing risks facing its workers. Although injuries are common in this field, existing datasets and tools are insufficient for risk assessment and mitigation for two primary reasons: they provide neither immediate nor long-term risk mitigation advice, and they do not account for hazards which fluctuate daily. The purpose of Demeter is to collect safety data about hazards on farms and produce risk analysis and mitigation reports. This application uses a combination of formula-based risk calculations and state-of-the-art graph neural networks (GNNs) to perform risk analysis and reduction. The formula-based risk calculations had a mean absolute error (MAE) of 0.2110, and the GNN had an accuracy of 94.9%, a precision of 0.3521, and a recall of 0.8333. Demeter has the potential to reduce the number of injuries and fatalities among agriculture workers by alerting them to risks present in their daily workflow and suggesting safety precautions.","[-0.02930052  0.37378964 -0.09693728 ...  0.7434231   0.00300446
 -0.03429749]",0.91559607,0.8040154,Both,neural network
38412074,An Automated Analysis Framework for Epidemiological Survey on COVID-19,"Lin Z, Lin X, Yang X.",IEEE J Biomed Health Inform. 2024 May;28(5):3186-3199. doi: 10.1109/JBHI.2024.3370253. Epub 2024 May 6.,Lin Z,IEEE J Biomed Health Inform,2024,2024/02/27,,,10.1109/JBHI.2024.3370253,"For a long time, the prevention and control of COVID-19 has received significant attention. A crucial aspect of controlling the disease's spread is the epidemiological survey of patients and the subsequent analysis of epidemiological survey reports (case reports). However, current mainstream analysis approaches are all made manually. This manual method is time-consuming and manpower-intensive. This paper designs an automated visual epidemiological survey analysis (AVESA) framework for the epidemiological survey on COVID-19. AVESA designs a deep neural network for information extraction from case reports and automatically constructs an epidemiological knowledge graph based on predefined pattern. Moreover, a multi-dimensional knowledge reasoning model is developed for conducting knowledge reasoning in the complete COVID-19 epidemiological knowledge graph. In the entity extraction sub-task and multi-task extraction sub-task, AVESA achieved F1 scores of 85.12% and 92.29% respectively on the constructed dataset, significantly outperforming the standalone information extraction models. In full-graph computing, all three experiments align closely with manual analysis standards. In the risk analysis experiment, the weighted PageRank algorithm showed an average improvement of 11.21% in Top_Recall_n% over the standard PageRank algorithm. In the community detection experiment, the weighted Louvain algorithm showed a mere 4.34% community difference rate compared to manual analysis.","An Automated Analysis Framework for Epidemiological Survey on COVID-19 For a long time, the prevention and control of COVID-19 has received significant attention. A crucial aspect of controlling the disease's spread is the epidemiological survey of patients and the subsequent analysis of epidemiological survey reports (case reports). However, current mainstream analysis approaches are all made manually. This manual method is time-consuming and manpower-intensive. This paper designs an automated visual epidemiological survey analysis (AVESA) framework for the epidemiological survey on COVID-19. AVESA designs a deep neural network for information extraction from case reports and automatically constructs an epidemiological knowledge graph based on predefined pattern. Moreover, a multi-dimensional knowledge reasoning model is developed for conducting knowledge reasoning in the complete COVID-19 epidemiological knowledge graph. In the entity extraction sub-task and multi-task extraction sub-task, AVESA achieved F1 scores of 85.12% and 92.29% respectively on the constructed dataset, significantly outperforming the standalone information extraction models. In full-graph computing, all three experiments align closely with manual analysis standards. In the risk analysis experiment, the weighted PageRank algorithm showed an average improvement of 11.21% in Top_Recall_n% over the standard PageRank algorithm. In the community detection experiment, the weighted Louvain algorithm showed a mere 4.34% community difference rate compared to manual analysis.","[ 0.13782632  0.39638677  0.05495868 ...  0.47293139 -0.2116451
 -0.40410727]",0.9004575,0.82270795,Both,neural network
38305899,Analysing the impact of comorbid conditions and media coverage on online symptom search data: a novel AI-based approach for COVID-19 tracking,"Lyu S, Adegboye O, Adhinugraha KM, Emeto TI, Taniar D.",Infect Dis (Lond). 2024 May;56(5):348-358. doi: 10.1080/23744235.2024.2311281. Epub 2024 Feb 2.,Lyu S,Infect Dis (Lond),2024,2024/02/02,,,10.1080/23744235.2024.2311281,"BACKGROUND: Web search data have proven to bea valuable early indicator of COVID-19 outbreaks. However, the influence of co-morbid conditions with similar symptoms and the effect of media coverage on symptom-related searches are often overlooked, leading to potential inaccuracies in COVID-19 simulations.
METHOD: This study introduces a machine learning-based approach to estimate the magnitude of the impact of media coverage and comorbid conditions with similar symptoms on online symptom searches, based on two scenarios with quantile levels 10-90 and 25-75. An incremental batch learning RNN-LSTM model was then developed for the COVID-19 simulation in Australia and New Zealand, allowing the model to dynamically simulate different infection rates and transmissibility of SARS-CoV-2 variants.
RESULT: The COVID-19 infected person-directed symptom searches were found to account for only a small proportion of the total search volume (on average 33.68% in Australia vs. 36.89% in New Zealand) compared to searches influenced by media coverage and comorbid conditions (on average 44.88% in Australia vs. 50.94% in New Zealand). The proposed method, which incorporates estimated symptom component ratios into the RNN-LSTM embedding model, significantly improved COVID-19 simulation performance.
CONCLUSION: Media coverage and comorbid conditions with similar symptoms dominate the total number of online symptom searches, suggesting that direct use of online symptom search data in COVID-19 simulations may overestimate COVID-19 infections. Our approach provides new insights into the accurate estimation of COVID-19 infections using online symptom searches, thereby assisting governments in developing complementary methods for public health surveillance.","Analysing the impact of comorbid conditions and media coverage on online symptom search data: a novel AI-based approach for COVID-19 tracking BACKGROUND: Web search data have proven to bea valuable early indicator of COVID-19 outbreaks. However, the influence of co-morbid conditions with similar symptoms and the effect of media coverage on symptom-related searches are often overlooked, leading to potential inaccuracies in COVID-19 simulations.
METHOD: This study introduces a machine learning-based approach to estimate the magnitude of the impact of media coverage and comorbid conditions with similar symptoms on online symptom searches, based on two scenarios with quantile levels 10-90 and 25-75. An incremental batch learning RNN-LSTM model was then developed for the COVID-19 simulation in Australia and New Zealand, allowing the model to dynamically simulate different infection rates and transmissibility of SARS-CoV-2 variants.
RESULT: The COVID-19 infected person-directed symptom searches were found to account for only a small proportion of the total search volume (on average 33.68% in Australia vs. 36.89% in New Zealand) compared to searches influenced by media coverage and comorbid conditions (on average 44.88% in Australia vs. 50.94% in New Zealand). The proposed method, which incorporates estimated symptom component ratios into the RNN-LSTM embedding model, significantly improved COVID-19 simulation performance.
CONCLUSION: Media coverage and comorbid conditions with similar symptoms dominate the total number of online symptom searches, suggesting that direct use of online symptom search data in COVID-19 simulations may overestimate COVID-19 infections. Our approach provides new insights into the accurate estimation of COVID-19 infections using online symptom searches, thereby assisting governments in developing complementary methods for public health surveillance.","[ 0.1808242   0.54334277  0.41485417 ...  0.78936404  0.09412733
 -0.24131495]",0.9062817,0.8299269,Other,"RNN, LSTM"
37858448,Using neural networks to calibrate agent based models enables improved regional evidence for vaccine strategy and policy,"Chopra A, Rodriguez A, Prakash BA, Raskar R, Kingsley T.",Vaccine. 2023 Nov 22;41(48):7067-7071. doi: 10.1016/j.vaccine.2023.08.060. Epub 2023 Oct 17.,Chopra A,Vaccine,2023,2023/10/20,,,10.1016/j.vaccine.2023.08.060,"Distribution and administration strategy are critical to successful population immunization efforts. Agent-based modeling (ABM) can reflect the complexity of real-world populations and can experimentally evaluate vaccine strategy and policy. However, ABMs historically have been limited in their time-to-development, long runtime, and difficulty calibrating. Our team had several technical advances in the development of our GradABMs: a novel class of scalable, fast and differentiable simulations. GradABMs can simulate million-size populations in a few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous sources. This allows for rapid and real-world sensitivity analyses. Our first epidemiological GradABM (EpiABMv1) enabled simulation interventions over real million-scale populations and was used in vaccine strategy and policy during the COVID-19 pandemic. Literature suggests decisions aided by evidence from these models saved thousands of lives. Our most recent model (EpiABMv2) extends EpiABMv1 to allow improved regional calibration using deep neural networks to incorporate local population data, and in some cases different policy recommendations versus our prior models. This is an important advance for our model to be more effective at vaccine strategy and policy decisions at the local public health level.","Using neural networks to calibrate agent based models enables improved regional evidence for vaccine strategy and policy Distribution and administration strategy are critical to successful population immunization efforts. Agent-based modeling (ABM) can reflect the complexity of real-world populations and can experimentally evaluate vaccine strategy and policy. However, ABMs historically have been limited in their time-to-development, long runtime, and difficulty calibrating. Our team had several technical advances in the development of our GradABMs: a novel class of scalable, fast and differentiable simulations. GradABMs can simulate million-size populations in a few seconds on commodity hardware, integrate with deep neural networks and ingest heterogeneous sources. This allows for rapid and real-world sensitivity analyses. Our first epidemiological GradABM (EpiABMv1) enabled simulation interventions over real million-scale populations and was used in vaccine strategy and policy during the COVID-19 pandemic. Literature suggests decisions aided by evidence from these models saved thousands of lives. Our most recent model (EpiABMv2) extends EpiABMv1 to allow improved regional calibration using deep neural networks to incorporate local population data, and in some cases different policy recommendations versus our prior models. This is an important advance for our model to be more effective at vaccine strategy and policy decisions at the local public health level.","[-0.05008514  0.76618975  0.28187945 ...  0.8369156  -0.1199268
 -0.40370318]",0.91495466,0.81865835,Both,"neural network, deep neural networks"
37516796,An innovative ensemble model based on deep learning for predicting COVID-19 infection,"Su X, Sun Y, Liu H, Lang Q, Zhang Y, Zhang J, Wang C, Chen Y.",Sci Rep. 2023 Jul 29;13(1):12322. doi: 10.1038/s41598-023-39408-8.,Su X,Sci Rep,2023,2023/07/29,PMC10387055,,10.1038/s41598-023-39408-8,"Nowadays, global public health crises are occurring more frequently, and accurate prediction of these diseases can reduce the burden on the healthcare system. Taking COVID-19 as an example, accurate prediction of infection can assist experts in effectively allocating medical resources and diagnosing diseases. Currently, scholars worldwide use single model approaches or epidemiology models more often to predict the outbreak trend of COVID-19, resulting in poor prediction accuracy. Although a few studies have employed ensemble models, there is still room for improvement in their performance. In addition, there are only a few models that use the laboratory results of patients to predict COVID-19 infection. To address these issues, research efforts should focus on improving disease prediction performance and expanding the use of medical disease prediction models. In this paper, we propose an innovative deep learning model Whale Optimization Convolutional Neural Networks (CNN), Long-Short Term Memory (LSTM) and Artificial Neural Network (ANN) called WOCLSA which incorporates three models ANN, CNN and LSTM. The WOCLSA model utilizes the Whale Optimization Algorithm to optimize the neuron number, dropout and batch size parameters in the integrated model of ANN, CNN and LSTM, thereby finding the global optimal solution parameters. WOCLSA employs 18 patient indicators as predictors, and compares its results with three other ensemble deep learning models. All models were validated with train-test split approaches. We evaluate and compare our proposed model and other models using accuracy, F1 score, recall, AUC and precision metrics. Through many studies and tests, our results show that our prediction models can identify patients with COVID-19 infection at the AUC of 91%, 91%, and 93% respectively. Other prediction results achieve a respectable accuracy of 92.82%, 92.79%, and 91.66% respectively, f1-score of 93.41%, 92.79%, and 92.33% respectively, precision of 93.41%, 92.79%, and 92.33% respectively, recall of 93.41%, 92.79%, and 92.33% respectively. All of these exceed 91%, surpassing those of comparable models. The execution time of WOCLSA is also an advantage. Therefore, the WOCLSA ensemble model can be used to assist in verifying laboratory research results and predict and to judge various diseases in public health events.","An innovative ensemble model based on deep learning for predicting COVID-19 infection Nowadays, global public health crises are occurring more frequently, and accurate prediction of these diseases can reduce the burden on the healthcare system. Taking COVID-19 as an example, accurate prediction of infection can assist experts in effectively allocating medical resources and diagnosing diseases. Currently, scholars worldwide use single model approaches or epidemiology models more often to predict the outbreak trend of COVID-19, resulting in poor prediction accuracy. Although a few studies have employed ensemble models, there is still room for improvement in their performance. In addition, there are only a few models that use the laboratory results of patients to predict COVID-19 infection. To address these issues, research efforts should focus on improving disease prediction performance and expanding the use of medical disease prediction models. In this paper, we propose an innovative deep learning model Whale Optimization Convolutional Neural Networks (CNN), Long-Short Term Memory (LSTM) and Artificial Neural Network (ANN) called WOCLSA which incorporates three models ANN, CNN and LSTM. The WOCLSA model utilizes the Whale Optimization Algorithm to optimize the neuron number, dropout and batch size parameters in the integrated model of ANN, CNN and LSTM, thereby finding the global optimal solution parameters. WOCLSA employs 18 patient indicators as predictors, and compares its results with three other ensemble deep learning models. All models were validated with train-test split approaches. We evaluate and compare our proposed model and other models using accuracy, F1 score, recall, AUC and precision metrics. Through many studies and tests, our results show that our prediction models can identify patients with COVID-19 infection at the AUC of 91%, 91%, and 93% respectively. Other prediction results achieve a respectable accuracy of 92.82%, 92.79%, and 91.66% respectively, f1-score of 93.41%, 92.79%, and 92.33% respectively, precision of 93.41%, 92.79%, and 92.33% respectively, recall of 93.41%, 92.79%, and 92.33% respectively. All of these exceed 91%, surpassing those of comparable models. The execution time of WOCLSA is also an advantage. Therefore, the WOCLSA ensemble model can be used to assist in verifying laboratory research results and predict and to judge various diseases in public health events.","[ 0.03300571  0.5893426   0.4586219  ...  0.61146814 -0.05205217
 -0.6274602 ]",0.9191079,0.8157083,Computer Vision,"neural network, artificial neural network, convolutional neural network, CNN, LSTM, deep learning"
37354142,ConvCoroNet: a deep convolutional neural network optimized with iterative thresholding algorithm for Covid-19 detection using chest X-ray images,"Merrouchi M, Benyoussef Y, Skittou M, Atifi K, Gadi T.",J Biomol Struct Dyn. 2024 Jul;42(11):5699-5712. doi: 10.1080/07391102.2023.2227726. Epub 2023 Jun 24.,Merrouchi M,J Biomol Struct Dyn,2024,2023/06/24,,,10.1080/07391102.2023.2227726,"Covid-19 is a global pandemic. Early and accurate detection of positive cases prevent the further spread of this epidemic and help to treat rapidly the infected patients. During the peak of this epidemic, there was an insufficiency of Covid-19 test kits. In addition, this technique takes a considerable time in the diagnosis. Hence the need to find fast, accurate and low-cost method to replace or supplement RT PCR-based methods. Covid-19 is a respiratory disease, chest X-ray images are often used to diagnose pneumonia. From this perspective, these images can play an important role in the Covid-19 detection. In this article, we propose ConvCoroNet, a deep convolutional neural network model optimized with new method based on iterative thresholding algorithm to detect coronavirus automatically from chest X-ray images. ConvCoroNet is trained on a dataset prepared by collecting chest X-ray images of Covid-19, pneumonia and normal cases from publically datasets. The experimental results of our proposed model show a high accuracy of 99.50%, sensitivity of 98.80% and specificity of 99.85% when detecting Covid-19 from chest X-ray images. ConvCoroNet achieves promising results in the automatic detection of Covid-19 from chest X-ray images. It may be able to help radiologists in the Covid-19 detection by reducing the examination time of X-ray images.Communicated by Ramaswamy H. Sarma.","ConvCoroNet: a deep convolutional neural network optimized with iterative thresholding algorithm for Covid-19 detection using chest X-ray images Covid-19 is a global pandemic. Early and accurate detection of positive cases prevent the further spread of this epidemic and help to treat rapidly the infected patients. During the peak of this epidemic, there was an insufficiency of Covid-19 test kits. In addition, this technique takes a considerable time in the diagnosis. Hence the need to find fast, accurate and low-cost method to replace or supplement RT PCR-based methods. Covid-19 is a respiratory disease, chest X-ray images are often used to diagnose pneumonia. From this perspective, these images can play an important role in the Covid-19 detection. In this article, we propose ConvCoroNet, a deep convolutional neural network model optimized with new method based on iterative thresholding algorithm to detect coronavirus automatically from chest X-ray images. ConvCoroNet is trained on a dataset prepared by collecting chest X-ray images of Covid-19, pneumonia and normal cases from publically datasets. The experimental results of our proposed model show a high accuracy of 99.50%, sensitivity of 98.80% and specificity of 99.85% when detecting Covid-19 from chest X-ray images. ConvCoroNet achieves promising results in the automatic detection of Covid-19 from chest X-ray images. It may be able to help radiologists in the Covid-19 detection by reducing the examination time of X-ray images.Communicated by Ramaswamy H. Sarma.","[ 0.37745476  0.29443222  0.13708825 ...  0.5372956   0.07695762
 -0.4318886 ]",0.9244008,0.8060385,Other,"neural network, convolutional neural network"
37161130,Data augmentation based semi-supervised method to improve COVID-19 CT classification,"Chen X, Bai Y, Wang P, Luo J.",Math Biosci Eng. 2023 Feb 6;20(4):6838-6852. doi: 10.3934/mbe.2023294.,Chen X,Math Biosci Eng,2023,2023/05/10,,,10.3934/mbe.2023294,"The Coronavirus (COVID-19) outbreak of December 2019 has become a serious threat to people around the world, creating a health crisis that infected millions of lives, as well as destroying the global economy. Early detection and diagnosis are essential to prevent further transmission. The detection of COVID-19 computed tomography images is one of the important approaches to rapid diagnosis. Many different branches of deep learning methods have played an important role in this area, including transfer learning, contrastive learning, ensemble strategy, etc. However, these works require a large number of samples of expensive manual labels, so in order to save costs, scholars adopted semi-supervised learning that applies only a few labels to classify COVID-19 CT images. Nevertheless, the existing semi-supervised methods focus primarily on class imbalance and pseudo-label filtering rather than on pseudo-label generation. Accordingly, in this paper, we organized a semi-supervised classification framework based on data augmentation to classify the CT images of COVID-19. We revised the classic teacher-student framework and introduced the popular data augmentation method Mixup, which widened the distribution of high confidence to improve the accuracy of selected pseudo-labels and ultimately obtain a model with better performance. For the COVID-CT dataset, our method makes precision, F1 score, accuracy and specificity 21.04%, 12.95%, 17.13% and 38.29% higher than average values for other methods respectively, For the SARS-COV-2 dataset, these increases were 8.40%, 7.59%, 9.35% and 12.80% respectively. For the Harvard Dataverse dataset, growth was 17.64%, 18.89%, 19.81% and 20.20% respectively. The codes are available at https://github.com/YutingBai99/COVID-19-SSL.","Data augmentation based semi-supervised method to improve COVID-19 CT classification The Coronavirus (COVID-19) outbreak of December 2019 has become a serious threat to people around the world, creating a health crisis that infected millions of lives, as well as destroying the global economy. Early detection and diagnosis are essential to prevent further transmission. The detection of COVID-19 computed tomography images is one of the important approaches to rapid diagnosis. Many different branches of deep learning methods have played an important role in this area, including transfer learning, contrastive learning, ensemble strategy, etc. However, these works require a large number of samples of expensive manual labels, so in order to save costs, scholars adopted semi-supervised learning that applies only a few labels to classify COVID-19 CT images. Nevertheless, the existing semi-supervised methods focus primarily on class imbalance and pseudo-label filtering rather than on pseudo-label generation. Accordingly, in this paper, we organized a semi-supervised classification framework based on data augmentation to classify the CT images of COVID-19. We revised the classic teacher-student framework and introduced the popular data augmentation method Mixup, which widened the distribution of high confidence to improve the accuracy of selected pseudo-labels and ultimately obtain a model with better performance. For the COVID-CT dataset, our method makes precision, F1 score, accuracy and specificity 21.04%, 12.95%, 17.13% and 38.29% higher than average values for other methods respectively, For the SARS-COV-2 dataset, these increases were 8.40%, 7.59%, 9.35% and 12.80% respectively. For the Harvard Dataverse dataset, growth was 17.64%, 18.89%, 19.81% and 20.20% respectively. The codes are available at https://github.com/YutingBai99/COVID-19-SSL.","[-0.01670801  0.54373354  0.17109914 ...  0.49198318 -0.14819402
 -0.44646123]",0.9129509,0.8416127,Computer Vision,deep learning
36962770,"What If…? Pandemic policy-decision-support to guide a cost-benefit-optimised, country-specific response","Mannarini G, Posa F, Bossy T, Massemin L, Fernandez-Castanon J, Chavdarova T, Cañas P, Gupta P, Jaggi M, Hartley MA.",PLOS Glob Public Health. 2022 Aug 24;2(8):e0000721. doi: 10.1371/journal.pgph.0000721. eCollection 2022.,Mannarini G,PLOS Glob Public Health,2022,2023/03/24,PMC10022143,,10.1371/journal.pgph.0000721,"BACKGROUND: After 18 months of responding to the COVID-19 pandemic, there is still no agreement on the optimal combination of mitigation strategies. The efficacy and collateral damage of pandemic policies are dependent on constantly evolving viral epidemiology as well as the volatile distribution of socioeconomic and cultural factors. This study proposes a data-driven approach to quantify the efficacy of the type, duration, and stringency of COVID-19 mitigation policies in terms of transmission control and economic loss, personalised to individual countries.
METHODS: We present What If…?, a deep learning pandemic-policy-decision-support algorithm simulating pandemic scenarios to guide and evaluate policy impact in real time. It leverages a uniquely diverse live global data-stream of socioeconomic, demographic, climatic, and epidemic trends on over a year of data (04/2020-06/2021) from 116 countries. The economic damage of the policies is also evaluated on the 29 higher income countries for which data is available. The efficacy and economic damage estimates are derived from two neural networks that infer respectively the daily R-value (RE) and unemployment rate (UER). Reinforcement learning then pits these models against each other to find the optimal policies minimising both RE and UER.
FINDINGS: The models made high accuracy predictions of RE and UER (average mean squared errors of 0.043 [CI95: 0.042-0.044] and 4.473% [CI95: 2.619-6.326] respectively), which allow the computation of country-specific policy efficacy in terms of cost and benefit. In the 29 countries where economic information was available, the reinforcement learning agent suggested a policy mix that is predicted to outperform those implemented in reality by over 10-fold for RE reduction (0.250 versus 0.025) and at 28-fold less cost in terms of UER (1.595% versus 0.057%).
CONCLUSION: These results show that deep learning has the potential to guide evidence-based understanding and implementation of public health policies.","What If…? Pandemic policy-decision-support to guide a cost-benefit-optimised, country-specific response BACKGROUND: After 18 months of responding to the COVID-19 pandemic, there is still no agreement on the optimal combination of mitigation strategies. The efficacy and collateral damage of pandemic policies are dependent on constantly evolving viral epidemiology as well as the volatile distribution of socioeconomic and cultural factors. This study proposes a data-driven approach to quantify the efficacy of the type, duration, and stringency of COVID-19 mitigation policies in terms of transmission control and economic loss, personalised to individual countries.
METHODS: We present What If…?, a deep learning pandemic-policy-decision-support algorithm simulating pandemic scenarios to guide and evaluate policy impact in real time. It leverages a uniquely diverse live global data-stream of socioeconomic, demographic, climatic, and epidemic trends on over a year of data (04/2020-06/2021) from 116 countries. The economic damage of the policies is also evaluated on the 29 higher income countries for which data is available. The efficacy and economic damage estimates are derived from two neural networks that infer respectively the daily R-value (RE) and unemployment rate (UER). Reinforcement learning then pits these models against each other to find the optimal policies minimising both RE and UER.
FINDINGS: The models made high accuracy predictions of RE and UER (average mean squared errors of 0.043 [CI95: 0.042-0.044] and 4.473% [CI95: 2.619-6.326] respectively), which allow the computation of country-specific policy efficacy in terms of cost and benefit. In the 29 countries where economic information was available, the reinforcement learning agent suggested a policy mix that is predicted to outperform those implemented in reality by over 10-fold for RE reduction (0.250 versus 0.025) and at 28-fold less cost in terms of UER (1.595% versus 0.057%).
CONCLUSION: These results show that deep learning has the potential to guide evidence-based understanding and implementation of public health policies.","[-0.19771597  0.3969547   0.29622987 ...  0.6392926   0.2457498
 -0.2873424 ]",0.9094144,0.80902797,Other,"neural network, deep learning"
36914765,The predictive model for COVID-19 pandemic plastic pollution by using deep learning method,"Nanehkaran YA, Licai Z, Azarafza M, Talaei S, Jinxia X, Chen J, Derakhshani R.",Sci Rep. 2023 Mar 13;13(1):4126. doi: 10.1038/s41598-023-31416-y.,Nanehkaran YA,Sci Rep,2023,2023/03/14,PMC10009853,,10.1038/s41598-023-31416-y,"Pandemic plastics (e.g., masks, gloves, aprons, and sanitizer bottles) are global consequences of COVID-19 pandemic-infected waste, which has increased significantly throughout the world. These hazardous wastes play an important role in environmental pollution and indirectly spread COVID-19. Predicting the environmental impacts of these wastes can be used to provide situational management, conduct control procedures, and reduce the COVID-19 effects. In this regard, the presented study attempted to provide a deep learning-based predictive model for forecasting the expansion of the pandemic plastic in the megacities of Iran. As a methodology, a database was gathered from February 27, 2020, to October 10, 2021, for COVID-19 spread and personal protective equipment usage in this period. The dataset was trained and validated using training (80%) and testing (20%) datasets by a deep neural network (DNN) procedure to forecast pandemic plastic pollution. Performance of the DNN-based model is controlled by the confusion matrix, receiver operating characteristic (ROC) curve, and justified by the k-nearest neighbours, decision tree, random forests, support vector machines, Gaussian naïve Bayes, logistic regression, and multilayer perceptron methods. According to the comparative modelling results, the DNN-based model was found to predict more accurately than other methods and have a significant predominance over others with a lower errors rate (MSE = 0.024, RMSE = 0.027, MAPE = 0.025). The ROC curve analysis results (overall accuracy) indicate the DNN model (AUC = 0.929) had the highest score among others.","The predictive model for COVID-19 pandemic plastic pollution by using deep learning method Pandemic plastics (e.g., masks, gloves, aprons, and sanitizer bottles) are global consequences of COVID-19 pandemic-infected waste, which has increased significantly throughout the world. These hazardous wastes play an important role in environmental pollution and indirectly spread COVID-19. Predicting the environmental impacts of these wastes can be used to provide situational management, conduct control procedures, and reduce the COVID-19 effects. In this regard, the presented study attempted to provide a deep learning-based predictive model for forecasting the expansion of the pandemic plastic in the megacities of Iran. As a methodology, a database was gathered from February 27, 2020, to October 10, 2021, for COVID-19 spread and personal protective equipment usage in this period. The dataset was trained and validated using training (80%) and testing (20%) datasets by a deep neural network (DNN) procedure to forecast pandemic plastic pollution. Performance of the DNN-based model is controlled by the confusion matrix, receiver operating characteristic (ROC) curve, and justified by the k-nearest neighbours, decision tree, random forests, support vector machines, Gaussian naïve Bayes, logistic regression, and multilayer perceptron methods. According to the comparative modelling results, the DNN-based model was found to predict more accurately than other methods and have a significant predominance over others with a lower errors rate (MSE = 0.024, RMSE = 0.027, MAPE = 0.025). The ROC curve analysis results (overall accuracy) indicate the DNN model (AUC = 0.929) had the highest score among others.","[ 0.04699433  0.41934103  0.19226405 ...  0.5050154   0.20286322
 -0.7366643 ]",0.9214614,0.81201595,Computer Vision,"neural network, multilayer perceptron, deep learning"
36913401,Deep learning models for hepatitis E incidence prediction leveraging meteorological factors,"Feng Y, Cui X, Lv J, Yan B, Meng X, Zhang L, Guo Y.",PLoS One. 2023 Mar 13;18(3):e0282928. doi: 10.1371/journal.pone.0282928. eCollection 2023.,Feng Y,PLoS One,2023,2023/03/13,PMC10010535,,10.1371/journal.pone.0282928,"BACKGROUND: Infectious diseases are a major threat to public health, causing serious medical consumption and casualties. Accurate prediction of infectious diseases incidence is of great significance for public health organizations to prevent the spread of diseases. However, only using historical incidence data for prediction can not get good results. This study analyzes the influence of meteorological factors on the incidence of hepatitis E, which are used to improve the accuracy of incidence prediction.
METHODS: We extracted the monthly meteorological data, incidence and cases number of hepatitis E from January 2005 to December 2017 in Shandong province, China. We employ GRA method to analyze the correlation between the incidence and meteorological factors. With these meteorological factors, we achieve a variety of methods for incidence of hepatitis E by LSTM and attention-based LSTM. We selected data from July 2015 to December 2017 to validate the models, and the rest was taken as training set. Three metrics were applied to compare the performance of models, including root mean square error(RMSE), mean absolute percentage error(MAPE) and mean absolute error(MAE).
RESULTS: Duration of sunshine and rainfall-related factors(total rainfall, maximum daily rainfall) are more relevant to the incidence of hepatitis E than other factors. Without meteorological factors, we obtained 20.74%, 19.50% for incidence in term of MAPE, by LSTM and A-LSTM, respectively. With meteorological factors, we obtained 14.74%, 12.91%, 13.21%, 16.83% for incidence, in term of MAPE, by LSTM-All, MA-LSTM-All, TA-LSTM-All, BiA-LSTM-All, respectively. The prediction accuracy increased by 7.83%. Without meteorological factors, we achieved 20.41%, 19.39% for cases in term of MAPE, by LSTM and A-LSTM, respectively. With meteorological factors, we achieved 14.20%, 12.49%, 12.72%, 15.73% for cases, in term of MAPE, by LSTM-All, MA-LSTM-All, TA-LSTM-All, BiA-LSTM-All, respectively. The prediction accuracy increased by 7.92%. More detailed results are shown in results section of this paper.
CONCLUSIONS: The experiments show that attention-based LSTM is superior to other comparative models. Multivariate attention and temporal attention can greatly improve the prediction performance of the models. Among them, when all meteorological factors are used, multivariate attention performance is better. This study can provide reference for the prediction of other infectious diseases.","Deep learning models for hepatitis E incidence prediction leveraging meteorological factors BACKGROUND: Infectious diseases are a major threat to public health, causing serious medical consumption and casualties. Accurate prediction of infectious diseases incidence is of great significance for public health organizations to prevent the spread of diseases. However, only using historical incidence data for prediction can not get good results. This study analyzes the influence of meteorological factors on the incidence of hepatitis E, which are used to improve the accuracy of incidence prediction.
METHODS: We extracted the monthly meteorological data, incidence and cases number of hepatitis E from January 2005 to December 2017 in Shandong province, China. We employ GRA method to analyze the correlation between the incidence and meteorological factors. With these meteorological factors, we achieve a variety of methods for incidence of hepatitis E by LSTM and attention-based LSTM. We selected data from July 2015 to December 2017 to validate the models, and the rest was taken as training set. Three metrics were applied to compare the performance of models, including root mean square error(RMSE), mean absolute percentage error(MAPE) and mean absolute error(MAE).
RESULTS: Duration of sunshine and rainfall-related factors(total rainfall, maximum daily rainfall) are more relevant to the incidence of hepatitis E than other factors. Without meteorological factors, we obtained 20.74%, 19.50% for incidence in term of MAPE, by LSTM and A-LSTM, respectively. With meteorological factors, we obtained 14.74%, 12.91%, 13.21%, 16.83% for incidence, in term of MAPE, by LSTM-All, MA-LSTM-All, TA-LSTM-All, BiA-LSTM-All, respectively. The prediction accuracy increased by 7.83%. Without meteorological factors, we achieved 20.41%, 19.39% for cases in term of MAPE, by LSTM and A-LSTM, respectively. With meteorological factors, we achieved 14.20%, 12.49%, 12.72%, 15.73% for cases, in term of MAPE, by LSTM-All, MA-LSTM-All, TA-LSTM-All, BiA-LSTM-All, respectively. The prediction accuracy increased by 7.92%. More detailed results are shown in results section of this paper.
CONCLUSIONS: The experiments show that attention-based LSTM is superior to other comparative models. Multivariate attention and temporal attention can greatly improve the prediction performance of the models. Among them, when all meteorological factors are used, multivariate attention performance is better. This study can provide reference for the prediction of other infectious diseases.","[-0.30319777  0.3298858   0.3923349  ...  0.49289942  0.14040394
 -0.24163945]",0.90822065,0.8051756,Other,"LSTM, deep learning"
36621078,A deep learning method to detect opioid prescription and opioid use disorder from electronic health records,"Kashyap A, Callison-Burch C, Boland MR.",Int J Med Inform. 2023 Mar;171:104979. doi: 10.1016/j.ijmedinf.2022.104979. Epub 2022 Dec 31.,Kashyap A,Int J Med Inform,2023,2023/01/09,PMC9898169,NIHMS1863451,10.1016/j.ijmedinf.2022.104979,"OBJECTIVE: As the opioid epidemic continues across the United States, methods are needed to accurately and quickly identify patients at risk for opioid use disorder (OUD). The purpose of this study is to develop two predictive algorithms: one to predict opioid prescription and one to predict OUD.
MATERIALS AND METHODS: We developed an informatics algorithm that trains two deep learning models over patient Electronic Health Records (EHRs) using the MIMIC-III database. We utilize both the structured and unstructured parts of the EHR and show that it is possible to predict both challenging outcomes.
RESULTS: Our deep learning models incorporate elements from EHRs to predict opioid prescription with an F1-score of 0.88 ± 0.003 and an AUC-ROC of 0.93 ± 0.002. We also constructed a model to predict OUD diagnosis achieving an F1-score of 0.82 ± 0.05 and AUC-ROC of 0.94 ± 0.008.
DISCUSSION: Our model for OUD prediction outperformed prior algorithms for specificity, F1 score and AUC-ROC while achieving equivalent sensitivity. This demonstrates the importance of a) deep learning approaches in predicting OUD and b) incorporating both structured and unstructured data for this prediction task. No prediction models for opioid prescription as an outcome were found in the literature and therefore our model is the first to predict opioid prescribing behavior.
CONCLUSION: Algorithms such as those described in this paper will become increasingly important to understand the drivers underlying this national epidemic.","A deep learning method to detect opioid prescription and opioid use disorder from electronic health records OBJECTIVE: As the opioid epidemic continues across the United States, methods are needed to accurately and quickly identify patients at risk for opioid use disorder (OUD). The purpose of this study is to develop two predictive algorithms: one to predict opioid prescription and one to predict OUD.
MATERIALS AND METHODS: We developed an informatics algorithm that trains two deep learning models over patient Electronic Health Records (EHRs) using the MIMIC-III database. We utilize both the structured and unstructured parts of the EHR and show that it is possible to predict both challenging outcomes.
RESULTS: Our deep learning models incorporate elements from EHRs to predict opioid prescription with an F1-score of 0.88 ± 0.003 and an AUC-ROC of 0.93 ± 0.002. We also constructed a model to predict OUD diagnosis achieving an F1-score of 0.82 ± 0.05 and AUC-ROC of 0.94 ± 0.008.
DISCUSSION: Our model for OUD prediction outperformed prior algorithms for specificity, F1 score and AUC-ROC while achieving equivalent sensitivity. This demonstrates the importance of a) deep learning approaches in predicting OUD and b) incorporating both structured and unstructured data for this prediction task. No prediction models for opioid prescription as an outcome were found in the literature and therefore our model is the first to predict opioid prescribing behavior.
CONCLUSION: Algorithms such as those described in this paper will become increasingly important to understand the drivers underlying this national epidemic.","[-0.00878171  0.4947725   0.5651576  ...  0.370914   -0.00495967
 -0.48057297]",0.90868354,0.80683774,Other,deep learning
36505893,"Predictive, preventive, and personalized management of retinal fluid via computer-aided detection app for optical coherence tomography scans","Quek TC, Takahashi K, Kang HG, Thakur S, Deshmukh M, Tseng RMWW, Nguyen H, Tham YC, Rim TH, Kim SS, Yanagi Y, Liew G, Cheng CY.",EPMA J. 2022 Nov 19;13(4):547-560. doi: 10.1007/s13167-022-00301-5. eCollection 2022 Dec.,Quek TC,EPMA J,2022,2022/12/12,PMC9727042,,10.1007/s13167-022-00301-5,"AIMS: Computer-aided detection systems for retinal fluid could be beneficial for disease monitoring and management by chronic age-related macular degeneration (AMD) and diabetic retinopathy (DR) patients, to assist in disease prevention via early detection before the disease progresses to a ""wet AMD"" pathology or diabetic macular edema (DME), requiring treatment. We propose a proof-of-concept AI-based app to help predict fluid via a ""fluid score"", prevent fluid progression, and provide personalized, serial monitoring, in the context of predictive, preventive, and personalized medicine (PPPM) for patients at risk of retinal fluid complications.
METHODS: The app comprises a convolutional neural network-Vision Transformer (CNN-ViT)-based segmentation deep learning (DL) network, trained on a small dataset of 100 training images (augmented to 992 images) from the Singapore Epidemiology of Eye Diseases (SEED) study, together with a CNN-based classification network trained on 8497 images, that can detect fluid vs. non-fluid optical coherence tomography (OCT) scans. Both networks are validated on external datasets.
RESULTS: Internal testing for our segmentation network produced an IoU score of 83.0% (95% CI = 76.7-89.3%) and a DICE score of 90.4% (86.3-94.4%); for external testing, we obtained an IoU score of 66.7% (63.5-70.0%) and a DICE score of 78.7% (76.0-81.4%). Internal testing of our classification network produced an area under the receiver operating characteristics curve (AUC) of 99.18%, and a Youden index threshold of 0.3806; for external testing, we obtained an AUC of 94.55%, and an accuracy of 94.98% and an F1 score of 85.73% with Youden index.
CONCLUSION: We have developed an AI-based app with an alternative transformer-based segmentation algorithm that could potentially be applied in the clinic with a PPPM approach for serial monitoring, and could allow for the generation of retrospective data to research into the varied use of treatments for AMD and DR. The modular system of our app can be scaled to add more iterative features based on user feedback for more efficient monitoring. Further study and scaling up of the algorithm dataset could potentially boost its usability in a real-world clinical setting.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s13167-022-00301-5.","Predictive, preventive, and personalized management of retinal fluid via computer-aided detection app for optical coherence tomography scans AIMS: Computer-aided detection systems for retinal fluid could be beneficial for disease monitoring and management by chronic age-related macular degeneration (AMD) and diabetic retinopathy (DR) patients, to assist in disease prevention via early detection before the disease progresses to a ""wet AMD"" pathology or diabetic macular edema (DME), requiring treatment. We propose a proof-of-concept AI-based app to help predict fluid via a ""fluid score"", prevent fluid progression, and provide personalized, serial monitoring, in the context of predictive, preventive, and personalized medicine (PPPM) for patients at risk of retinal fluid complications.
METHODS: The app comprises a convolutional neural network-Vision Transformer (CNN-ViT)-based segmentation deep learning (DL) network, trained on a small dataset of 100 training images (augmented to 992 images) from the Singapore Epidemiology of Eye Diseases (SEED) study, together with a CNN-based classification network trained on 8497 images, that can detect fluid vs. non-fluid optical coherence tomography (OCT) scans. Both networks are validated on external datasets.
RESULTS: Internal testing for our segmentation network produced an IoU score of 83.0% (95% CI = 76.7-89.3%) and a DICE score of 90.4% (86.3-94.4%); for external testing, we obtained an IoU score of 66.7% (63.5-70.0%) and a DICE score of 78.7% (76.0-81.4%). Internal testing of our classification network produced an area under the receiver operating characteristics curve (AUC) of 99.18%, and a Youden index threshold of 0.3806; for external testing, we obtained an AUC of 94.55%, and an accuracy of 94.98% and an F1 score of 85.73% with Youden index.
CONCLUSION: We have developed an AI-based app with an alternative transformer-based segmentation algorithm that could potentially be applied in the clinic with a PPPM approach for serial monitoring, and could allow for the generation of retrospective data to research into the varied use of treatments for AMD and DR. The modular system of our app can be scaled to add more iterative features based on user feedback for more efficient monitoring. Further study and scaling up of the algorithm dataset could potentially boost its usability in a real-world clinical setting.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s13167-022-00301-5.","[-0.08899184  0.46924457  0.16252388 ...  0.28492573  0.07170292
 -0.49197027]",0.901394,0.8028513,Other,"neural network, convolutional neural network, CNN, deep learning, transformer, vision transformer"
36302489,"Development, validation, and feature extraction of a deep learning model predicting in-hospital mortality using Japan's largest national ICU database: a validation framework for transparent clinical Artificial Intelligence (cAI) development","Ishii E, Nawa N, Hashimoto S, Shigemitsu H, Fujiwara T.",Anaesth Crit Care Pain Med. 2023 Apr;42(2):101167. doi: 10.1016/j.accpm.2022.101167. Epub 2022 Oct 24.,Ishii E,Anaesth Crit Care Pain Med,2023,2022/10/27,,,10.1016/j.accpm.2022.101167,"OBJECTIVE: While clinical Artificial Intelligence (cAI) mortality prediction models and relevant studies have increased, limitations including the lack of external validation studies and inadequate model calibration leading to decreased overall accuracy have been observed. To combat this, we developed and evaluated a novel deep neural network (DNN) and a validation framework to promote transparent cAI development.
METHODS: Data from Japan's largest ICU database was used to develop the DNN model, predicting in-hospital mortality including ICU and post-ICU mortality by days since ICU discharge. The most important variables to the model were extracted with SHapley Additive exPlanations (SHAP) to examine the DNN's efficacy as well as develop models that were also externally validated.
MAIN RESULTS: The area under the receiver operating characteristic curve (AUC) for predicting ICU mortality was 0.94 [0.93-0.95], and 0.91 [0.90-0.92] for in-hospital mortality, ranging between 0.91-0.95 throughout one year since ICU discharge. An external validation using only the top 20 variables resulted with higher AUCs than traditional severity scores.
CONCLUSIONS: Our DNN model consistently generated AUCs between 0.91-0.95 regardless of days since ICU discharge. The 20 most important variables to our DNN, also generated higher AUCs than traditional severity scores regardless of days since ICU discharge. To our knowledge, this is the first study that predicts ICU and in-hospital mortality using cAI by post-ICU discharge days up to over a year. This finding could contribute to increased transparency on cAI applications.","Development, validation, and feature extraction of a deep learning model predicting in-hospital mortality using Japan's largest national ICU database: a validation framework for transparent clinical Artificial Intelligence (cAI) development OBJECTIVE: While clinical Artificial Intelligence (cAI) mortality prediction models and relevant studies have increased, limitations including the lack of external validation studies and inadequate model calibration leading to decreased overall accuracy have been observed. To combat this, we developed and evaluated a novel deep neural network (DNN) and a validation framework to promote transparent cAI development.
METHODS: Data from Japan's largest ICU database was used to develop the DNN model, predicting in-hospital mortality including ICU and post-ICU mortality by days since ICU discharge. The most important variables to the model were extracted with SHapley Additive exPlanations (SHAP) to examine the DNN's efficacy as well as develop models that were also externally validated.
MAIN RESULTS: The area under the receiver operating characteristic curve (AUC) for predicting ICU mortality was 0.94 [0.93-0.95], and 0.91 [0.90-0.92] for in-hospital mortality, ranging between 0.91-0.95 throughout one year since ICU discharge. An external validation using only the top 20 variables resulted with higher AUCs than traditional severity scores.
CONCLUSIONS: Our DNN model consistently generated AUCs between 0.91-0.95 regardless of days since ICU discharge. The 20 most important variables to our DNN, also generated higher AUCs than traditional severity scores regardless of days since ICU discharge. To our knowledge, this is the first study that predicts ICU and in-hospital mortality using cAI by post-ICU discharge days up to over a year. This finding could contribute to increased transparency on cAI applications.","[-0.23331922  0.8441008   0.5118258  ...  0.29453707  0.19264667
 -0.7320002 ]",0.9074104,0.8007499,Other,"neural network, deep learning"
35991015,A graph convolutional network for predicting COVID-19 dynamics in 190 regions/countries,"Anno S, Hirakawa T, Sugita S, Yasumoto S.",Front Public Health. 2022 Aug 3;10:911336. doi: 10.3389/fpubh.2022.911336. eCollection 2022.,Anno S,Front Public Health,2022,2022/08/22,PMC9381970,,10.3389/fpubh.2022.911336,"INTRODUCTION: Coronavirus disease (COVID-19) rapidly spread from Wuhan, China to other parts of China and other regions/countries around the world, resulting in a pandemic due to large populations moving through the massive transport hubs connecting all regions of China via railways and a major international airport. COVID-19 will remain a threat until safe and effective vaccines and antiviral drugs have been developed, distributed, and administered on a global scale. Thus, there is urgent need to establish effective implementation of preemptive non-pharmaceutical interventions for appropriate prevention and control strategies, and predicting future COVID-19 cases is required to monitor and control the issue.
METHODS: This study attempts to utilize a three-layer graph convolutional network (GCN) model to predict future COVID-19 cases in 190 regions and countries using COVID-19 case data, commercial flight route data, and digital maps of public transportation in terms of transnational human mobility. We compared the performance of the proposed GCN model to a multilayer perceptron (MLP) model on a dataset of COVID-19 cases (excluding the graph representation). The prediction performance of the models was evaluated using the mean squared error.
RESULTS: Our results demonstrate that the proposed GCN model can achieve better graph utilization and performance compared to the baseline in terms of both prediction accuracy and stability.
DISCUSSION: The proposed GCN model is a useful means to predict COVID-19 cases at regional and national levels. Such predictions can be used to facilitate public health solutions in public health responses to the COVID-19 pandemic using deep learning and data pooling. In addition, the proposed GCN model may help public health policymakers in decision making in terms of epidemic prevention and control strategies.","A graph convolutional network for predicting COVID-19 dynamics in 190 regions/countries INTRODUCTION: Coronavirus disease (COVID-19) rapidly spread from Wuhan, China to other parts of China and other regions/countries around the world, resulting in a pandemic due to large populations moving through the massive transport hubs connecting all regions of China via railways and a major international airport. COVID-19 will remain a threat until safe and effective vaccines and antiviral drugs have been developed, distributed, and administered on a global scale. Thus, there is urgent need to establish effective implementation of preemptive non-pharmaceutical interventions for appropriate prevention and control strategies, and predicting future COVID-19 cases is required to monitor and control the issue.
METHODS: This study attempts to utilize a three-layer graph convolutional network (GCN) model to predict future COVID-19 cases in 190 regions and countries using COVID-19 case data, commercial flight route data, and digital maps of public transportation in terms of transnational human mobility. We compared the performance of the proposed GCN model to a multilayer perceptron (MLP) model on a dataset of COVID-19 cases (excluding the graph representation). The prediction performance of the models was evaluated using the mean squared error.
RESULTS: Our results demonstrate that the proposed GCN model can achieve better graph utilization and performance compared to the baseline in terms of both prediction accuracy and stability.
DISCUSSION: The proposed GCN model is a useful means to predict COVID-19 cases at regional and national levels. Such predictions can be used to facilitate public health solutions in public health responses to the COVID-19 pandemic using deep learning and data pooling. In addition, the proposed GCN model may help public health policymakers in decision making in terms of epidemic prevention and control strategies.","[ 0.06643031  0.4038444   0.38118693 ...  0.8357791  -0.09170325
 -0.4255567 ]",0.9215965,0.8026242,Other,"multilayer perceptron, deep learning"
35942743,Data driven time-varying SEIR-LSTM/GRU algorithms to track the spread of COVID-19,"Feng L, Chen Z, Jr HAL, Furati K, Khaliq A.",Math Biosci Eng. 2022 Jun 20;19(9):8935-8962. doi: 10.3934/mbe.2022415.,Feng L,Math Biosci Eng,2022,2022/08/09,,,10.3934/mbe.2022415,"COVID-19 is an infectious disease caused by a newly discovered coronavirus, which has become a worldwide pandemic greatly impacting our daily life and work. A large number of mathematical models, including the susceptible-exposed-infected-removed (SEIR) model and deep learning methods, such as long-short-term-memory (LSTM) and gated recurrent units (GRU)-based methods, have been employed for the analysis and prediction of the COVID-19 outbreak. This paper describes a SEIR-LSTM/GRU algorithm with time-varying parameters that can predict the number of active cases and removed cases in the US. Time-varying reproductive numbers that can illustrate the progress of the epidemic are also produced via this process. The investigation is based on the active cases and total cases data for the USA, as collected from the website ""Worldometer"". The root mean square error, mean absolute percentage error and r<sub>2</sub> score were utilized to assess the model's accuracy.","Data driven time-varying SEIR-LSTM/GRU algorithms to track the spread of COVID-19 COVID-19 is an infectious disease caused by a newly discovered coronavirus, which has become a worldwide pandemic greatly impacting our daily life and work. A large number of mathematical models, including the susceptible-exposed-infected-removed (SEIR) model and deep learning methods, such as long-short-term-memory (LSTM) and gated recurrent units (GRU)-based methods, have been employed for the analysis and prediction of the COVID-19 outbreak. This paper describes a SEIR-LSTM/GRU algorithm with time-varying parameters that can predict the number of active cases and removed cases in the US. Time-varying reproductive numbers that can illustrate the progress of the epidemic are also produced via this process. The investigation is based on the active cases and total cases data for the USA, as collected from the website ""Worldometer"". The root mean square error, mean absolute percentage error and r<sub>2</sub> score were utilized to assess the model's accuracy.","[ 0.07952585  0.531465    0.33217025 ...  0.38107738 -0.17774706
 -0.42958832]",0.9062718,0.81532484,Other,"LSTM, deep learning"
35885865,Enhanced Gravitational Search Optimization with Hybrid Deep Learning Model for COVID-19 Diagnosis on Epidemiology Data,"Ragab M, Choudhry H, H Asseri A, Binyamin SS, Al-Rabia MW.",Healthcare (Basel). 2022 Jul 19;10(7):1339. doi: 10.3390/healthcare10071339.,Ragab M,Healthcare (Basel),2022,2022/07/27,PMC9317045,,10.3390/healthcare10071339,"Effective screening provides efficient and quick diagnoses of COVID-19 and could alleviate related problems in the health care system. A prediction model that combines multiple features to assess contamination risks was established in the hope of supporting healthcare workers worldwide in triaging patients, particularly in situations with limited health care resources. Furthermore, a lack of diagnosis kits and asymptomatic cases can lead to missed or delayed diagnoses, exposing visitors, medical staff, and patients to 2019-nCoV contamination. Non-clinical techniques including data mining, expert systems, machine learning, and other artificial intelligence technologies have a crucial role to play in containment and diagnosis in the COVID-19 outbreak. This study developed Enhanced Gravitational Search Optimization with a Hybrid Deep Learning Model (EGSO-HDLM) for COVID-19 diagnoses using epidemiology data. The major aim of designing the EGSO-HDLM model was the identification and classification of COVID-19 using epidemiology data. In order to examine the epidemiology data, the EGSO-HDLM model employed a hybrid convolutional neural network with a gated recurrent unit based fusion (HCNN-GRUF) model. In addition, the hyperparameter optimization of the HCNN-GRUF model was improved by the use of the EGSO algorithm, which was derived by including the concepts of cat map and the traditional GSO algorithm. The design of the EGSO algorithm helps in reducing the ergodic problem, avoiding premature convergence, and enhancing algorithm efficiency. To demonstrate the better performance of the EGSO-HDLM model, experimental validation on a benchmark dataset was performed. The simulation results ensured the enhanced performance of the EGSO-HDLM model over recent approaches.","Enhanced Gravitational Search Optimization with Hybrid Deep Learning Model for COVID-19 Diagnosis on Epidemiology Data Effective screening provides efficient and quick diagnoses of COVID-19 and could alleviate related problems in the health care system. A prediction model that combines multiple features to assess contamination risks was established in the hope of supporting healthcare workers worldwide in triaging patients, particularly in situations with limited health care resources. Furthermore, a lack of diagnosis kits and asymptomatic cases can lead to missed or delayed diagnoses, exposing visitors, medical staff, and patients to 2019-nCoV contamination. Non-clinical techniques including data mining, expert systems, machine learning, and other artificial intelligence technologies have a crucial role to play in containment and diagnosis in the COVID-19 outbreak. This study developed Enhanced Gravitational Search Optimization with a Hybrid Deep Learning Model (EGSO-HDLM) for COVID-19 diagnoses using epidemiology data. The major aim of designing the EGSO-HDLM model was the identification and classification of COVID-19 using epidemiology data. In order to examine the epidemiology data, the EGSO-HDLM model employed a hybrid convolutional neural network with a gated recurrent unit based fusion (HCNN-GRUF) model. In addition, the hyperparameter optimization of the HCNN-GRUF model was improved by the use of the EGSO algorithm, which was derived by including the concepts of cat map and the traditional GSO algorithm. The design of the EGSO algorithm helps in reducing the ergodic problem, avoiding premature convergence, and enhancing algorithm efficiency. To demonstrate the better performance of the EGSO-HDLM model, experimental validation on a benchmark dataset was performed. The simulation results ensured the enhanced performance of the EGSO-HDLM model over recent approaches.","[ 0.12993787  0.5124886   0.33706164 ...  0.8166765  -0.07515461
 -0.5419925 ]",0.91138893,0.8195133,Both,"neural network, convolutional neural network, CNN, deep learning"
35812486,A Survey on Machine Learning and Internet of Medical Things-Based Approaches for Handling COVID-19: Meta-Analysis,"Band SS, Ardabili S, Yarahmadi A, Pahlevanzadeh B, Kiani AK, Beheshti A, Alinejad-Rokny H, Dehzangi I, Chang A, Mosavi A, Moslehpour M.",Front Public Health. 2022 Jun 23;10:869238. doi: 10.3389/fpubh.2022.869238. eCollection 2022.,Band SS,Front Public Health,2022,2022/07/11,PMC9260273,,10.3389/fpubh.2022.869238,"Early diagnosis, prioritization, screening, clustering, and tracking of patients with COVID-19, and production of drugs and vaccines are some of the applications that have made it necessary to use a new style of technology to involve, manage, and deal with this epidemic. Strategies backed by artificial intelligence (A.I.) and the Internet of Things (IoT) have been undeniably effective to understand how the virus works and prevent it from spreading. Accordingly, the main aim of this survey is to critically review the ML, IoT, and the integration of IoT and ML-based techniques in the applications related to COVID-19, from the diagnosis of the disease to the prediction of its outbreak. According to the main findings, IoT provided a prompt and efficient approach to tracking the disease spread. On the other hand, most of the studies developed by ML-based techniques aimed at the detection and handling of challenges associated with the COVID-19 pandemic. Among different approaches, Convolutional Neural Network (CNN), Support Vector Machine, Genetic CNN, and pre-trained CNN, followed by ResNet have demonstrated the best performances compared to other methods.","A Survey on Machine Learning and Internet of Medical Things-Based Approaches for Handling COVID-19: Meta-Analysis Early diagnosis, prioritization, screening, clustering, and tracking of patients with COVID-19, and production of drugs and vaccines are some of the applications that have made it necessary to use a new style of technology to involve, manage, and deal with this epidemic. Strategies backed by artificial intelligence (A.I.) and the Internet of Things (IoT) have been undeniably effective to understand how the virus works and prevent it from spreading. Accordingly, the main aim of this survey is to critically review the ML, IoT, and the integration of IoT and ML-based techniques in the applications related to COVID-19, from the diagnosis of the disease to the prediction of its outbreak. According to the main findings, IoT provided a prompt and efficient approach to tracking the disease spread. On the other hand, most of the studies developed by ML-based techniques aimed at the detection and handling of challenges associated with the COVID-19 pandemic. Among different approaches, Convolutional Neural Network (CNN), Support Vector Machine, Genetic CNN, and pre-trained CNN, followed by ResNet have demonstrated the best performances compared to other methods.","[ 0.36082813  0.43749845  0.5054039  ...  0.891866   -0.21249104
 -0.47205922]",0.9097715,0.80161124,Both,"neural network, convolutional neural network, CNN"
35753606,A knowledge graph embedding based approach to predict the adverse drug reactions using a deep neural network,"Joshi P, V M, Mukherjee A.",J Biomed Inform. 2022 Aug;132:104122. doi: 10.1016/j.jbi.2022.104122. Epub 2022 Jun 24.,Joshi P,J Biomed Inform,2022,2022/06/26,,,10.1016/j.jbi.2022.104122,"Recently Artificial Intelligence(AI) has not only been used to diagnose the disease but also to cure the disease. Researchers started using AI for drug discovery. Predicting the Adverse Drug Reactions(ADRs) caused by the drug in the manufacturing stage or in the clinical trial stage is a very important problem in drug discovery. ADRs have become a major concern resulting in injuries and also becoming fatal sometimes. Drug safety has gained much importance over the years propelling to the forefront investigation of predicting the ADRs. Although prior studies have queried diverse approaches to predict ADRs, very few were found to be effective. Also, the problem of having fewer reports makes the prediction of ADRs more difficult. To tackle this problem effectively, a novel method has been proposed in this paper. The proposed method is based on Knowledge Graph(KG) embedding. Using the KG embedding, we designed and trained a custom-made Deep Neural Network(DNN) called KGDNN(Knowledge Graph DNN) for predicting the ADRs. A KG has been constructed with 6 types of entities: drugs, ADRs, target proteins, indications, pathways, and genes. Using the Node2Vec algorithm, each node has been embedded into a feature space. Using those embeddings, the ADRs are classified by the KGDNN model. The proposed method has obtained an AUROC score of 0.917 and significantly outperformed the existing methods. Two case studies on drugs causing liver injury and COVID-19 recommended drugs have been performed to illustrate the model efficacy.","A knowledge graph embedding based approach to predict the adverse drug reactions using a deep neural network Recently Artificial Intelligence(AI) has not only been used to diagnose the disease but also to cure the disease. Researchers started using AI for drug discovery. Predicting the Adverse Drug Reactions(ADRs) caused by the drug in the manufacturing stage or in the clinical trial stage is a very important problem in drug discovery. ADRs have become a major concern resulting in injuries and also becoming fatal sometimes. Drug safety has gained much importance over the years propelling to the forefront investigation of predicting the ADRs. Although prior studies have queried diverse approaches to predict ADRs, very few were found to be effective. Also, the problem of having fewer reports makes the prediction of ADRs more difficult. To tackle this problem effectively, a novel method has been proposed in this paper. The proposed method is based on Knowledge Graph(KG) embedding. Using the KG embedding, we designed and trained a custom-made Deep Neural Network(DNN) called KGDNN(Knowledge Graph DNN) for predicting the ADRs. A KG has been constructed with 6 types of entities: drugs, ADRs, target proteins, indications, pathways, and genes. Using the Node2Vec algorithm, each node has been embedded into a feature space. Using those embeddings, the ADRs are classified by the KGDNN model. The proposed method has obtained an AUROC score of 0.917 and significantly outperformed the existing methods. Two case studies on drugs causing liver injury and COVID-19 recommended drugs have been performed to illustrate the model efficacy.","[-0.1422937   0.48216453  0.47506297 ...  0.6897525  -0.4230638
 -0.12561429]",0.90341777,0.8131669,Both,neural network
35681961,In the Seeking of Association between Air Pollutant and COVID-19 Confirmed Cases Using Deep Learning,"Tsan YT, Kristiani E, Liu PY, Chu WM, Yang CT.",Int J Environ Res Public Health. 2022 May 24;19(11):6373. doi: 10.3390/ijerph19116373.,Tsan YT,Int J Environ Res Public Health,2022,2022/06/10,PMC9180542,,10.3390/ijerph19116373,"The COVID-19 pandemic raises awareness of how the fatal spreading of infectious disease impacts economic, political, and cultural sectors, which causes social implications. Across the world, strategies aimed at quickly recognizing risk factors have also helped shape public health guidelines and direct resources; however, they are challenging to analyze and predict since those events still happen. This paper intends to invesitgate the association between air pollutants and COVID-19 confirmed cases using Deep Learning. We used Delhi, India, for daily confirmed cases and air pollutant data for the dataset. We used LSTM deep learning for training the combination of COVID-19 Confirmed Case and AQI parameters over the four different lag times of 1, 3, 7, and 14 days. The finding indicates that CO is the most excellent model compared with the others, having on average, 13 RMSE values. This was followed by pressure at 15, PM<sub>2.5</sub> at 20, NO<sub>2</sub> at 20, and O<sub>3</sub> at 22 error rates.","In the Seeking of Association between Air Pollutant and COVID-19 Confirmed Cases Using Deep Learning The COVID-19 pandemic raises awareness of how the fatal spreading of infectious disease impacts economic, political, and cultural sectors, which causes social implications. Across the world, strategies aimed at quickly recognizing risk factors have also helped shape public health guidelines and direct resources; however, they are challenging to analyze and predict since those events still happen. This paper intends to invesitgate the association between air pollutants and COVID-19 confirmed cases using Deep Learning. We used Delhi, India, for daily confirmed cases and air pollutant data for the dataset. We used LSTM deep learning for training the combination of COVID-19 Confirmed Case and AQI parameters over the four different lag times of 1, 3, 7, and 14 days. The finding indicates that CO is the most excellent model compared with the others, having on average, 13 RMSE values. This was followed by pressure at 15, PM<sub>2.5</sub> at 20, NO<sub>2</sub> at 20, and O<sub>3</sub> at 22 error rates.","[-0.2982103   0.40686688  0.20811403 ...  0.7740218   0.2717723
 -0.10853729]",0.9267401,0.8011514,Other,"LSTM, deep learning"
35591208,Deep Spatiotemporal Model for COVID-19 Forecasting,"Muñoz-Organero M, Queipo-Álvarez P.",Sensors (Basel). 2022 May 5;22(9):3519. doi: 10.3390/s22093519.,Muñoz-Organero M,Sensors (Basel),2022,2022/05/20,PMC9101138,,10.3390/s22093519,"COVID-19 has caused millions of infections and deaths over the last 2 years. Machine learning models have been proposed as an alternative to conventional epidemiologic models in an effort to optimize short- and medium-term forecasts that will help health authorities to optimize the use of policies and resources to tackle the spread of the SARS-CoV-2 virus. Although previous machine learning models based on time pattern analysis for COVID-19 sensed data have shown promising results, the spread of the virus has both spatial and temporal components. This manuscript proposes a new deep learning model that combines a time pattern extraction based on the use of a Long-Short Term Memory (LSTM) Recurrent Neural Network (RNN) over a preceding spatial analysis based on a Convolutional Neural Network (CNN) applied to a sequence of COVID-19 incidence images. The model has been validated with data from the 286 health primary care centers in the Comunidad de Madrid (Madrid region, Spain). The results show improved scores in terms of both root mean square error (RMSE) and explained variance (EV) when compared with previous models that have mainly focused on the temporal patterns and dependencies.","Deep Spatiotemporal Model for COVID-19 Forecasting COVID-19 has caused millions of infections and deaths over the last 2 years. Machine learning models have been proposed as an alternative to conventional epidemiologic models in an effort to optimize short- and medium-term forecasts that will help health authorities to optimize the use of policies and resources to tackle the spread of the SARS-CoV-2 virus. Although previous machine learning models based on time pattern analysis for COVID-19 sensed data have shown promising results, the spread of the virus has both spatial and temporal components. This manuscript proposes a new deep learning model that combines a time pattern extraction based on the use of a Long-Short Term Memory (LSTM) Recurrent Neural Network (RNN) over a preceding spatial analysis based on a Convolutional Neural Network (CNN) applied to a sequence of COVID-19 incidence images. The model has been validated with data from the 286 health primary care centers in the Comunidad de Madrid (Madrid region, Spain). The results show improved scores in terms of both root mean square error (RMSE) and explained variance (EV) when compared with previous models that have mainly focused on the temporal patterns and dependencies.","[-0.27610698  0.4075491   0.28949738 ...  0.7767202  -0.09086059
 -0.7701419 ]",0.90135443,0.8030771,Other,"neural network, machine learning model, convolutional neural network, recurrent neural network, CNN, RNN, LSTM, deep learning"
35534142,Artificial intelligence for forecasting and diagnosing COVID-19 pandemic: A focused review,"Comito C, Pizzuti C.",Artif Intell Med. 2022 Jun;128:102286. doi: 10.1016/j.artmed.2022.102286. Epub 2022 Mar 28.,Comito C,Artif Intell Med,2022,2022/05/09,PMC8958821,,10.1016/j.artmed.2022.102286,"The outbreak of novel corona virus 2019 (COVID-19) has been treated as a public health crisis of global concern by the World Health Organization (WHO). COVID-19 pandemic hugely affected countries worldwide raising the need to exploit novel, alternative and emerging technologies to respond to the emergency created by the weak health-care systems. In this context, Artificial Intelligence (AI) techniques can give a valid support to public health authorities, complementing traditional approaches with advanced tools. This study provides a comprehensive review of methods, algorithms, applications, and emerging AI technologies that can be utilized for forecasting and diagnosing COVID-19. The main objectives of this review are summarized as follows. (i) Understanding the importance of AI approaches such as machine learning and deep learning for COVID-19 pandemic; (ii) discussing the efficiency and impact of these methods for COVID-19 forecasting and diagnosing; (iii) providing an extensive background description of AI techniques to help non-expert to better catch the underlying concepts; (iv) for each work surveyed, give a detailed analysis of the rationale behind the approach, highlighting the method used, the type and size of data analyzed, the validation method, the target application and the results achieved; (v) focusing on some future challenges in COVID-19 forecasting and diagnosing.","Artificial intelligence for forecasting and diagnosing COVID-19 pandemic: A focused review The outbreak of novel corona virus 2019 (COVID-19) has been treated as a public health crisis of global concern by the World Health Organization (WHO). COVID-19 pandemic hugely affected countries worldwide raising the need to exploit novel, alternative and emerging technologies to respond to the emergency created by the weak health-care systems. In this context, Artificial Intelligence (AI) techniques can give a valid support to public health authorities, complementing traditional approaches with advanced tools. This study provides a comprehensive review of methods, algorithms, applications, and emerging AI technologies that can be utilized for forecasting and diagnosing COVID-19. The main objectives of this review are summarized as follows. (i) Understanding the importance of AI approaches such as machine learning and deep learning for COVID-19 pandemic; (ii) discussing the efficiency and impact of these methods for COVID-19 forecasting and diagnosing; (iii) providing an extensive background description of AI techniques to help non-expert to better catch the underlying concepts; (iv) for each work surveyed, give a detailed analysis of the rationale behind the approach, highlighting the method used, the type and size of data analyzed, the validation method, the target application and the results achieved; (v) focusing on some future challenges in COVID-19 forecasting and diagnosing.","[ 0.3602393   0.87022346  0.23333198 ...  0.7659081  -0.17602462
 -0.49535757]",0.91270375,0.8113838,Both,deep learning
35115652,Spatiotemporal sentiment variation analysis of geotagged COVID-19 tweets from India using a hybrid deep learning model,Kumar V.,Sci Rep. 2022 Feb 3;12(1):1849. doi: 10.1038/s41598-022-05974-6.,Kumar V,Sci Rep,2022,2022/02/04,PMC8814057,,10.1038/s41598-022-05974-6,"India is a hotspot of the COVID-19 crisis. During the first wave, several lockdowns (L) and gradual unlock (UL) phases were implemented by the government of India (GOI) to curb the virus spread. These phases witnessed many challenges and various day-to-day developments such as virus spread and resource management. Twitter, a social media platform, was extensively used by citizens to react to these events and related topics that varied temporally and geographically. Analyzing these variations can be a potent tool for informed decision-making. This paper attempts to capture these spatiotemporal variations of citizen reactions by predicting and analyzing the sentiments of geotagged tweets during L and UL phases. Various sentiment analysis based studies on the related subject have been done; however, its integration with location intelligence for decision making remains a research gap. The sentiments were predicted through a proposed hybrid Deep Learning (DL) model which leverages the strengths of BiLSTM and CNN model classes. The model was trained on a freely available Sentiment140 dataset and was tested over manually annotated COVID-19 related tweets from India. The model classified the tweets with high accuracy of around 90%, and analysis of geotagged tweets during L and UL phases reveal significant geographical variations. The findings as a decision support system can aid in analyzing citizen reactions toward the resources and events during an ongoing pandemic. The system can have various applications such as resource planning, crowd management, policy formulation, vaccination, prompt response, etc.","Spatiotemporal sentiment variation analysis of geotagged COVID-19 tweets from India using a hybrid deep learning model India is a hotspot of the COVID-19 crisis. During the first wave, several lockdowns (L) and gradual unlock (UL) phases were implemented by the government of India (GOI) to curb the virus spread. These phases witnessed many challenges and various day-to-day developments such as virus spread and resource management. Twitter, a social media platform, was extensively used by citizens to react to these events and related topics that varied temporally and geographically. Analyzing these variations can be a potent tool for informed decision-making. This paper attempts to capture these spatiotemporal variations of citizen reactions by predicting and analyzing the sentiments of geotagged tweets during L and UL phases. Various sentiment analysis based studies on the related subject have been done; however, its integration with location intelligence for decision making remains a research gap. The sentiments were predicted through a proposed hybrid Deep Learning (DL) model which leverages the strengths of BiLSTM and CNN model classes. The model was trained on a freely available Sentiment140 dataset and was tested over manually annotated COVID-19 related tweets from India. The model classified the tweets with high accuracy of around 90%, and analysis of geotagged tweets during L and UL phases reveal significant geographical variations. The findings as a decision support system can aid in analyzing citizen reactions toward the resources and events during an ongoing pandemic. The system can have various applications such as resource planning, crowd management, policy formulation, vaccination, prompt response, etc.","[-0.04045748  0.11080171  0.12620525 ...  0.92047054 -0.22595075
 -0.46681958]",0.90394187,0.80679834,Both,"CNN, LSTM, deep learning"
35072838,Detection of polypharmacy side effects by integrating multiple data sources and convolutional neural networks,"Lakizadeh A, Babaei M.",Mol Divers. 2022 Dec;26(6):3193-3203. doi: 10.1007/s11030-022-10382-z. Epub 2022 Jan 24.,Lakizadeh A,Mol Divers,2022,2022/01/24,,,10.1007/s11030-022-10382-z,"The consumption of drug combinations, named polypharmacy, is commonly used for treating patients with several diseases or those with complex conditions. However, the main drawback of polypharmacy is the increased probability of harmful side effects. The polypharmacy side effects are caused by an interaction between two medications. It means that the drug-drug interaction causes changes in their activities due to interfering in each other's performance. Therefore, discovering these side effects is one of the most challenging and important aspects of drug production and consumption as it is associated with human health. In this paper, a method has been introduced for predicting the polypharmacy side effects, called PSECNN. It is a multi-label multi-class deep learning method that combines various basic features of drugs to predict the polypharmacy side effects. Firstly, PSECNN collects five basic features of drugs, such as individual drug's side effects, drug-protein interactions, chemical substructures, targets, and enzymes in order to create a novel combination of drug features. A feature extraction module creates five feature vectors with the same dimension for each drug based on the Jaccard similarity index. Based on the feature vectors, a unique representative is then created for each drug. These representative vectors are given in pairs as input to the deep neural network to predict the occurrence probability of side effects. According to the experimental evaluations, PSECNN could outperform the state-of-the-art polypharmacy side effects prediction methods up to 74%. It has been found that PSECNN has better performance with polypharmacy side effects with a cause of molecular basis due to the novel combination of basic drug features.","Detection of polypharmacy side effects by integrating multiple data sources and convolutional neural networks The consumption of drug combinations, named polypharmacy, is commonly used for treating patients with several diseases or those with complex conditions. However, the main drawback of polypharmacy is the increased probability of harmful side effects. The polypharmacy side effects are caused by an interaction between two medications. It means that the drug-drug interaction causes changes in their activities due to interfering in each other's performance. Therefore, discovering these side effects is one of the most challenging and important aspects of drug production and consumption as it is associated with human health. In this paper, a method has been introduced for predicting the polypharmacy side effects, called PSECNN. It is a multi-label multi-class deep learning method that combines various basic features of drugs to predict the polypharmacy side effects. Firstly, PSECNN collects five basic features of drugs, such as individual drug's side effects, drug-protein interactions, chemical substructures, targets, and enzymes in order to create a novel combination of drug features. A feature extraction module creates five feature vectors with the same dimension for each drug based on the Jaccard similarity index. Based on the feature vectors, a unique representative is then created for each drug. These representative vectors are given in pairs as input to the deep neural network to predict the occurrence probability of side effects. According to the experimental evaluations, PSECNN could outperform the state-of-the-art polypharmacy side effects prediction methods up to 74%. It has been found that PSECNN has better performance with polypharmacy side effects with a cause of molecular basis due to the novel combination of basic drug features.","[-0.11387792  0.43492818  0.2977579  ...  0.56665796 -0.49121517
 -0.28498664]",0.90679365,0.80817974,Computer Vision,"neural network, convolutional neural network, CNN, deep learning"
35062101,A Deep Learning Program to Predict Acute Kidney Injury,Li X.,Stud Health Technol Inform. 2022 Jan 14;289:97-101. doi: 10.3233/SHTI210868.,Li X,Stud Health Technol Inform,2022,2022/01/22,,,10.3233/SHTI210868,"Acute kidney injury is a dangerous and sometime fatal clinical situation, which can cause irreversible damage. If we can predict it earlier and make appropriate prevention before its outbreak, kidney injury could be avoided. One challenge of early recognition of AKI is that the most e-alerts have focused on creatinine-based algorithms, but the elevation of serum creatinine lags behind renal injury. We use recurrent neural network (RNN) to make data mining on laboratory results of MIMIC-III Database. At first, we transfer the case data into Pandas DataFrame of series framed for supervised learning. Then we can use RNN predicts the next serum creatinine values (SCr) based on the last laboratory test results after emergency admissions. We train the RNN on whole dataset (i.e. multi-cases prediction) with LSTM. As the result shown, this prototype can predict criteria (SCr) of AKI with a RMSE (Root Mean Square Error) of 0.017mg/dL.","A Deep Learning Program to Predict Acute Kidney Injury Acute kidney injury is a dangerous and sometime fatal clinical situation, which can cause irreversible damage. If we can predict it earlier and make appropriate prevention before its outbreak, kidney injury could be avoided. One challenge of early recognition of AKI is that the most e-alerts have focused on creatinine-based algorithms, but the elevation of serum creatinine lags behind renal injury. We use recurrent neural network (RNN) to make data mining on laboratory results of MIMIC-III Database. At first, we transfer the case data into Pandas DataFrame of series framed for supervised learning. Then we can use RNN predicts the next serum creatinine values (SCr) based on the last laboratory test results after emergency admissions. We train the RNN on whole dataset (i.e. multi-cases prediction) with LSTM. As the result shown, this prototype can predict criteria (SCr) of AKI with a RMSE (Root Mean Square Error) of 0.017mg/dL.","[-0.3087731   0.7304803   0.34661943 ...  0.40380394 -0.1155599
 -0.52802515]",0.9130187,0.8019706,Other,"neural network, recurrent neural network, RNN, LSTM, deep learning"
35061767,Automated detection of COVID-19 through convolutional neural network using chest x-ray images,"Sarki R, Ahmed K, Wang H, Zhang Y, Wang K.",PLoS One. 2022 Jan 21;17(1):e0262052. doi: 10.1371/journal.pone.0262052. eCollection 2022.,Sarki R,PLoS One,2022,2022/01/21,PMC8782355,,10.1371/journal.pone.0262052,"The COVID-19 epidemic has a catastrophic impact on global well-being and public health. More than 27 million confirmed cases have been reported worldwide until now. Due to the growing number of confirmed cases, and challenges to the variations of the COVID-19, timely and accurate classification of healthy and infected patients is essential to control and treat COVID-19. We aim to develop a deep learning-based system for the persuasive classification and reliable detection of COVID-19 using chest radiography. Firstly, we evaluate the performance of various state-of-the-art convolutional neural networks (CNNs) proposed over recent years for medical image classification. Secondly, we develop and train CNN from scratch. In both cases, we use a public X-Ray dataset for training and validation purposes. For transfer learning, we obtain 100% accuracy for binary classification (i.e., Normal/COVID-19) and 87.50% accuracy for tertiary classification (Normal/COVID-19/Pneumonia). With the CNN trained from scratch, we achieve 93.75% accuracy for tertiary classification. In the case of transfer learning, the classification accuracy drops with the increased number of classes. The results are demonstrated by comprehensive receiver operating characteristics (ROC) and confusion metric analysis with 10-fold cross-validation.","Automated detection of COVID-19 through convolutional neural network using chest x-ray images The COVID-19 epidemic has a catastrophic impact on global well-being and public health. More than 27 million confirmed cases have been reported worldwide until now. Due to the growing number of confirmed cases, and challenges to the variations of the COVID-19, timely and accurate classification of healthy and infected patients is essential to control and treat COVID-19. We aim to develop a deep learning-based system for the persuasive classification and reliable detection of COVID-19 using chest radiography. Firstly, we evaluate the performance of various state-of-the-art convolutional neural networks (CNNs) proposed over recent years for medical image classification. Secondly, we develop and train CNN from scratch. In both cases, we use a public X-Ray dataset for training and validation purposes. For transfer learning, we obtain 100% accuracy for binary classification (i.e., Normal/COVID-19) and 87.50% accuracy for tertiary classification (Normal/COVID-19/Pneumonia). With the CNN trained from scratch, we achieve 93.75% accuracy for tertiary classification. In the case of transfer learning, the classification accuracy drops with the increased number of classes. The results are demonstrated by comprehensive receiver operating characteristics (ROC) and confusion metric analysis with 10-fold cross-validation.","[ 0.01548425  0.68318576  0.19984652 ...  0.6860159  -0.04856514
 -0.60753053]",0.9170329,0.8146523,Computer Vision,"neural network, convolutional neural network, CNN, deep learning"
34661658,Updates in deep learning research in ophthalmology,"Ng WY, Zhang S, Wang Z, Ong CJT, Gunasekeran DV, Lim GYS, Zheng F, Tan SCY, Tan GSW, Rim TH, Schmetterer L, Ting DSW.",Clin Sci (Lond). 2021 Oct 29;135(20):2357-2376. doi: 10.1042/CS20210207.,Ng WY,Clin Sci (Lond),2021,2021/10/18,,,10.1042/CS20210207,"Ophthalmology has been one of the early adopters of artificial intelligence (AI) within the medical field. Deep learning (DL), in particular, has garnered significant attention due to the availability of large amounts of data and digitized ocular images. Currently, AI in Ophthalmology is mainly focused on improving disease classification and supporting decision-making when treating ophthalmic diseases such as diabetic retinopathy, age-related macular degeneration (AMD), glaucoma and retinopathy of prematurity (ROP). However, most of the DL systems (DLSs) developed thus far remain in the research stage and only a handful are able to achieve clinical translation. This phenomenon is due to a combination of factors including concerns over security and privacy, poor generalizability, trust and explainability issues, unfavorable end-user perceptions and uncertain economic value. Overcoming this challenge would require a combination approach. Firstly, emerging techniques such as federated learning (FL), generative adversarial networks (GANs), autonomous AI and blockchain will be playing an increasingly critical role to enhance privacy, collaboration and DLS performance. Next, compliance to reporting and regulatory guidelines, such as CONSORT-AI and STARD-AI, will be required to in order to improve transparency, minimize abuse and ensure reproducibility. Thirdly, frameworks will be required to obtain patient consent, perform ethical assessment and evaluate end-user perception. Lastly, proper health economic assessment (HEA) must be performed to provide financial visibility during the early phases of DLS development. This is necessary to manage resources prudently and guide the development of DLS.","Updates in deep learning research in ophthalmology Ophthalmology has been one of the early adopters of artificial intelligence (AI) within the medical field. Deep learning (DL), in particular, has garnered significant attention due to the availability of large amounts of data and digitized ocular images. Currently, AI in Ophthalmology is mainly focused on improving disease classification and supporting decision-making when treating ophthalmic diseases such as diabetic retinopathy, age-related macular degeneration (AMD), glaucoma and retinopathy of prematurity (ROP). However, most of the DL systems (DLSs) developed thus far remain in the research stage and only a handful are able to achieve clinical translation. This phenomenon is due to a combination of factors including concerns over security and privacy, poor generalizability, trust and explainability issues, unfavorable end-user perceptions and uncertain economic value. Overcoming this challenge would require a combination approach. Firstly, emerging techniques such as federated learning (FL), generative adversarial networks (GANs), autonomous AI and blockchain will be playing an increasingly critical role to enhance privacy, collaboration and DLS performance. Next, compliance to reporting and regulatory guidelines, such as CONSORT-AI and STARD-AI, will be required to in order to improve transparency, minimize abuse and ensure reproducibility. Thirdly, frameworks will be required to obtain patient consent, perform ethical assessment and evaluate end-user perception. Lastly, proper health economic assessment (HEA) must be performed to provide financial visibility during the early phases of DLS development. This is necessary to manage resources prudently and guide the development of DLS.","[-0.02861917  0.44698843  0.6846062  ...  0.36524612 -0.21887966
 -0.35915956]",0.9027318,0.8156835,Both,deep learning
34412843,Data-based algorithms and models using diabetics real data for blood glucose and hypoglycaemia prediction - A systematic literature review,"Felizardo V, Garcia NM, Pombo N, Megdiche I.",Artif Intell Med. 2021 Aug;118:102120. doi: 10.1016/j.artmed.2021.102120. Epub 2021 May 28.,Felizardo V,Artif Intell Med,2021,2021/08/20,,,10.1016/j.artmed.2021.102120,"BACKGROUND AND AIM: Hypoglycaemia prediction play an important role in diabetes management being able to reduce the number of dangerous situations. Thus, it is relevant to present a systematic review on the currently available prediction algorithms and models for hypoglycaemia (or hypoglycemia in US English) prediction.
METHODS: This study aims to systematically review the literature on data-based algorithms and models using diabetics real data for hypoglycaemia prediction. Five electronic databases were screened for studies published from January 2014 to June 2020: ScienceDirect, IEEE Xplore, ACM Digital Library, SCOPUS, and PubMed.
RESULTS: Sixty-three eligible studies were retrieved that met the inclusion criteria. The review identifies the current trend in this topic: most of the studies perform short-term predictions (82.5%). Also, the review pinpoints the inputs and shows that information fusion is relevant for hypoglycaemia prediction. Regarding data-based models (80.9%) and hybrid models (19.1%) different predictive techniques are used: Artificial neural network (22.2%), ensemble learning (27.0%), supervised learning (20.6%), statistic/probabilistic (7.9%), autoregressive (7.9%), evolutionary (6.4%), deep learning (4.8%) and adaptative filter (3.2%). Artificial Neural networks and hybrid models show better results.
CONCLUSIONS: The data-based models for blood glucose and hypoglycaemia prediction should be able to provide a good balance between the applicability and performance, integrating complementary data from different sources or from different models. This review identifies trends and possible opportunities for research in this topic.","Data-based algorithms and models using diabetics real data for blood glucose and hypoglycaemia prediction - A systematic literature review BACKGROUND AND AIM: Hypoglycaemia prediction play an important role in diabetes management being able to reduce the number of dangerous situations. Thus, it is relevant to present a systematic review on the currently available prediction algorithms and models for hypoglycaemia (or hypoglycemia in US English) prediction.
METHODS: This study aims to systematically review the literature on data-based algorithms and models using diabetics real data for hypoglycaemia prediction. Five electronic databases were screened for studies published from January 2014 to June 2020: ScienceDirect, IEEE Xplore, ACM Digital Library, SCOPUS, and PubMed.
RESULTS: Sixty-three eligible studies were retrieved that met the inclusion criteria. The review identifies the current trend in this topic: most of the studies perform short-term predictions (82.5%). Also, the review pinpoints the inputs and shows that information fusion is relevant for hypoglycaemia prediction. Regarding data-based models (80.9%) and hybrid models (19.1%) different predictive techniques are used: Artificial neural network (22.2%), ensemble learning (27.0%), supervised learning (20.6%), statistic/probabilistic (7.9%), autoregressive (7.9%), evolutionary (6.4%), deep learning (4.8%) and adaptative filter (3.2%). Artificial Neural networks and hybrid models show better results.
CONCLUSIONS: The data-based models for blood glucose and hypoglycaemia prediction should be able to provide a good balance between the applicability and performance, integrating complementary data from different sources or from different models. This review identifies trends and possible opportunities for research in this topic.","[ 0.03536465  0.45753825  0.3579892  ...  0.7363953  -0.1544116
 -0.60490215]",0.9212265,0.83117235,Both,"neural network, artificial neural network, deep learning"
34373554,Explainable DCNN based chest X-ray image analysis and classification for COVID-19 pneumonia detection,"Hou J, Gao T.",Sci Rep. 2021 Aug 9;11(1):16071. doi: 10.1038/s41598-021-95680-6.,Hou J,Sci Rep,2021,2021/08/10,PMC8352869,,10.1038/s41598-021-95680-6,"To speed up the discovery of COVID-19 disease mechanisms by X-ray images, this research developed a new diagnosis platform using a deep convolutional neural network (DCNN) that is able to assist radiologists with diagnosis by distinguishing COVID-19 pneumonia from non-COVID-19 pneumonia in patients based on chest X-ray classification and analysis. Such a tool can save time in interpreting chest X-rays and increase the accuracy and thereby enhance our medical capacity for the detection and diagnosis of COVID-19. The explainable method is also used in the DCNN to select instances of the X-ray dataset images to explain the behavior of training-learning models to achieve higher prediction accuracy. The average accuracy of our method is above 96%, which can replace manual reading and has the potential to be applied to large-scale rapid screening of COVID-9 for widely use cases.","Explainable DCNN based chest X-ray image analysis and classification for COVID-19 pneumonia detection To speed up the discovery of COVID-19 disease mechanisms by X-ray images, this research developed a new diagnosis platform using a deep convolutional neural network (DCNN) that is able to assist radiologists with diagnosis by distinguishing COVID-19 pneumonia from non-COVID-19 pneumonia in patients based on chest X-ray classification and analysis. Such a tool can save time in interpreting chest X-rays and increase the accuracy and thereby enhance our medical capacity for the detection and diagnosis of COVID-19. The explainable method is also used in the DCNN to select instances of the X-ray dataset images to explain the behavior of training-learning models to achieve higher prediction accuracy. The average accuracy of our method is above 96%, which can replace manual reading and has the potential to be applied to large-scale rapid screening of COVID-9 for widely use cases.","[ 0.21773939  0.5311655   0.15082717 ...  0.5779252   0.10799456
 -0.4620245 ]",0.921307,0.8130695,Computer Vision,"neural network, convolutional neural network, CNN"
34321491,Combining a convolutional neural network with autoencoders to predict the survival chance of COVID-19 patients,"Khozeimeh F, Sharifrazi D, Izadi NH, Joloudari JH, Shoeibi A, Alizadehsani R, Gorriz JM, Hussain S, Sani ZA, Moosaei H, Khosravi A, Nahavandi S, Islam SMS.",Sci Rep. 2021 Jul 28;11(1):15343. doi: 10.1038/s41598-021-93543-8.,Khozeimeh F,Sci Rep,2021,2021/07/29,PMC8319175,,10.1038/s41598-021-93543-8,"COVID-19 has caused many deaths worldwide. The automation of the diagnosis of this virus is highly desired. Convolutional neural networks (CNNs) have shown outstanding classification performance on image datasets. To date, it appears that COVID computer-aided diagnosis systems based on CNNs and clinical information have not yet been analysed or explored. We propose a novel method, named the CNN-AE, to predict the survival chance of COVID-19 patients using a CNN trained with clinical information. Notably, the required resources to prepare CT images are expensive and limited compared to those required to collect clinical data, such as blood pressure, liver disease, etc. We evaluated our method using a publicly available clinical dataset that we collected. The dataset properties were carefully analysed to extract important features and compute the correlations of features. A data augmentation procedure based on autoencoders (AEs) was proposed to balance the dataset. The experimental results revealed that the average accuracy of the CNN-AE (96.05%) was higher than that of the CNN (92.49%). To demonstrate the generality of our augmentation method, we trained some existing mortality risk prediction methods on our dataset (with and without data augmentation) and compared their performances. We also evaluated our method using another dataset for further generality verification. To show that clinical data can be used for COVID-19 survival chance prediction, the CNN-AE was compared with multiple pre-trained deep models that were tuned based on CT images.","Combining a convolutional neural network with autoencoders to predict the survival chance of COVID-19 patients COVID-19 has caused many deaths worldwide. The automation of the diagnosis of this virus is highly desired. Convolutional neural networks (CNNs) have shown outstanding classification performance on image datasets. To date, it appears that COVID computer-aided diagnosis systems based on CNNs and clinical information have not yet been analysed or explored. We propose a novel method, named the CNN-AE, to predict the survival chance of COVID-19 patients using a CNN trained with clinical information. Notably, the required resources to prepare CT images are expensive and limited compared to those required to collect clinical data, such as blood pressure, liver disease, etc. We evaluated our method using a publicly available clinical dataset that we collected. The dataset properties were carefully analysed to extract important features and compute the correlations of features. A data augmentation procedure based on autoencoders (AEs) was proposed to balance the dataset. The experimental results revealed that the average accuracy of the CNN-AE (96.05%) was higher than that of the CNN (92.49%). To demonstrate the generality of our augmentation method, we trained some existing mortality risk prediction methods on our dataset (with and without data augmentation) and compared their performances. We also evaluated our method using another dataset for further generality verification. To show that clinical data can be used for COVID-19 survival chance prediction, the CNN-AE was compared with multiple pre-trained deep models that were tuned based on CT images.","[-0.23656982  0.5338556   0.33889952 ...  0.6247716  -0.19360256
 -0.56989926]",0.90500104,0.80936813,Computer Vision,"neural network, convolutional neural network, CNN"
34253768,COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 prediction,"Er S, Yang S, Zhao T.",Sci Rep. 2021 Jul 12;11(1):14262. doi: 10.1038/s41598-021-93545-6.,Er S,Sci Rep,2021,2021/07/13,PMC8275764,,10.1038/s41598-021-93545-6,"The global spread of COVID-19, the disease caused by the novel coronavirus SARS-CoV-2, has casted a significant threat to mankind. As the COVID-19 situation continues to evolve, predicting localized disease severity is crucial for advanced resource allocation. This paper proposes a method named COURAGE (COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of 2-week-ahead COVID-19 related deaths for each county in the United States, leveraging modern deep learning techniques. Specifically, our method adopts a self-attention model from Natural Language Processing, known as the transformer model, to capture both short-term and long-term dependencies within the time series while enjoying computational efficiency. Our model solely utilizes publicly available information for COVID-19 related confirmed cases, deaths, community mobility trends and demographic information, and can produce state-level predictions as an aggregation of the corresponding county-level predictions. Our numerical experiments demonstrate that our model achieves the state-of-the-art performance among the publicly available benchmark models.","COUnty aggRegation mixup AuGmEntation (COURAGE) COVID-19 prediction The global spread of COVID-19, the disease caused by the novel coronavirus SARS-CoV-2, has casted a significant threat to mankind. As the COVID-19 situation continues to evolve, predicting localized disease severity is crucial for advanced resource allocation. This paper proposes a method named COURAGE (COUnty aggRegation mixup AuGmEntation) to generate a short-term prediction of 2-week-ahead COVID-19 related deaths for each county in the United States, leveraging modern deep learning techniques. Specifically, our method adopts a self-attention model from Natural Language Processing, known as the transformer model, to capture both short-term and long-term dependencies within the time series while enjoying computational efficiency. Our model solely utilizes publicly available information for COVID-19 related confirmed cases, deaths, community mobility trends and demographic information, and can produce state-level predictions as an aggregation of the corresponding county-level predictions. Our numerical experiments demonstrate that our model achieves the state-of-the-art performance among the publicly available benchmark models.","[-0.12920973  0.31382814  0.3120824  ...  0.6600243  -0.15778686
 -0.43856037]",0.90937483,0.8348104,Other,"deep learning, natural language processing, language processing, transformer"
34117734,DeepR2cov: deep representation learning on heterogeneous drug networks to discover anti-inflammatory agents for COVID-19,"Wang X, Xin B, Tan W, Xu Z, Li K, Li F, Zhong W, Peng S.",Brief Bioinform. 2021 Nov 5;22(6):bbab226. doi: 10.1093/bib/bbab226.,Wang X,Brief Bioinform,2021,2021/06/12,PMC8344611,,10.1093/bib/bbab226,"Recent studies have demonstrated that the excessive inflammatory response is an important factor of death in coronavirus disease 2019 (COVID-19) patients. In this study, we propose a deep representation on heterogeneous drug networks, termed DeepR2cov, to discover potential agents for treating the excessive inflammatory response in COVID-19 patients. This work explores the multi-hub characteristic of a heterogeneous drug network integrating eight unique networks. Inspired by the multi-hub characteristic, we design 3 billion special meta paths to train a deep representation model for learning low-dimensional vectors that integrate long-range structure dependency and complex semantic relation among network nodes. Based on the representation vectors and transcriptomics data, we predict 22 drugs that bind to tumor necrosis factor-α or interleukin-6, whose therapeutic associations with the inflammation storm in COVID-19 patients, and molecular binding model are further validated via data from PubMed publications, ongoing clinical trials and a docking program. In addition, the results on five biomedical applications suggest that DeepR2cov significantly outperforms five existing representation approaches. In summary, DeepR2cov is a powerful network representation approach and holds the potential to accelerate treatment of the inflammatory responses in COVID-19 patients. The source code and data can be downloaded from https://github.com/pengsl-lab/DeepR2cov.git.","DeepR2cov: deep representation learning on heterogeneous drug networks to discover anti-inflammatory agents for COVID-19 Recent studies have demonstrated that the excessive inflammatory response is an important factor of death in coronavirus disease 2019 (COVID-19) patients. In this study, we propose a deep representation on heterogeneous drug networks, termed DeepR2cov, to discover potential agents for treating the excessive inflammatory response in COVID-19 patients. This work explores the multi-hub characteristic of a heterogeneous drug network integrating eight unique networks. Inspired by the multi-hub characteristic, we design 3 billion special meta paths to train a deep representation model for learning low-dimensional vectors that integrate long-range structure dependency and complex semantic relation among network nodes. Based on the representation vectors and transcriptomics data, we predict 22 drugs that bind to tumor necrosis factor-α or interleukin-6, whose therapeutic associations with the inflammation storm in COVID-19 patients, and molecular binding model are further validated via data from PubMed publications, ongoing clinical trials and a docking program. In addition, the results on five biomedical applications suggest that DeepR2cov significantly outperforms five existing representation approaches. In summary, DeepR2cov is a powerful network representation approach and holds the potential to accelerate treatment of the inflammatory responses in COVID-19 patients. The source code and data can be downloaded from https://github.com/pengsl-lab/DeepR2cov.git.","[-0.2796908   0.6407392   0.32450145 ...  0.6487188  -0.58378476
 -0.10403576]",0.9001995,0.8033585,Other,Not Specified
33961681,MGP-AttTCN: An interpretable machine learning model for the prediction of sepsis,"Rosnati M, Fortuin V.",PLoS One. 2021 May 7;16(5):e0251248. doi: 10.1371/journal.pone.0251248. eCollection 2021.,Rosnati M,PLoS One,2021,2021/05/07,PMC8104377,,10.1371/journal.pone.0251248,"With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing western world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, we propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. We show that our model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty. For instance, when predicting sepsis five hours prior to onset on our new realistic labels, our proposed model achieves an area under the ROC curve of 0.660 and an area under the PR curve of 0.483, whereas the (less interpretable) previous state-of-the-art model (MGP-TCN) achieves 0.635 AUROC and 0.460 AUPR and the popular commercial InSight model achieves 0.490 AUROC and 0.359 AUPR.","MGP-AttTCN: An interpretable machine learning model for the prediction of sepsis With a mortality rate of 5.4 million lives worldwide every year and a healthcare cost of more than 16 billion dollars in the USA alone, sepsis is one of the leading causes of hospital mortality and an increasing concern in the ageing western world. Recently, medical and technological advances have helped re-define the illness criteria of this disease, which is otherwise poorly understood by the medical society. Together with the rise of widely accessible Electronic Health Records, the advances in data mining and complex nonlinear algorithms are a promising avenue for the early detection of sepsis. This work contributes to the research effort in the field of automated sepsis detection with an open-access labelling of the medical MIMIC-III data set. Moreover, we propose MGP-AttTCN: a joint multitask Gaussian Process and attention-based deep learning model to early predict the occurrence of sepsis in an interpretable manner. We show that our model outperforms the current state-of-the-art and present evidence that different labelling heuristics lead to discrepancies in task difficulty. For instance, when predicting sepsis five hours prior to onset on our new realistic labels, our proposed model achieves an area under the ROC curve of 0.660 and an area under the PR curve of 0.483, whereas the (less interpretable) previous state-of-the-art model (MGP-TCN) achieves 0.635 AUROC and 0.460 AUPR and the popular commercial InSight model achieves 0.490 AUROC and 0.359 AUPR.","[-0.18537885  0.8378203   0.16015425 ...  0.5402255  -0.03972232
 -0.6214336 ]",0.9186731,0.8042907,Other,"machine learning model, deep learning"
33961635,ai-corona: Radiologist-assistant deep learning framework for COVID-19 diagnosis in chest CT scans,"Yousefzadeh M, Esfahanian P, Movahed SMS, Gorgin S, Rahmati D, Abedini A, Nadji SA, Haseli S, Bakhshayesh Karam M, Kiani A, Hoseinyazdi M, Roshandel J, Lashgari R.",PLoS One. 2021 May 7;16(5):e0250952. doi: 10.1371/journal.pone.0250952. eCollection 2021.,Yousefzadeh M,PLoS One,2021,2021/05/07,PMC8104381,,10.1371/journal.pone.0250952,"The development of medical assisting tools based on artificial intelligence advances is essential in the global fight against COVID-19 outbreak and the future of medical systems. In this study, we introduce ai-corona, a radiologist-assistant deep learning framework for COVID-19 infection diagnosis using chest CT scans. Our framework incorporates an EfficientNetB3-based feature extractor. We employed three datasets; the CC-CCII set, the MasihDaneshvari Hospital (MDH) cohort, and the MosMedData cohort. Overall, these datasets constitute 7184 scans from 5693 subjects and include the COVID-19, non-COVID abnormal (NCA), common pneumonia (CP), non-pneumonia, and Normal classes. We evaluate ai-corona on test sets from the CC-CCII set, MDH cohort, and the entirety of the MosMedData cohort, for which it gained AUC scores of 0.997, 0.989, and 0.954, respectively. Our results indicates ai-corona outperforms all the alternative models. Lastly, our framework's diagnosis capabilities were evaluated as assistant to several experts. Accordingly, We observed an increase in both speed and accuracy of expert diagnosis when incorporating ai-corona's assistance.","ai-corona: Radiologist-assistant deep learning framework for COVID-19 diagnosis in chest CT scans The development of medical assisting tools based on artificial intelligence advances is essential in the global fight against COVID-19 outbreak and the future of medical systems. In this study, we introduce ai-corona, a radiologist-assistant deep learning framework for COVID-19 infection diagnosis using chest CT scans. Our framework incorporates an EfficientNetB3-based feature extractor. We employed three datasets; the CC-CCII set, the MasihDaneshvari Hospital (MDH) cohort, and the MosMedData cohort. Overall, these datasets constitute 7184 scans from 5693 subjects and include the COVID-19, non-COVID abnormal (NCA), common pneumonia (CP), non-pneumonia, and Normal classes. We evaluate ai-corona on test sets from the CC-CCII set, MDH cohort, and the entirety of the MosMedData cohort, for which it gained AUC scores of 0.997, 0.989, and 0.954, respectively. Our results indicates ai-corona outperforms all the alternative models. Lastly, our framework's diagnosis capabilities were evaluated as assistant to several experts. Accordingly, We observed an increase in both speed and accuracy of expert diagnosis when incorporating ai-corona's assistance.","[-0.23240842  0.46932003  0.082274   ...  0.33349457  0.09502766
 -0.41338632]",0.90444064,0.8026122,Other,deep learning
33907522,Artificial intelligence in the diagnosis of COVID-19: challenges and perspectives,"Huang S, Yang J, Fong S, Zhao Q.",Int J Biol Sci. 2021 Apr 10;17(6):1581-1587. doi: 10.7150/ijbs.58855. eCollection 2021.,Huang S,Int J Biol Sci,2021,2021/04/28,PMC8071762,,10.7150/ijbs.58855,"Artificial intelligence (AI) is being used to aid in various aspects of the COVID-19 crisis, including epidemiology, molecular research and drug development, medical diagnosis and treatment, and socioeconomics. The association of AI and COVID-19 can accelerate to rapidly diagnose positive patients. To learn the dynamics of a pandemic with relevance to AI, we search the literature using the different academic databases (PubMed, PubMed Central, Scopus, Google Scholar) and preprint servers (bioRxiv, medRxiv, arXiv). In the present review, we address the clinical applications of machine learning and deep learning, including clinical characteristics, electronic medical records, medical images (CT, X-ray, ultrasound images, etc.) in the COVID-19 diagnosis. The current challenges and future perspectives provided in this review can be used to direct an ideal deployment of AI technology in a pandemic.","Artificial intelligence in the diagnosis of COVID-19: challenges and perspectives Artificial intelligence (AI) is being used to aid in various aspects of the COVID-19 crisis, including epidemiology, molecular research and drug development, medical diagnosis and treatment, and socioeconomics. The association of AI and COVID-19 can accelerate to rapidly diagnose positive patients. To learn the dynamics of a pandemic with relevance to AI, we search the literature using the different academic databases (PubMed, PubMed Central, Scopus, Google Scholar) and preprint servers (bioRxiv, medRxiv, arXiv). In the present review, we address the clinical applications of machine learning and deep learning, including clinical characteristics, electronic medical records, medical images (CT, X-ray, ultrasound images, etc.) in the COVID-19 diagnosis. The current challenges and future perspectives provided in this review can be used to direct an ideal deployment of AI technology in a pandemic.","[ 0.42546222  0.6614234   0.2735855  ...  0.5705339  -0.06772098
 -0.44423425]",0.92838347,0.8030545,Both,deep learning
33866552,Artificial Intelligence for Unstructured Healthcare Data: Application to Coding of Patient Reporting of Adverse Drug Reactions,"Létinier L, Jouganous J, Benkebil M, Bel-Létoile A, Goehrs C, Singier A, Rouby F, Lacroix C, Miremont G, Micallef J, Salvo F, Pariente A.",Clin Pharmacol Ther. 2021 Aug;110(2):392-400. doi: 10.1002/cpt.2266. Epub 2021 May 8.,Létinier L,Clin Pharmacol Ther,2021,2021/04/18,PMC8359992,,10.1002/cpt.2266,"Adverse drug reaction (ADR) reporting is a major component of drug safety monitoring; its input will, however, only be optimized if systems can manage to deal with its tremendous flow of information, based primarily on unstructured text fields. The aim of this study was to develop an automated system allowing to code ADRs from patient reports. Our system was based on a knowledge base about drugs, enriched by supervised machine learning (ML) models trained on patients reporting data. To train our models, we selected all cases of ADRs reported by patients to a French Pharmacovigilance Centre through a national web-portal between March 2017 and March 2019 (n = 2,058 reports). We tested both conventional ML models and deep-learning models. We performed an external validation using a dataset constituted of a random sample of ADRs reported to the Marseille Pharmacovigilance Centre over the same period (n = 187). Here, we show that regarding area under the curve (AUC) and F-measure, the best model to identify ADRs was gradient boosting trees (LGBM), with an AUC of 0.93 (0.92-0.94) and F-measure of 0.72 (0.68-0.75). This model was run for external validation showing an AUC of 0.91 and a F-measure of 0.58. We evaluated an artificial intelligence pipeline that was found able to learn how to identify correctly ADRs from unstructured data. This result allowed us to start a new study using more data to further improve our performance and offer a tool that is useful in practice to efficiently manage drug safety information.","Artificial Intelligence for Unstructured Healthcare Data: Application to Coding of Patient Reporting of Adverse Drug Reactions Adverse drug reaction (ADR) reporting is a major component of drug safety monitoring; its input will, however, only be optimized if systems can manage to deal with its tremendous flow of information, based primarily on unstructured text fields. The aim of this study was to develop an automated system allowing to code ADRs from patient reports. Our system was based on a knowledge base about drugs, enriched by supervised machine learning (ML) models trained on patients reporting data. To train our models, we selected all cases of ADRs reported by patients to a French Pharmacovigilance Centre through a national web-portal between March 2017 and March 2019 (n = 2,058 reports). We tested both conventional ML models and deep-learning models. We performed an external validation using a dataset constituted of a random sample of ADRs reported to the Marseille Pharmacovigilance Centre over the same period (n = 187). Here, we show that regarding area under the curve (AUC) and F-measure, the best model to identify ADRs was gradient boosting trees (LGBM), with an AUC of 0.93 (0.92-0.94) and F-measure of 0.72 (0.68-0.75). This model was run for external validation showing an AUC of 0.91 and a F-measure of 0.58. We evaluated an artificial intelligence pipeline that was found able to learn how to identify correctly ADRs from unstructured data. This result allowed us to start a new study using more data to further improve our performance and offer a tool that is useful in practice to efficiently manage drug safety information.","[-0.19831796  0.7186141   0.36069673 ...  0.4000475  -0.24569643
 -0.4946709 ]",0.9058743,0.8162229,Text Mining,Not Specified
33738639,Transfer learning-based ensemble support vector machine model for automated COVID-19 detection using lung computerized tomography scan data,"Singh M, Bansal S, Ahuja S, Dubey RK, Panigrahi BK, Dey N.",Med Biol Eng Comput. 2021 Apr;59(4):825-839. doi: 10.1007/s11517-020-02299-2. Epub 2021 Mar 18.,Singh M,Med Biol Eng Comput,2021,2021/03/19,PMC7972022,,10.1007/s11517-020-02299-2,"The novel discovered disease coronavirus popularly known as COVID-19 is caused due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and declared a pandemic by the World Health Organization (WHO). An early-stage detection of COVID-19 is crucial for the containment of the pandemic it has caused. In this study, a transfer learning-based COVID-19 screening technique is proposed. The motivation of this study is to design an automated system that can assist medical staff especially in areas where trained staff are outnumbered. The study investigates the potential of transfer learning-based models for automatically diagnosing diseases like COVID-19 to assist the medical force, especially in times of an outbreak. In the proposed work, a deep learning model, i.e., truncated VGG16 (Visual Geometry Group from Oxford) is implemented to screen COVID-19 CT scans. The VGG16 architecture is fine-tuned and used to extract features from CT scan images. Further principal component analysis (PCA) is used for feature selection. For the final classification, four different classifiers, namely deep convolutional neural network (DCNN), extreme learning machine (ELM), online sequential ELM, and bagging ensemble with support vector machine (SVM) are compared. The best performing classifier bagging ensemble with SVM within 385 ms achieved an accuracy of 95.7%, the precision of 95.8%, area under curve (AUC) of 0.958, and an F1 score of 95.3% on 208 test images. The results obtained on diverse datasets prove the superiority and robustness of the proposed work. A pre-processing technique has also been proposed for radiological data. The study further compares pre-trained CNN architectures and classification models against the proposed technique.","Transfer learning-based ensemble support vector machine model for automated COVID-19 detection using lung computerized tomography scan data The novel discovered disease coronavirus popularly known as COVID-19 is caused due to severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) and declared a pandemic by the World Health Organization (WHO). An early-stage detection of COVID-19 is crucial for the containment of the pandemic it has caused. In this study, a transfer learning-based COVID-19 screening technique is proposed. The motivation of this study is to design an automated system that can assist medical staff especially in areas where trained staff are outnumbered. The study investigates the potential of transfer learning-based models for automatically diagnosing diseases like COVID-19 to assist the medical force, especially in times of an outbreak. In the proposed work, a deep learning model, i.e., truncated VGG16 (Visual Geometry Group from Oxford) is implemented to screen COVID-19 CT scans. The VGG16 architecture is fine-tuned and used to extract features from CT scan images. Further principal component analysis (PCA) is used for feature selection. For the final classification, four different classifiers, namely deep convolutional neural network (DCNN), extreme learning machine (ELM), online sequential ELM, and bagging ensemble with support vector machine (SVM) are compared. The best performing classifier bagging ensemble with SVM within 385 ms achieved an accuracy of 95.7%, the precision of 95.8%, area under curve (AUC) of 0.958, and an F1 score of 95.3% on 208 test images. The results obtained on diverse datasets prove the superiority and robustness of the proposed work. A pre-processing technique has also been proposed for radiological data. The study further compares pre-trained CNN architectures and classification models against the proposed technique.","[ 0.20386183  0.5276971   0.33703676 ...  0.54954255 -0.09176534
 -0.6382532 ]",0.9112847,0.81407934,Computer Vision,"neural network, convolutional neural network, CNN, deep learning"
33587262,DON: Deep Learning and Optimization-Based Framework for Detection of Novel Coronavirus Disease Using X-ray Images,"Dhiman G, Vinoth Kumar V, Kaur A, Sharma A.",Interdiscip Sci. 2021 Jun;13(2):260-272. doi: 10.1007/s12539-021-00418-7. Epub 2021 Feb 15.,Dhiman G,Interdiscip Sci,2021,2021/02/15,PMC7882874,,10.1007/s12539-021-00418-7,"In the hospital, a limited number of COVID-19 test kits are available due to the spike in cases every day. For this reason, a rapid alternative diagnostic option should be introduced as an automated detection method to prevent COVID-19 spreading among individuals. This article proposes multi-objective optimization and a deep-learning methodology for the detection of infected coronavirus patients with X-rays. J48 decision tree method classifies the deep characteristics of affected X-ray corona images to detect the contaminated patients effectively. Eleven different convolutional neuronal network-based (CNN) models were developed in this study to detect infected patients with coronavirus pneumonia using X-ray images (AlexNet, VGG16, VGG19, GoogleNet, ResNet18, ResNet500, ResNet101, InceptionV3, InceptionResNetV2, DenseNet201 and XceptionNet). In addition, the parameters of the CNN profound learning model are described using an emperor penguin optimizer with several objectives (MOEPO). A broad review reveals that the proposed model can categorise the X-ray images at the correct rates of precision, accuracy, recall, specificity and F1-score. Extensive test results show that the proposed model outperforms competitive models with well-known efficiency metrics. The proposed model is, therefore, useful for the real-time classification of X-ray chest images of COVID-19 disease.","DON: Deep Learning and Optimization-Based Framework for Detection of Novel Coronavirus Disease Using X-ray Images In the hospital, a limited number of COVID-19 test kits are available due to the spike in cases every day. For this reason, a rapid alternative diagnostic option should be introduced as an automated detection method to prevent COVID-19 spreading among individuals. This article proposes multi-objective optimization and a deep-learning methodology for the detection of infected coronavirus patients with X-rays. J48 decision tree method classifies the deep characteristics of affected X-ray corona images to detect the contaminated patients effectively. Eleven different convolutional neuronal network-based (CNN) models were developed in this study to detect infected patients with coronavirus pneumonia using X-ray images (AlexNet, VGG16, VGG19, GoogleNet, ResNet18, ResNet500, ResNet101, InceptionV3, InceptionResNetV2, DenseNet201 and XceptionNet). In addition, the parameters of the CNN profound learning model are described using an emperor penguin optimizer with several objectives (MOEPO). A broad review reveals that the proposed model can categorise the X-ray images at the correct rates of precision, accuracy, recall, specificity and F1-score. Extensive test results show that the proposed model outperforms competitive models with well-known efficiency metrics. The proposed model is, therefore, useful for the real-time classification of X-ray chest images of COVID-19 disease.","[ 0.13623402  0.39525107  0.23865351 ...  0.64298755  0.03227232
 -0.49901304]",0.90052396,0.8274477,Other,"CNN, deep learning"
33461009,Lung cancer survival period prediction and understanding: Deep learning approaches,"Doppalapudi S, Qiu RG, Badr Y.",Int J Med Inform. 2021 Apr;148:104371. doi: 10.1016/j.ijmedinf.2020.104371. Epub 2020 Dec 29.,Doppalapudi S,Int J Med Inform,2021,2021/01/18,,,10.1016/j.ijmedinf.2020.104371,"INTRODUCTION: Survival period prediction through early diagnosis of cancer has many benefits. It allows both patients and caregivers to plan resources, time and intensity of care to provide the best possible treatment path for the patients. In this paper, by focusing on lung cancer patients, we build several survival prediction models using deep learning techniques to tackle both cancer survival classification and regression problems. We also conduct feature importance analysis to understand how lung cancer patients' relevant factors impact their survival periods. We contribute to identifying an approach to estimate survivability that are commonly and practically appropriate for medical use.
METHODOLOGIES: We have compared the performance across three of the most popular deep learning architectures - Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) while comparing the performing of deep learning models against traditional machine learning models. The data was obtained from the lung cancer section of Surveillance, Epidemiology, and End Results (SEER) cancer registry.
RESULTS: The deep learning models outperformed traditional machine learning models across both classification and regression approaches. We obtained a best of 71.18 % accuracy for the classification approach when patients' survival periods are segmented into classes of '&lt;=6 months',' 0.5 - 2 years' and '&gt;2 years' and Root Mean Squared Error (RMSE) of 13.5 % andR2 value of 0.5 for the regression approach for the deep learning models while the traditional machine learning models saturated at 61.12 % classification accuracy and 14.87 % RMSE in regression.
CONCLUSIONS: This approach can be a baseline for early prediction with predictions that can be further improved with more temporal treatment information collected from treated patients. In addition, we evaluated the feature importance to investigate the model interpretability, gaining further insight into the survival analysis models and the factors that are important in cancer survival period prediction.","Lung cancer survival period prediction and understanding: Deep learning approaches INTRODUCTION: Survival period prediction through early diagnosis of cancer has many benefits. It allows both patients and caregivers to plan resources, time and intensity of care to provide the best possible treatment path for the patients. In this paper, by focusing on lung cancer patients, we build several survival prediction models using deep learning techniques to tackle both cancer survival classification and regression problems. We also conduct feature importance analysis to understand how lung cancer patients' relevant factors impact their survival periods. We contribute to identifying an approach to estimate survivability that are commonly and practically appropriate for medical use.
METHODOLOGIES: We have compared the performance across three of the most popular deep learning architectures - Artificial Neural Networks (ANN), Convolutional Neural Networks (CNN), and Recurrent Neural Networks (RNN) while comparing the performing of deep learning models against traditional machine learning models. The data was obtained from the lung cancer section of Surveillance, Epidemiology, and End Results (SEER) cancer registry.
RESULTS: The deep learning models outperformed traditional machine learning models across both classification and regression approaches. We obtained a best of 71.18 % accuracy for the classification approach when patients' survival periods are segmented into classes of '&lt;=6 months',' 0.5 - 2 years' and '&gt;2 years' and Root Mean Squared Error (RMSE) of 13.5 % andR2 value of 0.5 for the regression approach for the deep learning models while the traditional machine learning models saturated at 61.12 % classification accuracy and 14.87 % RMSE in regression.
CONCLUSIONS: This approach can be a baseline for early prediction with predictions that can be further improved with more temporal treatment information collected from treated patients. In addition, we evaluated the feature importance to investigate the model interpretability, gaining further insight into the survival analysis models and the factors that are important in cancer survival period prediction.","[-0.29247877  0.72691494  0.4623037  ...  0.82282025  0.20147282
 -0.6847872 ]",0.9149357,0.82264906,Both,"neural network, artificial neural network, machine learning model, convolutional neural network, recurrent neural network, CNN, RNN, deep learning"
33459907,Automated processing of social media content for radiologists: applied deep learning to radiological content on twitter during COVID-19 pandemic,"Khurana S, Chopra R, Khurana B.",Emerg Radiol. 2021 Jun;28(3):477-483. doi: 10.1007/s10140-020-01885-z. Epub 2021 Jan 18.,Khurana S,Emerg Radiol,2021,2021/01/18,PMC7811945,,10.1007/s10140-020-01885-z,"PURPOSE: The purpose of this study was to develop an automated process to analyze multimedia content on Twitter during the COVID-19 outbreak and classify content for radiological significance using deep learning (DL).
MATERIALS AND METHODS: Using Twitter search features, all tweets containing keywords from both ""radiology"" and ""COVID-19"" were collected for the period January 01, 2020 up to April 24, 2020. The resulting dataset comprised of 8354 tweets. Images were classified as (i) images with text (ii) radiological content (e.g., CT scan snapshots, X-ray images), and (iii) non-medical content like personal images or memes. We trained our deep learning model using Convolutional Neural Networks (CNN) on training dataset of 1040 labeled images drawn from all three classes. We then trained another DL classifier for segmenting images into categories based on human anatomy. All software used is open-source and adapted for this research. The diagnostic performance of the algorithm was assessed by comparing results on a test set of 1885 images.
RESULTS: Our analysis shows that in COVID-19 related tweets on radiology, nearly 32% had textual images, another 24% had radiological content, and 44% were not of radiological significance. Our results indicated a 92% accuracy in classifying images originally labeled as chest X-ray or chest CT and a nearly 99% accurate classification of images containing medically relevant text. With larger training dataset and algorithmic tweaks, the accuracy can be further improved.
CONCLUSION: Applying DL on rich textual images and other metadata in tweets we can process and classify content for radiological significance in real time.","Automated processing of social media content for radiologists: applied deep learning to radiological content on twitter during COVID-19 pandemic PURPOSE: The purpose of this study was to develop an automated process to analyze multimedia content on Twitter during the COVID-19 outbreak and classify content for radiological significance using deep learning (DL).
MATERIALS AND METHODS: Using Twitter search features, all tweets containing keywords from both ""radiology"" and ""COVID-19"" were collected for the period January 01, 2020 up to April 24, 2020. The resulting dataset comprised of 8354 tweets. Images were classified as (i) images with text (ii) radiological content (e.g., CT scan snapshots, X-ray images), and (iii) non-medical content like personal images or memes. We trained our deep learning model using Convolutional Neural Networks (CNN) on training dataset of 1040 labeled images drawn from all three classes. We then trained another DL classifier for segmenting images into categories based on human anatomy. All software used is open-source and adapted for this research. The diagnostic performance of the algorithm was assessed by comparing results on a test set of 1885 images.
RESULTS: Our analysis shows that in COVID-19 related tweets on radiology, nearly 32% had textual images, another 24% had radiological content, and 44% were not of radiological significance. Our results indicated a 92% accuracy in classifying images originally labeled as chest X-ray or chest CT and a nearly 99% accurate classification of images containing medically relevant text. With larger training dataset and algorithmic tweaks, the accuracy can be further improved.
CONCLUSION: Applying DL on rich textual images and other metadata in tweets we can process and classify content for radiological significance in real time.","[ 0.02389955  0.41844577  0.13716061 ...  0.76672083 -0.00464768
 -0.49084088]",0.9094445,0.80788165,Both,"neural network, convolutional neural network, CNN, deep learning"
33432172,Robust breast cancer detection in mammography and digital breast tomosynthesis using an annotation-efficient deep learning approach,"Lotter W, Diab AR, Haslam B, Kim JG, Grisot G, Wu E, Wu K, Onieva JO, Boyer Y, Boxerman JL, Wang M, Bandler M, Vijayaraghavan GR, Gregory Sorensen A.",Nat Med. 2021 Feb;27(2):244-249. doi: 10.1038/s41591-020-01174-9. Epub 2021 Jan 11.,Lotter W,Nat Med,2021,2021/01/12,PMC9426656,NIHMS1829595,10.1038/s41591-020-01174-9,"Breast cancer remains a global challenge, causing over 600,000 deaths in 2018 (ref. 1). To achieve earlier cancer detection, health organizations worldwide recommend screening mammography, which is estimated to decrease breast cancer mortality by 20-40% (refs. 2,3). Despite the clear value of screening mammography, significant false positive and false negative rates along with non-uniformities in expert reader availability leave opportunities for improving quality and access4,5. To address these limitations, there has been much recent interest in applying deep learning to mammography6-18, and these efforts have highlighted two key difficulties: obtaining large amounts of annotated training data and ensuring generalization across populations, acquisition equipment and modalities. Here we present an annotation-efficient deep learning approach that (1) achieves state-of-the-art performance in mammogram classification, (2) successfully extends to digital breast tomosynthesis (DBT; '3D mammography'), (3) detects cancers in clinically negative prior mammograms of patients with cancer, (4) generalizes well to a population with low screening rates and (5) outperforms five out of five full-time breast-imaging specialists with an average increase in sensitivity of 14%. By creating new 'maximum suspicion projection' (MSP) images from DBT data, our progressively trained, multiple-instance learning approach effectively trains on DBT exams using only breast-level labels while maintaining localization-based interpretability. Altogether, our results demonstrate promise towards software that can improve the accuracy of and access to screening mammography worldwide.","Robust breast cancer detection in mammography and digital breast tomosynthesis using an annotation-efficient deep learning approach Breast cancer remains a global challenge, causing over 600,000 deaths in 2018 (ref. 1). To achieve earlier cancer detection, health organizations worldwide recommend screening mammography, which is estimated to decrease breast cancer mortality by 20-40% (refs. 2,3). Despite the clear value of screening mammography, significant false positive and false negative rates along with non-uniformities in expert reader availability leave opportunities for improving quality and access4,5. To address these limitations, there has been much recent interest in applying deep learning to mammography6-18, and these efforts have highlighted two key difficulties: obtaining large amounts of annotated training data and ensuring generalization across populations, acquisition equipment and modalities. Here we present an annotation-efficient deep learning approach that (1) achieves state-of-the-art performance in mammogram classification, (2) successfully extends to digital breast tomosynthesis (DBT; '3D mammography'), (3) detects cancers in clinically negative prior mammograms of patients with cancer, (4) generalizes well to a population with low screening rates and (5) outperforms five out of five full-time breast-imaging specialists with an average increase in sensitivity of 14%. By creating new 'maximum suspicion projection' (MSP) images from DBT data, our progressively trained, multiple-instance learning approach effectively trains on DBT exams using only breast-level labels while maintaining localization-based interpretability. Altogether, our results demonstrate promise towards software that can improve the accuracy of and access to screening mammography worldwide.","[-0.44849312  0.5530095   0.07963893 ...  0.33355707  0.32088575
 -0.56985694]",0.90614575,0.80341005,Computer Vision,deep learning
33406530,Towards deep phenotyping pregnancy: a systematic review on artificial intelligence and machine learning methods to improve pregnancy outcomes,"Davidson L, Boland MR.",Brief Bioinform. 2021 Sep 2;22(5):bbaa369. doi: 10.1093/bib/bbaa369.,Davidson L,Brief Bioinform,2021,2021/01/06,PMC8424395,,10.1093/bib/bbaa369,"OBJECTIVE: Development of novel informatics methods focused on improving pregnancy outcomes remains an active area of research. The purpose of this study is to systematically review the ways that artificial intelligence (AI) and machine learning (ML), including deep learning (DL), methodologies can inform patient care during pregnancy and improve outcomes.
MATERIALS AND METHODS: We searched English articles on EMBASE, PubMed and SCOPUS. Search terms included ML, AI, pregnancy and informatics. We included research articles and book chapters, excluding conference papers, editorials and notes.
RESULTS: We identified 127 distinct studies from our queries that were relevant to our topic and included in the review. We found that supervised learning methods were more popular (n = 69) than unsupervised methods (n = 9). Popular methods included support vector machines (n = 30), artificial neural networks (n = 22), regression analysis (n = 17) and random forests (n = 16). Methods such as DL are beginning to gain traction (n = 13). Common areas within the pregnancy domain where AI and ML methods were used the most include prenatal care (e.g. fetal anomalies, placental functioning) (n = 73); perinatal care, birth and delivery (n = 20); and preterm birth (n = 13). Efforts to translate AI into clinical care include clinical decision support systems (n = 24) and mobile health applications (n = 9).
CONCLUSIONS: Overall, we found that ML and AI methods are being employed to optimize pregnancy outcomes, including modern DL methods (n = 13). Future research should focus on less-studied pregnancy domain areas, including postnatal and postpartum care (n = 2). Also, more work on clinical adoption of AI methods and the ethical implications of such adoption is needed.","Towards deep phenotyping pregnancy: a systematic review on artificial intelligence and machine learning methods to improve pregnancy outcomes OBJECTIVE: Development of novel informatics methods focused on improving pregnancy outcomes remains an active area of research. The purpose of this study is to systematically review the ways that artificial intelligence (AI) and machine learning (ML), including deep learning (DL), methodologies can inform patient care during pregnancy and improve outcomes.
MATERIALS AND METHODS: We searched English articles on EMBASE, PubMed and SCOPUS. Search terms included ML, AI, pregnancy and informatics. We included research articles and book chapters, excluding conference papers, editorials and notes.
RESULTS: We identified 127 distinct studies from our queries that were relevant to our topic and included in the review. We found that supervised learning methods were more popular (n = 69) than unsupervised methods (n = 9). Popular methods included support vector machines (n = 30), artificial neural networks (n = 22), regression analysis (n = 17) and random forests (n = 16). Methods such as DL are beginning to gain traction (n = 13). Common areas within the pregnancy domain where AI and ML methods were used the most include prenatal care (e.g. fetal anomalies, placental functioning) (n = 73); perinatal care, birth and delivery (n = 20); and preterm birth (n = 13). Efforts to translate AI into clinical care include clinical decision support systems (n = 24) and mobile health applications (n = 9).
CONCLUSIONS: Overall, we found that ML and AI methods are being employed to optimize pregnancy outcomes, including modern DL methods (n = 13). Future research should focus on less-studied pregnancy domain areas, including postnatal and postpartum care (n = 2). Also, more work on clinical adoption of AI methods and the ethical implications of such adoption is needed.","[-0.0778914   0.45323682  0.56167674 ...  0.5772815   0.00237374
 -0.48942292]",0.90864366,0.81739926,Both,"neural network, artificial neural network, deep learning"
33398067,Fast automated detection of COVID-19 from medical images using convolutional neural networks,"Liang S, Liu H, Gu Y, Guo X, Li H, Li L, Wu Z, Liu M, Tao L.",Commun Biol. 2021 Jan 4;4(1):35. doi: 10.1038/s42003-020-01535-7.,Liang S,Commun Biol,2021,2021/01/05,PMC7782580,,10.1038/s42003-020-01535-7,"Coronavirus disease 2019 (COVID-19) is a global pandemic posing significant health risks. The diagnostic test sensitivity of COVID-19 is limited due to irregularities in specimen handling. We propose a deep learning framework that identifies COVID-19 from medical images as an auxiliary testing method to improve diagnostic sensitivity. We use pseudo-coloring methods and a platform for annotating X-ray and computed tomography images to train the convolutional neural network, which achieves a performance similar to that of experts and provides high scores for multiple statistical indices (F1 scores > 96.72% (0.9307, 0.9890) and specificity >99.33% (0.9792, 1.0000)). Heatmaps are used to visualize the salient features extracted by the neural network. The neural network-based regression provides strong correlations between the lesion areas in the images and five clinical indicators, resulting in high accuracy of the classification framework. The proposed method represents a potential computer-aided diagnosis method for COVID-19 in clinical practice.","Fast automated detection of COVID-19 from medical images using convolutional neural networks Coronavirus disease 2019 (COVID-19) is a global pandemic posing significant health risks. The diagnostic test sensitivity of COVID-19 is limited due to irregularities in specimen handling. We propose a deep learning framework that identifies COVID-19 from medical images as an auxiliary testing method to improve diagnostic sensitivity. We use pseudo-coloring methods and a platform for annotating X-ray and computed tomography images to train the convolutional neural network, which achieves a performance similar to that of experts and provides high scores for multiple statistical indices (F1 scores > 96.72% (0.9307, 0.9890) and specificity >99.33% (0.9792, 1.0000)). Heatmaps are used to visualize the salient features extracted by the neural network. The neural network-based regression provides strong correlations between the lesion areas in the images and five clinical indicators, resulting in high accuracy of the classification framework. The proposed method represents a potential computer-aided diagnosis method for COVID-19 in clinical practice.","[ 0.15593527  0.72714126  0.24483615 ...  0.6727414  -0.03103101
 -0.63534117]",0.91000277,0.81284845,Computer Vision,"neural network, convolutional neural network, deep learning"
33387306,A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images,"Rasheed J, Hameed AA, Djeddi C, Jamil A, Al-Turjman F.",Interdiscip Sci. 2021 Mar;13(1):103-117. doi: 10.1007/s12539-020-00403-6. Epub 2021 Jan 2.,Rasheed J,Interdiscip Sci,2021,2021/01/02,PMC7776293,,10.1007/s12539-020-00403-6,"Corona virus disease (COVID-19) acknowledged as a pandemic by the WHO and mankind all over the world is vulnerable to this virus. Alternative tools are needed that can help in diagnosis of the coronavirus. Researchers of this article investigated the potential of machine learning methods for automatic diagnosis of corona virus with high accuracy from X-ray images. Two most commonly used classifiers were selected: logistic regression (LR) and convolutional neural networks (CNN). The main reason was to make the system fast and efficient. Moreover, a dimensionality reduction approach was also investigated based on principal component analysis (PCA) to further speed up the learning process and improve the classification accuracy by selecting the highly discriminate features. The deep learning-based methods demand large amount of training samples compared to conventional approaches, yet adequate amount of labelled training samples was not available for COVID-19 X-ray images. Therefore, data augmentation technique using generative adversarial network (GAN) was employed to further increase the training samples and reduce the overfitting problem. We used the online available dataset and incorporated GAN to have 500 X-ray images in total for this study. Both CNN and LR showed encouraging results for COVID-19 patient identification. The LR and CNN models showed 95.2-97.6% overall accuracy without PCA and 97.6-100% with PCA for positive cases identification, respectively.","A machine learning-based framework for diagnosis of COVID-19 from chest X-ray images Corona virus disease (COVID-19) acknowledged as a pandemic by the WHO and mankind all over the world is vulnerable to this virus. Alternative tools are needed that can help in diagnosis of the coronavirus. Researchers of this article investigated the potential of machine learning methods for automatic diagnosis of corona virus with high accuracy from X-ray images. Two most commonly used classifiers were selected: logistic regression (LR) and convolutional neural networks (CNN). The main reason was to make the system fast and efficient. Moreover, a dimensionality reduction approach was also investigated based on principal component analysis (PCA) to further speed up the learning process and improve the classification accuracy by selecting the highly discriminate features. The deep learning-based methods demand large amount of training samples compared to conventional approaches, yet adequate amount of labelled training samples was not available for COVID-19 X-ray images. Therefore, data augmentation technique using generative adversarial network (GAN) was employed to further increase the training samples and reduce the overfitting problem. We used the online available dataset and incorporated GAN to have 500 X-ray images in total for this study. Both CNN and LR showed encouraging results for COVID-19 patient identification. The LR and CNN models showed 95.2-97.6% overall accuracy without PCA and 97.6-100% with PCA for positive cases identification, respectively.","[ 0.23503348  0.49115103  0.11810727 ...  0.55986565 -0.03388617
 -0.6763984 ]",0.91045535,0.8175845,Both,"neural network, convolutional neural network, CNN, deep learning"
33335183,Dynamic survival prediction in intensive care units from heterogeneous time series without the need for variable selection or curation,"Deasy J, Liò P, Ercole A.",Sci Rep. 2020 Dec 17;10(1):22129. doi: 10.1038/s41598-020-79142-z.,Deasy J,Sci Rep,2020,2020/12/18,PMC7747558,,10.1038/s41598-020-79142-z,"Extensive monitoring in intensive care units (ICUs) generates large quantities of data which contain numerous trends that are difficult for clinicians to systematically evaluate. Current approaches to such heterogeneity in electronic health records (EHRs) discard pertinent information. We present a deep learning pipeline that uses all uncurated chart, lab, and output events for prediction of in-hospital mortality without variable selection. Over 21,000 ICU patients and tens of thousands of variables derived from the MIMIC-III database were used to train and validate our model. Recordings in the first few hours of a patient's stay were found to be strongly predictive of mortality, outperforming models using SAPS II and OASIS scores, AUROC 0.72 and 0.76 at 24 h respectively, within just 12 h of ICU admission. Our model achieves a very strong predictive performance of AUROC 0.85 (95% CI 0.83-0.86) after 48 h. Predictive performance increases over the first 48 h, but suffers from diminishing returns, providing rationale for time-limited trials of critical care and suggesting that the timing of decision making can be optimised and individualised.","Dynamic survival prediction in intensive care units from heterogeneous time series without the need for variable selection or curation Extensive monitoring in intensive care units (ICUs) generates large quantities of data which contain numerous trends that are difficult for clinicians to systematically evaluate. Current approaches to such heterogeneity in electronic health records (EHRs) discard pertinent information. We present a deep learning pipeline that uses all uncurated chart, lab, and output events for prediction of in-hospital mortality without variable selection. Over 21,000 ICU patients and tens of thousands of variables derived from the MIMIC-III database were used to train and validate our model. Recordings in the first few hours of a patient's stay were found to be strongly predictive of mortality, outperforming models using SAPS II and OASIS scores, AUROC 0.72 and 0.76 at 24 h respectively, within just 12 h of ICU admission. Our model achieves a very strong predictive performance of AUROC 0.85 (95% CI 0.83-0.86) after 48 h. Predictive performance increases over the first 48 h, but suffers from diminishing returns, providing rationale for time-limited trials of critical care and suggesting that the timing of decision making can be optimised and individualised.","[-0.3410082   0.6366209   0.49388912 ...  0.4331422   0.09650183
 -0.80904305]",0.90668046,0.8074162,Computer Vision,deep learning
33320858,Optimised genetic algorithm-extreme learning machine approach for automatic COVID-19 detection,"Albadr MAA, Tiun S, Ayob M, Al-Dhief FT, Omar K, Hamzah FA.",PLoS One. 2020 Dec 15;15(12):e0242899. doi: 10.1371/journal.pone.0242899. eCollection 2020.,Albadr MAA,PLoS One,2020,2020/12/15,PMC7737907,,10.1371/journal.pone.0242899,"The coronavirus disease (COVID-19), is an ongoing global pandemic caused by severe acute respiratory syndrome. Chest Computed Tomography (CT) is an effective method for detecting lung illnesses, including COVID-19. However, the CT scan is expensive and time-consuming. Therefore, this work focus on detecting COVID-19 using chest X-ray images because it is widely available, faster, and cheaper than CT scan. Many machine learning approaches such as Deep Learning, Neural Network, and Support Vector Machine; have used X-ray for detecting the COVID-19. Although the performance of those approaches is acceptable in terms of accuracy, however, they require high computational time and more memory space. Therefore, this work employs an Optimised Genetic Algorithm-Extreme Learning Machine (OGA-ELM) with three selection criteria (i.e., random, K-tournament, and roulette wheel) to detect COVID-19 using X-ray images. The most crucial strength factors of the Extreme Learning Machine (ELM) are: (i) high capability of the ELM in avoiding overfitting; (ii) its usability on binary and multi-type classifiers; and (iii) ELM could work as a kernel-based support vector machine with a structure of a neural network. These advantages make the ELM efficient in achieving an excellent learning performance. ELMs have successfully been applied in many domains, including medical domains such as breast cancer detection, pathological brain detection, and ductal carcinoma in situ detection, but not yet tested on detecting COVID-19. Hence, this work aims to identify the effectiveness of employing OGA-ELM in detecting COVID-19 using chest X-ray images. In order to reduce the dimensionality of a histogram oriented gradient features, we use principal component analysis. The performance of OGA-ELM is evaluated on a benchmark dataset containing 188 chest X-ray images with two classes: a healthy and a COVID-19 infected. The experimental result shows that the OGA-ELM achieves 100.00% accuracy with fast computation time. This demonstrates that OGA-ELM is an efficient method for COVID-19 detecting using chest X-ray images.","Optimised genetic algorithm-extreme learning machine approach for automatic COVID-19 detection The coronavirus disease (COVID-19), is an ongoing global pandemic caused by severe acute respiratory syndrome. Chest Computed Tomography (CT) is an effective method for detecting lung illnesses, including COVID-19. However, the CT scan is expensive and time-consuming. Therefore, this work focus on detecting COVID-19 using chest X-ray images because it is widely available, faster, and cheaper than CT scan. Many machine learning approaches such as Deep Learning, Neural Network, and Support Vector Machine; have used X-ray for detecting the COVID-19. Although the performance of those approaches is acceptable in terms of accuracy, however, they require high computational time and more memory space. Therefore, this work employs an Optimised Genetic Algorithm-Extreme Learning Machine (OGA-ELM) with three selection criteria (i.e., random, K-tournament, and roulette wheel) to detect COVID-19 using X-ray images. The most crucial strength factors of the Extreme Learning Machine (ELM) are: (i) high capability of the ELM in avoiding overfitting; (ii) its usability on binary and multi-type classifiers; and (iii) ELM could work as a kernel-based support vector machine with a structure of a neural network. These advantages make the ELM efficient in achieving an excellent learning performance. ELMs have successfully been applied in many domains, including medical domains such as breast cancer detection, pathological brain detection, and ductal carcinoma in situ detection, but not yet tested on detecting COVID-19. Hence, this work aims to identify the effectiveness of employing OGA-ELM in detecting COVID-19 using chest X-ray images. In order to reduce the dimensionality of a histogram oriented gradient features, we use principal component analysis. The performance of OGA-ELM is evaluated on a benchmark dataset containing 188 chest X-ray images with two classes: a healthy and a COVID-19 infected. The experimental result shows that the OGA-ELM achieves 100.00% accuracy with fast computation time. This demonstrates that OGA-ELM is an efficient method for COVID-19 detecting using chest X-ray images.","[ 0.24213655  0.58119535  0.27159187 ...  0.69752634  0.05162617
 -0.76242435]",0.90945166,0.8236588,Computer Vision,"neural network, deep learning"
33306549,Identifying influential neighbors in social networks and venue affiliations among young MSM: a data science approach to predict HIV infection,"Xiang Y, Fujimoto K, Li F, Wang Q, Del Vecchio N, Schneider J, Zhi D, Tao C.",AIDS. 2021 May 1;35(Suppl 1):S65-S73. doi: 10.1097/QAD.0000000000002784.,Xiang Y,AIDS,2021,2020/12/11,PMC8058230,NIHMS1661854,10.1097/QAD.0000000000002784,"OBJECTIVE: Young MSM (YMSM) bear a disproportionate burden of HIV infection in the United States and their risks of acquiring HIV may be shaped by complex multilayer social networks. These networks are formed through not only direct contact with social/sex partners but also indirect anonymous contacts encountered when attending social venues. We introduced a new application of a state-of-the-art graph-based deep learning method to predict HIV infection that can identify influential neighbors within these multiple network contexts.
DESIGN AND METHODS: We used empirical network data among YMSM aged 16-29 years old collected from Houston and Chicago in the United States between 2014 and 2016. A computational framework GAT-HIV (Graph Attention Networks for HIV) was proposed to predict HIV infections by identifying influential neighbors within social networks. These networks were formed by multiple relations constituted of social/sex partners and shared venue attendances, and using individual-level variables. Further, GAT-HIV was extended to combine multiple social networks using multigraph GAT methods. A visualization tool was also developed to highlight influential network members for each individual within the multiple social networks.
RESULTS: The multigraph GAT-HIV models obtained average AUC values of 0.776 and 0.824 for Chicago and Houston, respectively, performing better than empirical predictive models (e.g. AUCs of random forest: 0.758 and 0.798). GAT-HIV on single networks also delivered promising prediction performances.
CONCLUSION: The proposed methods provide a comprehensive and interpretable framework for graph-based modeling that may inform effective HIV prevention intervention strategies among populations most vulnerable to HIV.","Identifying influential neighbors in social networks and venue affiliations among young MSM: a data science approach to predict HIV infection OBJECTIVE: Young MSM (YMSM) bear a disproportionate burden of HIV infection in the United States and their risks of acquiring HIV may be shaped by complex multilayer social networks. These networks are formed through not only direct contact with social/sex partners but also indirect anonymous contacts encountered when attending social venues. We introduced a new application of a state-of-the-art graph-based deep learning method to predict HIV infection that can identify influential neighbors within these multiple network contexts.
DESIGN AND METHODS: We used empirical network data among YMSM aged 16-29 years old collected from Houston and Chicago in the United States between 2014 and 2016. A computational framework GAT-HIV (Graph Attention Networks for HIV) was proposed to predict HIV infections by identifying influential neighbors within social networks. These networks were formed by multiple relations constituted of social/sex partners and shared venue attendances, and using individual-level variables. Further, GAT-HIV was extended to combine multiple social networks using multigraph GAT methods. A visualization tool was also developed to highlight influential network members for each individual within the multiple social networks.
RESULTS: The multigraph GAT-HIV models obtained average AUC values of 0.776 and 0.824 for Chicago and Houston, respectively, performing better than empirical predictive models (e.g. AUCs of random forest: 0.758 and 0.798). GAT-HIV on single networks also delivered promising prediction performances.
CONCLUSION: The proposed methods provide a comprehensive and interpretable framework for graph-based modeling that may inform effective HIV prevention intervention strategies among populations most vulnerable to HIV.","[-0.15781876  0.16722886  0.17457254 ...  0.9903118  -0.1697642
 -0.15365505]",0.90769386,0.8037404,Other,deep learning
33301073,Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images,"Bayoudh K, Hamdaoui F, Mtibaa A.",Phys Eng Sci Med. 2020 Dec;43(4):1415-1431. doi: 10.1007/s13246-020-00957-1. Epub 2020 Dec 10.,Bayoudh K,Phys Eng Sci Med,2020,2020/12/10,PMC7726306,,10.1007/s13246-020-00957-1,"The novel Coronavirus disease (COVID-19), which first appeared at the end of December 2019, continues to spread rapidly in most countries of the world. Respiratory infections occur primarily in the majority of patients treated with COVID-19. In light of the growing number of COVID-19 cases, the need for diagnostic tools to identify COVID-19 infection at early stages is of vital importance. For decades, chest X-ray (CXR) technologies have proven their ability to accurately detect respiratory diseases. More recently, with the availability of COVID-19 CXR scans, deep learning algorithms have played a critical role in the healthcare arena by allowing radiologists to recognize COVID-19 patients from their CXR images. However, the majority of screening methods for COVID-19 reported in recent studies are based on 2D convolutional neural networks (CNNs). Although 3D CNNs are capable of capturing contextual information compared to their 2D counterparts, their use is limited due to their increased computational cost (i.e. requires much extra memory and much more computing power). In this study, a transfer learning-based hybrid 2D/3D CNN architecture for COVID-19 screening using CXRs has been developed. The proposed architecture consists of the incorporation of a pre-trained deep model (VGG16) and a shallow 3D CNN, combined with a depth-wise separable convolution layer and a spatial pyramid pooling module (SPP). Specifically, the depth-wise separable convolution helps to preserve the useful features while reducing the computational burden of the model. The SPP module is designed to extract multi-level representations from intermediate ones. Experimental results show that the proposed framework can achieve reasonable performances when evaluated on a collected dataset (3 classes to be predicted: COVID-19, Pneumonia, and Normal). Notably, it achieved a sensitivity of 98.33%, a specificity of 98.68% and an overall accuracy of 96.91.","Hybrid-COVID: a novel hybrid 2D/3D CNN based on cross-domain adaptation approach for COVID-19 screening from chest X-ray images The novel Coronavirus disease (COVID-19), which first appeared at the end of December 2019, continues to spread rapidly in most countries of the world. Respiratory infections occur primarily in the majority of patients treated with COVID-19. In light of the growing number of COVID-19 cases, the need for diagnostic tools to identify COVID-19 infection at early stages is of vital importance. For decades, chest X-ray (CXR) technologies have proven their ability to accurately detect respiratory diseases. More recently, with the availability of COVID-19 CXR scans, deep learning algorithms have played a critical role in the healthcare arena by allowing radiologists to recognize COVID-19 patients from their CXR images. However, the majority of screening methods for COVID-19 reported in recent studies are based on 2D convolutional neural networks (CNNs). Although 3D CNNs are capable of capturing contextual information compared to their 2D counterparts, their use is limited due to their increased computational cost (i.e. requires much extra memory and much more computing power). In this study, a transfer learning-based hybrid 2D/3D CNN architecture for COVID-19 screening using CXRs has been developed. The proposed architecture consists of the incorporation of a pre-trained deep model (VGG16) and a shallow 3D CNN, combined with a depth-wise separable convolution layer and a spatial pyramid pooling module (SPP). Specifically, the depth-wise separable convolution helps to preserve the useful features while reducing the computational burden of the model. The SPP module is designed to extract multi-level representations from intermediate ones. Experimental results show that the proposed framework can achieve reasonable performances when evaluated on a collected dataset (3 classes to be predicted: COVID-19, Pneumonia, and Normal). Notably, it achieved a sensitivity of 98.33%, a specificity of 98.68% and an overall accuracy of 96.91.","[-0.0758289   0.45832077  0.32100204 ...  0.7021509  -0.01603916
 -0.47629097]",0.9108919,0.818149,Computer Vision,"neural network, convolutional neural network, CNN, deep learning"
33166392,LitCovid: an open database of COVID-19 literature,"Chen Q, Allot A, Lu Z.",Nucleic Acids Res. 2021 Jan 8;49(D1):D1534-D1540. doi: 10.1093/nar/gkaa952.,Chen Q,Nucleic Acids Res,2021,2020/11/09,PMC7778958,,10.1093/nar/gkaa952,"Since the outbreak of the current pandemic in 2020, there has been a rapid growth of published articles on COVID-19 and SARS-CoV-2, with about 10,000 new articles added each month. This is causing an increasingly serious information overload, making it difficult for scientists, healthcare professionals and the general public to remain up to date on the latest SARS-CoV-2 and COVID-19 research. Hence, we developed LitCovid (https://www.ncbi.nlm.nih.gov/research/coronavirus/), a curated literature hub, to track up-to-date scientific information in PubMed. LitCovid is updated daily with newly identified relevant articles organized into curated categories. To support manual curation, advanced machine-learning and deep-learning algorithms have been developed, evaluated and integrated into the curation workflow. To the best of our knowledge, LitCovid is the first-of-its-kind COVID-19-specific literature resource, with all of its collected articles and curated data freely available. Since its release, LitCovid has been widely used, with millions of accesses by users worldwide for various information needs, such as evidence synthesis, drug discovery and text and data mining, among others.","LitCovid: an open database of COVID-19 literature Since the outbreak of the current pandemic in 2020, there has been a rapid growth of published articles on COVID-19 and SARS-CoV-2, with about 10,000 new articles added each month. This is causing an increasingly serious information overload, making it difficult for scientists, healthcare professionals and the general public to remain up to date on the latest SARS-CoV-2 and COVID-19 research. Hence, we developed LitCovid (https://www.ncbi.nlm.nih.gov/research/coronavirus/), a curated literature hub, to track up-to-date scientific information in PubMed. LitCovid is updated daily with newly identified relevant articles organized into curated categories. To support manual curation, advanced machine-learning and deep-learning algorithms have been developed, evaluated and integrated into the curation workflow. To the best of our knowledge, LitCovid is the first-of-its-kind COVID-19-specific literature resource, with all of its collected articles and curated data freely available. Since its release, LitCovid has been widely used, with millions of accesses by users worldwide for various information needs, such as evidence synthesis, drug discovery and text and data mining, among others.","[-4.3244831e-02  6.2001073e-01  4.4497498e-04 ...  5.0726330e-01
 -2.9561177e-01 -6.5793596e-02]",0.92176676,0.82003635,Both,Not Specified
33161334,The importance of standardisation - COVID-19 CT & Radiograph Image Data Stock for deep learning purpose,"Misztal K, Pocha A, Durak-Kozica M, Wątor M, Kubica-Misztal A, Hartel M.",Comput Biol Med. 2020 Dec;127:104092. doi: 10.1016/j.compbiomed.2020.104092. Epub 2020 Oct 28.,Misztal K,Comput Biol Med,2020,2020/11/08,PMC7591316,,10.1016/j.compbiomed.2020.104092,"With the number of affected individuals still growing world-wide, the research on COVID-19 is continuously expanding. The deep learning community concentrates their efforts on exploring if neural networks can potentially support the diagnosis using CT and radiograph images of patients' lungs. The two most popular publicly available datasets for COVID-19 classification are COVID-CT and COVID-19 Image Data Collection. In this work, we propose a new dataset which we call COVID-19 CT & Radiograph Image Data Stock. It contains both CT and radiograph samples of COVID-19 lung findings and combines them with additional data to ensure a sufficient number of diverse COVID-19-negative samples. Moreover, it is supplemented with a carefully defined split. The aim of COVID-19 CT & Radiograph Image Data Stock is to create a public pool of CT and radiograph images of lungs to increase the efficiency of distinguishing COVID-19 disease from other types of pneumonia and from healthy chest. We hope that the creation of this dataset would allow standardisation of the approach taken for training deep neural networks for COVID-19 classification and eventually for building more reliable models.","The importance of standardisation - COVID-19 CT & Radiograph Image Data Stock for deep learning purpose With the number of affected individuals still growing world-wide, the research on COVID-19 is continuously expanding. The deep learning community concentrates their efforts on exploring if neural networks can potentially support the diagnosis using CT and radiograph images of patients' lungs. The two most popular publicly available datasets for COVID-19 classification are COVID-CT and COVID-19 Image Data Collection. In this work, we propose a new dataset which we call COVID-19 CT & Radiograph Image Data Stock. It contains both CT and radiograph samples of COVID-19 lung findings and combines them with additional data to ensure a sufficient number of diverse COVID-19-negative samples. Moreover, it is supplemented with a carefully defined split. The aim of COVID-19 CT & Radiograph Image Data Stock is to create a public pool of CT and radiograph images of lungs to increase the efficiency of distinguishing COVID-19 disease from other types of pneumonia and from healthy chest. We hope that the creation of this dataset would allow standardisation of the approach taken for training deep neural networks for COVID-19 classification and eventually for building more reliable models.","[-0.05954257  0.74219173  0.02779115 ...  0.8210103  -0.0406261
 -0.48126253]",0.91125035,0.80412924,Both,"neural network, deep learning, deep neural networks"
32968435,Advancing COVID-19 differentiation with a robust preprocessing and integration of multi-institutional open-repository computer tomography datasets for deep learning analysis,"Trivizakis E, Tsiknakis N, Vassalou EE, Papadakis GZ, Spandidos DA, Sarigiannis D, Tsatsakis A, Papanikolaou N, Karantanas AH, Marias K.",Exp Ther Med. 2020 Nov;20(5):78. doi: 10.3892/etm.2020.9210. Epub 2020 Sep 11.,Trivizakis E,Exp Ther Med,2020,2020/09/24,PMC7500043,,10.3892/etm.2020.9210,"The coronavirus pandemic and its unprecedented consequences globally has spurred the interest of the artificial intelligence research community. A plethora of published studies have investigated the role of imaging such as chest X-rays and computer tomography in coronavirus disease 2019 (COVID-19) automated diagnosis. Οpen repositories of medical imaging data can play a significant role by promoting cooperation among institutes in a world-wide scale. However, they may induce limitations related to variable data quality and intrinsic differences due to the wide variety of scanner vendors and imaging parameters. In this study, a state-of-the-art custom U-Net model is presented with a dice similarity coefficient performance of 99.6% along with a transfer learning VGG-19 based model for COVID-19 versus pneumonia differentiation exhibiting an area under curve of 96.1%. The above was significantly improved over the baseline model trained with no segmentation in selected tomographic slices of the same dataset. The presented study highlights the importance of a robust preprocessing protocol for image analysis within a heterogeneous imaging dataset and assesses the potential diagnostic value of the presented COVID-19 model by comparing its performance to the state of the art.","Advancing COVID-19 differentiation with a robust preprocessing and integration of multi-institutional open-repository computer tomography datasets for deep learning analysis The coronavirus pandemic and its unprecedented consequences globally has spurred the interest of the artificial intelligence research community. A plethora of published studies have investigated the role of imaging such as chest X-rays and computer tomography in coronavirus disease 2019 (COVID-19) automated diagnosis. Οpen repositories of medical imaging data can play a significant role by promoting cooperation among institutes in a world-wide scale. However, they may induce limitations related to variable data quality and intrinsic differences due to the wide variety of scanner vendors and imaging parameters. In this study, a state-of-the-art custom U-Net model is presented with a dice similarity coefficient performance of 99.6% along with a transfer learning VGG-19 based model for COVID-19 versus pneumonia differentiation exhibiting an area under curve of 96.1%. The above was significantly improved over the baseline model trained with no segmentation in selected tomographic slices of the same dataset. The presented study highlights the importance of a robust preprocessing protocol for image analysis within a heterogeneous imaging dataset and assesses the potential diagnostic value of the presented COVID-19 model by comparing its performance to the state of the art.","[ 0.03489095  0.76679575  0.22197175 ...  0.44604912 -0.17688352
 -0.5235906 ]",0.9087155,0.80987656,Both,deep learning
32959234,COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images,"Zhang R, Guo Z, Sun Y, Lu Q, Xu Z, Yao Z, Duan M, Liu S, Ren Y, Huang L, Zhou F.",Interdiscip Sci. 2020 Dec;12(4):555-565. doi: 10.1007/s12539-020-00393-5. Epub 2020 Sep 21.,Zhang R,Interdiscip Sci,2020,2020/09/22,PMC7505483,,10.1007/s12539-020-00393-5,"The novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a major pandemic outbreak recently. Various diagnostic technologies have been under active development. The novel coronavirus disease (COVID-19) may induce pulmonary failures, and chest X-ray imaging becomes one of the major confirmed diagnostic technologies. The very limited number of publicly available samples has rendered the training of the deep neural networks unstable and inaccurate. This study proposed a two-step transfer learning pipeline and a deep residual network framework COVID19XrayNet for the COVID-19 detection problem based on chest X-ray images. COVID19XrayNet firstly tunes the transferred model on a large dataset of chest X-ray images, which is further tuned using a small dataset of annotated chest X-ray images. The final model achieved 0.9108 accuracy. The experimental data also suggested that the model may be improved with more training samples being released. COVID19XrayNet, a two-step transfer learning framework designed for biomedical images.","COVID19XrayNet: A Two-Step Transfer Learning Model for the COVID-19 Detecting Problem Based on a Limited Number of Chest X-Ray Images The novel coronavirus severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has caused a major pandemic outbreak recently. Various diagnostic technologies have been under active development. The novel coronavirus disease (COVID-19) may induce pulmonary failures, and chest X-ray imaging becomes one of the major confirmed diagnostic technologies. The very limited number of publicly available samples has rendered the training of the deep neural networks unstable and inaccurate. This study proposed a two-step transfer learning pipeline and a deep residual network framework COVID19XrayNet for the COVID-19 detection problem based on chest X-ray images. COVID19XrayNet firstly tunes the transferred model on a large dataset of chest X-ray images, which is further tuned using a small dataset of annotated chest X-ray images. The final model achieved 0.9108 accuracy. The experimental data also suggested that the model may be improved with more training samples being released. COVID19XrayNet, a two-step transfer learning framework designed for biomedical images.","[ 0.03520342  0.50810355  0.28546074 ...  0.6490544  -0.12602682
 -0.41417736]",0.9003308,0.8224402,Other,"neural network, deep neural networks"
32941452,Prediction of hepatitis E using machine learning models,"Guo Y, Feng Y, Qu F, Zhang L, Yan B, Lv J.",PLoS One. 2020 Sep 17;15(9):e0237750. doi: 10.1371/journal.pone.0237750. eCollection 2020.,Guo Y,PLoS One,2020,2020/09/17,PMC7497991,,10.1371/journal.pone.0237750,"BACKGROUND: Accurate and reliable predictions of infectious disease can be valuable to public health organizations that plan interventions to decrease or prevent disease transmission. A great variety of models have been developed for this task. However, for different data series, the performance of these models varies. Hepatitis E, as an acute liver disease, has been a major public health problem. Which model is more appropriate for predicting the incidence of hepatitis E? In this paper, three different methods are used and the performance of the three methods is compared.
METHODS: Autoregressive integrated moving average(ARIMA), support vector machine(SVM) and long short-term memory(LSTM) recurrent neural network were adopted and compared. ARIMA was implemented by python with the help of statsmodels. SVM was accomplished by matlab with libSVM library. LSTM was designed by ourselves with Keras, a deep learning library. To tackle the problem of overfitting caused by limited training samples, we adopted dropout and regularization strategies in our LSTM model. Experimental data were obtained from the monthly incidence and cases number of hepatitis E from January 2005 to December 2017 in Shandong province, China. We selected data from July 2015 to December 2017 to validate the models, and the rest was taken as training set. Three metrics were applied to compare the performance of models, including root mean square error(RMSE), mean absolute percentage error(MAPE) and mean absolute error(MAE).
RESULTS: By analyzing data, we took ARIMA(1, 1, 1), ARIMA(3, 1, 2) as monthly incidence prediction model and cases number prediction model, respectively. Cross-validation and grid search were used to optimize parameters of SVM. Penalty coefficient C and kernel function parameter g were set 8, 0.125 for incidence prediction, and 22, 0.01 for cases number prediction. LSTM has 4 nodes. Dropout and L2 regularization parameters were set 0.15, 0.001, respectively. By the metrics of RMSE, we obtained 0.022, 0.0204, 0.01 for incidence prediction, using ARIMA, SVM and LSTM. And we obtained 22.25, 20.0368, 11.75 for cases number prediction, using three models. For MAPE metrics, the results were 23.5%, 21.7%, 15.08%, and 23.6%, 21.44%, 13.6%, for incidence prediction and cases number prediction, respectively. For MAE metrics, the results were 0.018, 0.0167, 0.011 and 18.003, 16.5815, 9.984, for incidence prediction and cases number prediction, respectively.
CONCLUSIONS: Comparing ARIMA, SVM and LSTM, we found that nonlinear models(SVM, LSTM) outperform linear models(ARIMA). LSTM obtained the best performance in all three metrics of RSME, MAPE, MAE. Hence, LSTM is the most suitable for predicting hepatitis E monthly incidence and cases number.","Prediction of hepatitis E using machine learning models BACKGROUND: Accurate and reliable predictions of infectious disease can be valuable to public health organizations that plan interventions to decrease or prevent disease transmission. A great variety of models have been developed for this task. However, for different data series, the performance of these models varies. Hepatitis E, as an acute liver disease, has been a major public health problem. Which model is more appropriate for predicting the incidence of hepatitis E? In this paper, three different methods are used and the performance of the three methods is compared.
METHODS: Autoregressive integrated moving average(ARIMA), support vector machine(SVM) and long short-term memory(LSTM) recurrent neural network were adopted and compared. ARIMA was implemented by python with the help of statsmodels. SVM was accomplished by matlab with libSVM library. LSTM was designed by ourselves with Keras, a deep learning library. To tackle the problem of overfitting caused by limited training samples, we adopted dropout and regularization strategies in our LSTM model. Experimental data were obtained from the monthly incidence and cases number of hepatitis E from January 2005 to December 2017 in Shandong province, China. We selected data from July 2015 to December 2017 to validate the models, and the rest was taken as training set. Three metrics were applied to compare the performance of models, including root mean square error(RMSE), mean absolute percentage error(MAPE) and mean absolute error(MAE).
RESULTS: By analyzing data, we took ARIMA(1, 1, 1), ARIMA(3, 1, 2) as monthly incidence prediction model and cases number prediction model, respectively. Cross-validation and grid search were used to optimize parameters of SVM. Penalty coefficient C and kernel function parameter g were set 8, 0.125 for incidence prediction, and 22, 0.01 for cases number prediction. LSTM has 4 nodes. Dropout and L2 regularization parameters were set 0.15, 0.001, respectively. By the metrics of RMSE, we obtained 0.022, 0.0204, 0.01 for incidence prediction, using ARIMA, SVM and LSTM. And we obtained 22.25, 20.0368, 11.75 for cases number prediction, using three models. For MAPE metrics, the results were 23.5%, 21.7%, 15.08%, and 23.6%, 21.44%, 13.6%, for incidence prediction and cases number prediction, respectively. For MAE metrics, the results were 0.018, 0.0167, 0.011 and 18.003, 16.5815, 9.984, for incidence prediction and cases number prediction, respectively.
CONCLUSIONS: Comparing ARIMA, SVM and LSTM, we found that nonlinear models(SVM, LSTM) outperform linear models(ARIMA). LSTM obtained the best performance in all three metrics of RSME, MAPE, MAE. Hence, LSTM is the most suitable for predicting hepatitis E monthly incidence and cases number.","[-0.20728573  0.42945987  0.413493   ...  0.39100102  0.26660195
 -0.4339267 ]",0.905218,0.8263476,Other,"neural network, machine learning model, recurrent neural network, LSTM, deep learning"
32915901,"Inconsistency in the use of the term ""validation"" in studies reporting the performance of deep learning algorithms in providing diagnosis from medical imaging","Kim DW, Jang HY, Ko Y, Son JH, Kim PH, Kim SO, Lim JS, Park SH.",PLoS One. 2020 Sep 11;15(9):e0238908. doi: 10.1371/journal.pone.0238908. eCollection 2020.,Kim DW,PLoS One,2020,2020/09/11,PMC7485764,,10.1371/journal.pone.0238908,"BACKGROUND: The development of deep learning (DL) algorithms is a three-step process-training, tuning, and testing. Studies are inconsistent in the use of the term ""validation"", with some using it to refer to tuning and others testing, which hinders accurate delivery of information and may inadvertently exaggerate the performance of DL algorithms. We investigated the extent of inconsistency in usage of the term ""validation"" in studies on the accuracy of DL algorithms in providing diagnosis from medical imaging.
METHODS AND FINDINGS: We analyzed the full texts of research papers cited in two recent systematic reviews. The papers were categorized according to whether the term ""validation"" was used to refer to tuning alone, both tuning and testing, or testing alone. We analyzed whether paper characteristics (i.e., journal category, field of study, year of print publication, journal impact factor [JIF], and nature of test data) were associated with the usage of the terminology using multivariable logistic regression analysis with generalized estimating equations. Of 201 papers published in 125 journals, 118 (58.7%), 9 (4.5%), and 74 (36.8%) used the term to refer to tuning alone, both tuning and testing, and testing alone, respectively. A weak association was noted between higher JIF and using the term to refer to testing (i.e., testing alone or both tuning and testing) instead of tuning alone (vs. JIF <5; JIF 5 to 10: adjusted odds ratio 2.11, P = 0.042; JIF >10: adjusted odds ratio 2.41, P = 0.089). Journal category, field of study, year of print publication, and nature of test data were not significantly associated with the terminology usage.
CONCLUSIONS: Existing literature has a significant degree of inconsistency in using the term ""validation"" when referring to the steps in DL algorithm development. Efforts are needed to improve the accuracy and clarity in the terminology usage.","Inconsistency in the use of the term ""validation"" in studies reporting the performance of deep learning algorithms in providing diagnosis from medical imaging BACKGROUND: The development of deep learning (DL) algorithms is a three-step process-training, tuning, and testing. Studies are inconsistent in the use of the term ""validation"", with some using it to refer to tuning and others testing, which hinders accurate delivery of information and may inadvertently exaggerate the performance of DL algorithms. We investigated the extent of inconsistency in usage of the term ""validation"" in studies on the accuracy of DL algorithms in providing diagnosis from medical imaging.
METHODS AND FINDINGS: We analyzed the full texts of research papers cited in two recent systematic reviews. The papers were categorized according to whether the term ""validation"" was used to refer to tuning alone, both tuning and testing, or testing alone. We analyzed whether paper characteristics (i.e., journal category, field of study, year of print publication, journal impact factor [JIF], and nature of test data) were associated with the usage of the terminology using multivariable logistic regression analysis with generalized estimating equations. Of 201 papers published in 125 journals, 118 (58.7%), 9 (4.5%), and 74 (36.8%) used the term to refer to tuning alone, both tuning and testing, and testing alone, respectively. A weak association was noted between higher JIF and using the term to refer to testing (i.e., testing alone or both tuning and testing) instead of tuning alone (vs. JIF <5; JIF 5 to 10: adjusted odds ratio 2.11, P = 0.042; JIF >10: adjusted odds ratio 2.41, P = 0.089). Journal category, field of study, year of print publication, and nature of test data were not significantly associated with the terminology usage.
CONCLUSIONS: Existing literature has a significant degree of inconsistency in using the term ""validation"" when referring to the steps in DL algorithm development. Efforts are needed to improve the accuracy and clarity in the terminology usage.","[-0.095268    0.52342623  0.20510767 ...  0.5094142   0.08791176
 -0.2916397 ]",0.9024172,0.8130255,Both,deep learning
32832047,Identifying COVID19 from Chest CT Images: A Deep Convolutional Neural Networks Based Approach,"Mishra AK, Das SK, Roy P, Bandyopadhyay S.",J Healthc Eng. 2020 Aug 11;2020:8843664. doi: 10.1155/2020/8843664. eCollection 2020.,Mishra AK,J Healthc Eng,2020,2020/08/25,PMC7424536,,10.1155/2020/8843664,"Coronavirus Disease (COVID19) is a fast-spreading infectious disease that is currently causing a healthcare crisis around the world. Due to the current limitations of the reverse transcription-polymerase chain reaction (RT-PCR) based tests for detecting COVID19, recently radiology imaging based ideas have been proposed by various works. In this work, various Deep CNN based approaches are explored for detecting the presence of COVID19 from chest CT images. A decision fusion based approach is also proposed, which combines predictions from multiple individual models, to produce a final prediction. Experimental results show that the proposed decision fusion based approach is able to achieve above 86% results across all the performance metrics under consideration, with average AUROC and F1-Score being 0.883 and 0.867, respectively. The experimental observations suggest the potential applicability of such Deep CNN based approach in real diagnostic scenarios, which could be of very high utility in terms of achieving fast testing for COVID19.","Identifying COVID19 from Chest CT Images: A Deep Convolutional Neural Networks Based Approach Coronavirus Disease (COVID19) is a fast-spreading infectious disease that is currently causing a healthcare crisis around the world. Due to the current limitations of the reverse transcription-polymerase chain reaction (RT-PCR) based tests for detecting COVID19, recently radiology imaging based ideas have been proposed by various works. In this work, various Deep CNN based approaches are explored for detecting the presence of COVID19 from chest CT images. A decision fusion based approach is also proposed, which combines predictions from multiple individual models, to produce a final prediction. Experimental results show that the proposed decision fusion based approach is able to achieve above 86% results across all the performance metrics under consideration, with average AUROC and F1-Score being 0.883 and 0.867, respectively. The experimental observations suggest the potential applicability of such Deep CNN based approach in real diagnostic scenarios, which could be of very high utility in terms of achieving fast testing for COVID19.","[ 0.09072795  0.7235106   0.36448675 ...  0.8560662   0.05939393
 -0.5048601 ]",0.91397154,0.80094457,Other,"neural network, convolutional neural network, CNN"
32750891,Deep Bidirectional Classification Model for COVID-19 Disease Infected Patients,"Pathak Y, Shukla PK, Arya KV.",IEEE/ACM Trans Comput Biol Bioinform. 2021 Jul-Aug;18(4):1234-1241. doi: 10.1109/TCBB.2020.3009859. Epub 2021 Aug 6.,Pathak Y,IEEE/ACM Trans Comput Biol Bioinform,2021,2020/08/06,,,10.1109/TCBB.2020.3009859,"In December of 2019, a novel coronavirus (COVID-19) appeared in Wuhan city, China and has been reported in many countries with millions of people infected within only four months. Chest computed Tomography (CT) has proven to be a useful supplement to reverse transcription polymerase chain reaction (RT-PCR) and has been shown to have high sensitivity to diagnose this condition. Therefore, radiological examinations are becoming crucial in early examination of COVID-19 infection. Currently, CT findings have already been suggested as an important evidence for scientific examination of COVID-19 in Hubei, China. However, classification of patient from chest CT images is not an easy task. Therefore, in this paper, a deep bidirectional long short-term memory network with mixture density network (DBM) model is proposed. To tune the hyperparameters of the DBM model, a Memetic Adaptive Differential Evolution (MADE) algorithm is used. Extensive experiments are drawn by considering the benchmark chest-Computed Tomography (chest-CT) images datasets. Comparative analysis reveals that the proposed MADE-DBM model outperforms the competitive COVID-19 classification approaches in terms of various performance metrics. Therefore, the proposed MADE-DBM model can be used in real-time COVID-19 classification systems.","Deep Bidirectional Classification Model for COVID-19 Disease Infected Patients In December of 2019, a novel coronavirus (COVID-19) appeared in Wuhan city, China and has been reported in many countries with millions of people infected within only four months. Chest computed Tomography (CT) has proven to be a useful supplement to reverse transcription polymerase chain reaction (RT-PCR) and has been shown to have high sensitivity to diagnose this condition. Therefore, radiological examinations are becoming crucial in early examination of COVID-19 infection. Currently, CT findings have already been suggested as an important evidence for scientific examination of COVID-19 in Hubei, China. However, classification of patient from chest CT images is not an easy task. Therefore, in this paper, a deep bidirectional long short-term memory network with mixture density network (DBM) model is proposed. To tune the hyperparameters of the DBM model, a Memetic Adaptive Differential Evolution (MADE) algorithm is used. Extensive experiments are drawn by considering the benchmark chest-Computed Tomography (chest-CT) images datasets. Comparative analysis reveals that the proposed MADE-DBM model outperforms the competitive COVID-19 classification approaches in terms of various performance metrics. Therefore, the proposed MADE-DBM model can be used in real-time COVID-19 classification systems.","[ 0.01823167  0.61333585  0.34776682 ...  0.59075004 -0.07395393
 -0.62486583]",0.9159909,0.8057734,Other,long short-term memory network
32699245,Using deep learning to predict the hand-foot-and-mouth disease of enterovirus A71 subtype in Beijing from 2011 to 2018,"Wang Y, Cao Z, Zeng D, Wang X, Wang Q.",Sci Rep. 2020 Jul 22;10(1):12201. doi: 10.1038/s41598-020-68840-3.,Wang Y,Sci Rep,2020,2020/07/24,PMC7376109,,10.1038/s41598-020-68840-3,"Hand-foot-and-month disease (HFMD), especially the enterovirus A71 (EV-A71) subtype, is a major health problem in Beijing, China. Previous studies mainly used regressive models to forecast the prevalence of HFMD, ignoring its intrinsic age groups. This study aims to predict HFMD of EV-A71 subtype in three age groups (0-3, 3-6 and > 6 years old) from 2011 to 2018 using residual-convolutional-recurrent neural network (CNNRNN-Res), convolutional-recurrent neural network (CNNRNN) and recurrent neural network (RNN). They were compared with auto-regressio, global auto-regression and vector auto-regression on both short-term and long-term prediction. Results showed that CNNRNN-Res and RNN had higher accuracies on point forecast tasks, as well as robust performances in long-term prediction. Three deep learning models also had better skills in peak intensity forecast, and CNNRNN-Res achieved the best results in the peak month forecast. We also found that three age groups had consistent outbreak trends and similar patterns of prediction errors. These results highlight the superior performance of deep learning models in HFMD prediction and can assist the decision-makers to refine the HFMD control measures according to age groups.","Using deep learning to predict the hand-foot-and-mouth disease of enterovirus A71 subtype in Beijing from 2011 to 2018 Hand-foot-and-month disease (HFMD), especially the enterovirus A71 (EV-A71) subtype, is a major health problem in Beijing, China. Previous studies mainly used regressive models to forecast the prevalence of HFMD, ignoring its intrinsic age groups. This study aims to predict HFMD of EV-A71 subtype in three age groups (0-3, 3-6 and > 6 years old) from 2011 to 2018 using residual-convolutional-recurrent neural network (CNNRNN-Res), convolutional-recurrent neural network (CNNRNN) and recurrent neural network (RNN). They were compared with auto-regressio, global auto-regression and vector auto-regression on both short-term and long-term prediction. Results showed that CNNRNN-Res and RNN had higher accuracies on point forecast tasks, as well as robust performances in long-term prediction. Three deep learning models also had better skills in peak intensity forecast, and CNNRNN-Res achieved the best results in the peak month forecast. We also found that three age groups had consistent outbreak trends and similar patterns of prediction errors. These results highlight the superior performance of deep learning models in HFMD prediction and can assist the decision-makers to refine the HFMD control measures according to age groups.","[-0.21795043  0.42443565  0.5139267  ...  0.4378573  -0.1015101
 -0.44046792]",0.9107497,0.8072107,Other,"neural network, recurrent neural network, CNN, RNN, deep learning"
32308832,Machine Learning Based Opioid Overdose Prediction Using Electronic Health Records,"Dong X, Rashidian S, Wang Y, Hajagos J, Zhao X, Rosenthal RN, Kong J, Saltz M, Saltz J, Wang F.",AMIA Annu Symp Proc. 2020 Mar 4;2019:389-398. eCollection 2019.,Dong X,AMIA Annu Symp Proc,2020,2020/04/21,PMC7153049,,,"Opioid addiction in the United States has come to national attention as opioid overdose (OD) related deaths have risen at alarming rates. Combating opioid epidemic becomes a high priority for not only governments but also healthcare providers. This depends on critical knowledge to understand the risk of opioid overdose of patients. In this paper, we present our work on building machine learning based prediction models to predict opioid overdose of patients based on the history of patients' electronic health records (EHR). We performed two studies using New York State claims data (SPARCS) with 440,000 patients and Cerner's Health Facts database with 110,000 patients. Our experiments demonstrated that EHR based prediction can achieve best recall with random forest method (precision: 95.3%, recall: 85.7%, F1 score: 90.3%), best precision with deep learning (precision: 99.2%, recall: 77.8%, F1 score: 87.2%). We also discovered that clinical events are among critical features for the predictions.","Machine Learning Based Opioid Overdose Prediction Using Electronic Health Records Opioid addiction in the United States has come to national attention as opioid overdose (OD) related deaths have risen at alarming rates. Combating opioid epidemic becomes a high priority for not only governments but also healthcare providers. This depends on critical knowledge to understand the risk of opioid overdose of patients. In this paper, we present our work on building machine learning based prediction models to predict opioid overdose of patients based on the history of patients' electronic health records (EHR). We performed two studies using New York State claims data (SPARCS) with 440,000 patients and Cerner's Health Facts database with 110,000 patients. Our experiments demonstrated that EHR based prediction can achieve best recall with random forest method (precision: 95.3%, recall: 85.7%, F1 score: 90.3%), best precision with deep learning (precision: 99.2%, recall: 77.8%, F1 score: 87.2%). We also discovered that clinical events are among critical features for the predictions.","[-0.07024314  0.5014881   0.35557124 ...  0.6862316   0.01109115
 -0.31684098]",0.91709715,0.80487406,Other,deep learning
32253623,"Machine Learning in Dermatology: Current Applications, Opportunities, and Limitations","Chan S, Reddy V, Myers B, Thibodeaux Q, Brownstone N, Liao W.",Dermatol Ther (Heidelb). 2020 Jun;10(3):365-386. doi: 10.1007/s13555-020-00372-0. Epub 2020 Apr 6.,Chan S,Dermatol Ther (Heidelb),2020,2020/04/08,PMC7211783,,10.1007/s13555-020-00372-0,"Machine learning (ML) has the potential to improve the dermatologist's practice from diagnosis to personalized treatment. Recent advancements in access to large datasets (e.g., electronic medical records, image databases, omics), faster computing, and cheaper data storage have encouraged the development of ML algorithms with human-like intelligence in dermatology. This article is an overview of the basics of ML, current applications of ML, and potential limitations and considerations for further development of ML. We have identified five current areas of applications for ML in dermatology: (1) disease classification using clinical images; (2) disease classification using dermatopathology images; (3) assessment of skin diseases using mobile applications and personal monitoring devices; (4) facilitating large-scale epidemiology research; and (5) precision medicine. The purpose of this review is to provide a guide for dermatologists to help demystify the fundamentals of ML and its wide range of applications in order to better evaluate its potential opportunities and challenges.","Machine Learning in Dermatology: Current Applications, Opportunities, and Limitations Machine learning (ML) has the potential to improve the dermatologist's practice from diagnosis to personalized treatment. Recent advancements in access to large datasets (e.g., electronic medical records, image databases, omics), faster computing, and cheaper data storage have encouraged the development of ML algorithms with human-like intelligence in dermatology. This article is an overview of the basics of ML, current applications of ML, and potential limitations and considerations for further development of ML. We have identified five current areas of applications for ML in dermatology: (1) disease classification using clinical images; (2) disease classification using dermatopathology images; (3) assessment of skin diseases using mobile applications and personal monitoring devices; (4) facilitating large-scale epidemiology research; and (5) precision medicine. The purpose of this review is to provide a guide for dermatologists to help demystify the fundamentals of ML and its wide range of applications in order to better evaluate its potential opportunities and challenges.","[-0.19785944  0.79658103  0.6313716  ...  0.63744026 -0.4393233
 -0.5438461 ]",0.90667665,0.80134094,Both,Not Specified
31699071,Deep learning for pollen allergy surveillance from twitter in Australia,"Rong J, Michalska S, Subramani S, Du J, Wang H.",BMC Med Inform Decis Mak. 2019 Nov 8;19(1):208. doi: 10.1186/s12911-019-0921-x.,Rong J,BMC Med Inform Decis Mak,2019,2019/11/09,PMC6839169,,10.1186/s12911-019-0921-x,"BACKGROUND: The paper introduces a deep learning-based approach for real-time detection and insights generation about one of the most prevalent chronic conditions in Australia - Pollen allergy. The popular social media platform is used for data collection as cost-effective and unobtrusive alternative for public health monitoring to complement the traditional survey-based approaches.
METHODS: The data was extracted from Twitter based on pre-defined keywords (i.e. 'hayfever' OR 'hay fever') throughout the period of 6 months, covering the high pollen season in Australia. The following deep learning architectures were adopted in the experiments: CNN, RNN, LSTM and GRU. Both default (GloVe) and domain-specific (HF) word embeddings were used in training the classifiers. Standard evaluation metrics (i.e. Accuracy, Precision and Recall) were calculated for the results validation. Finally, visual correlation with weather variables was performed.
RESULTS: The neural networks-based approach was able to correctly identify the implicit mentions of the symptoms and treatments, even unseen previously (accuracy up to 87.9% for GRU with GloVe embeddings of 300 dimensions).
CONCLUSIONS: The system addresses the shortcomings of the conventional machine learning techniques with manual feature-engineering that prove limiting when exposed to a wide range of non-standard expressions relating to medical concepts. The case-study presented demonstrates an application of 'black-box' approach to the real-world problem, along with its internal workings demonstration towards more transparent, interpretable and reproducible decision-making in health informatics domain.","Deep learning for pollen allergy surveillance from twitter in Australia BACKGROUND: The paper introduces a deep learning-based approach for real-time detection and insights generation about one of the most prevalent chronic conditions in Australia - Pollen allergy. The popular social media platform is used for data collection as cost-effective and unobtrusive alternative for public health monitoring to complement the traditional survey-based approaches.
METHODS: The data was extracted from Twitter based on pre-defined keywords (i.e. 'hayfever' OR 'hay fever') throughout the period of 6 months, covering the high pollen season in Australia. The following deep learning architectures were adopted in the experiments: CNN, RNN, LSTM and GRU. Both default (GloVe) and domain-specific (HF) word embeddings were used in training the classifiers. Standard evaluation metrics (i.e. Accuracy, Precision and Recall) were calculated for the results validation. Finally, visual correlation with weather variables was performed.
RESULTS: The neural networks-based approach was able to correctly identify the implicit mentions of the symptoms and treatments, even unseen previously (accuracy up to 87.9% for GRU with GloVe embeddings of 300 dimensions).
CONCLUSIONS: The system addresses the shortcomings of the conventional machine learning techniques with manual feature-engineering that prove limiting when exposed to a wide range of non-standard expressions relating to medical concepts. The case-study presented demonstrates an application of 'black-box' approach to the real-world problem, along with its internal workings demonstration towards more transparent, interpretable and reproducible decision-making in health informatics domain.","[-0.04836225  0.3942178   0.25930092 ...  0.92615414 -0.21448481
 -0.5698211 ]",0.9112196,0.8026203,Both,"neural network, CNN, RNN, LSTM, deep learning"
31583282,Medical device surveillance with electronic health records,"Callahan A, Fries JA, Ré C, Huddleston JI 3rd, Giori NJ, Delp S, Shah NH.",NPJ Digit Med. 2019 Sep 25;2:94. doi: 10.1038/s41746-019-0168-z. eCollection 2019.,Callahan A,NPJ Digit Med,2019,2019/10/05,PMC6761113,,10.1038/s41746-019-0168-z,"Post-market medical device surveillance is a challenge facing manufacturers, regulatory agencies, and health care providers. Electronic health records are valuable sources of real-world evidence for assessing device safety and tracking device-related patient outcomes over time. However, distilling this evidence remains challenging, as information is fractured across clinical notes and structured records. Modern machine learning methods for machine reading promise to unlock increasingly complex information from text, but face barriers due to their reliance on large and expensive hand-labeled training sets. To address these challenges, we developed and validated state-of-the-art deep learning methods that identify patient outcomes from clinical notes without requiring hand-labeled training data. Using hip replacements-one of the most common implantable devices-as a test case, our methods accurately extracted implant details and reports of complications and pain from electronic health records with up to 96.3% precision, 98.5% recall, and 97.4% F1, improved classification performance by 12.8-53.9% over rule-based methods, and detected over six times as many complication events compared to using structured data alone. Using these additional events to assess complication-free survivorship of different implant systems, we found significant variation between implants, including for risk of revision surgery, which could not be detected using coded data alone. Patients with revision surgeries had more hip pain mentions in the post-hip replacement, pre-revision period compared to patients with no evidence of revision surgery (mean hip pain mentions 4.97 vs. 3.23; t = 5.14; p &lt; 0.001). Some implant models were associated with higher or lower rates of hip pain mentions. Our methods complement existing surveillance mechanisms by requiring orders of magnitude less hand-labeled training data, offering a scalable solution for national medical device surveillance using electronic health records.","Medical device surveillance with electronic health records Post-market medical device surveillance is a challenge facing manufacturers, regulatory agencies, and health care providers. Electronic health records are valuable sources of real-world evidence for assessing device safety and tracking device-related patient outcomes over time. However, distilling this evidence remains challenging, as information is fractured across clinical notes and structured records. Modern machine learning methods for machine reading promise to unlock increasingly complex information from text, but face barriers due to their reliance on large and expensive hand-labeled training sets. To address these challenges, we developed and validated state-of-the-art deep learning methods that identify patient outcomes from clinical notes without requiring hand-labeled training data. Using hip replacements-one of the most common implantable devices-as a test case, our methods accurately extracted implant details and reports of complications and pain from electronic health records with up to 96.3% precision, 98.5% recall, and 97.4% F1, improved classification performance by 12.8-53.9% over rule-based methods, and detected over six times as many complication events compared to using structured data alone. Using these additional events to assess complication-free survivorship of different implant systems, we found significant variation between implants, including for risk of revision surgery, which could not be detected using coded data alone. Patients with revision surgeries had more hip pain mentions in the post-hip replacement, pre-revision period compared to patients with no evidence of revision surgery (mean hip pain mentions 4.97 vs. 3.23; t = 5.14; p &lt; 0.001). Some implant models were associated with higher or lower rates of hip pain mentions. Our methods complement existing surveillance mechanisms by requiring orders of magnitude less hand-labeled training data, offering a scalable solution for national medical device surveillance using electronic health records.","[ 0.0018648   0.5146006   0.41527277 ...  0.38565812 -0.11907557
 -0.48212054]",0.9074434,0.80829775,Both,deep learning
32381039,Generation and evaluation of synthetic patient data,"Goncalves A, Ray P, Soper B, Stevens J, Coyle L, Sales AP.",BMC Med Res Methodol. 2020 May 7;20(1):108. doi: 10.1186/s12874-020-00977-1.,Goncalves A,BMC Med Res Methodol,2020,2020/05/09,PMC7204018,,10.1186/s12874-020-00977-1,"BACKGROUND: Machine learning (ML) has made a significant impact in medicine and cancer research; however, its impact in these areas has been undeniably slower and more limited than in other application domains. A major reason for this has been the lack of availability of patient data to the broader ML research community, in large part due to patient privacy protection concerns. High-quality, realistic, synthetic datasets can be leveraged to accelerate methodological developments in medicine. By and large, medical data is high dimensional and often categorical. These characteristics pose multiple modeling challenges.
METHODS: In this paper, we evaluate three classes of synthetic data generation approaches; probabilistic models, classification-based imputation models, and generative adversarial neural networks. Metrics for evaluating the quality of the generated synthetic datasets are presented and discussed.
RESULTS: While the results and discussions are broadly applicable to medical data, for demonstration purposes we generate synthetic datasets for cancer based on the publicly available cancer registry data from the Surveillance Epidemiology and End Results (SEER) program. Specifically, our cohort consists of breast, respiratory, and non-solid cancer cases diagnosed between 2010 and 2015, which includes over 360,000 individual cases.
CONCLUSIONS: We discuss the trade-offs of the different methods and metrics, providing guidance on considerations for the generation and usage of medical synthetic data.","Generation and evaluation of synthetic patient data BACKGROUND: Machine learning (ML) has made a significant impact in medicine and cancer research; however, its impact in these areas has been undeniably slower and more limited than in other application domains. A major reason for this has been the lack of availability of patient data to the broader ML research community, in large part due to patient privacy protection concerns. High-quality, realistic, synthetic datasets can be leveraged to accelerate methodological developments in medicine. By and large, medical data is high dimensional and often categorical. These characteristics pose multiple modeling challenges.
METHODS: In this paper, we evaluate three classes of synthetic data generation approaches; probabilistic models, classification-based imputation models, and generative adversarial neural networks. Metrics for evaluating the quality of the generated synthetic datasets are presented and discussed.
RESULTS: While the results and discussions are broadly applicable to medical data, for demonstration purposes we generate synthetic datasets for cancer based on the publicly available cancer registry data from the Surveillance Epidemiology and End Results (SEER) program. Specifically, our cohort consists of breast, respiratory, and non-solid cancer cases diagnosed between 2010 and 2015, which includes over 360,000 individual cases.
CONCLUSIONS: We discuss the trade-offs of the different methods and metrics, providing guidance on considerations for the generation and usage of medical synthetic data.","[-0.12270575  0.79209167  0.3487054  ...  0.694953   -0.07030118
 -0.4511668 ]",0.9093726,0.8043682,Both,neural network
39167785,Comparing GPT-4 and Human Researchers in Health Care Data Analysis: Qualitative Description Study,"Li KD, Fernandez AM, Schwartz R, Rios N, Carlisle MN, Amend GM, Patel HV, Breyer BN.",J Med Internet Res. 2024 Aug 21;26:e56500. doi: 10.2196/56500.,Li KD,J Med Internet Res,2024,2024/08/21,PMC11375389,,10.2196/56500,"BACKGROUND: Large language models including GPT-4 (OpenAI) have opened new avenues in health care and qualitative research. Traditional qualitative methods are time-consuming and require expertise to capture nuance. Although large language models have demonstrated enhanced contextual understanding and inferencing compared with traditional natural language processing, their performance in qualitative analysis versus that of humans remains unexplored.
OBJECTIVE: We evaluated the effectiveness of GPT-4 versus human researchers in qualitative analysis of interviews with patients with adult-acquired buried penis (AABP).
METHODS: Qualitative data were obtained from semistructured interviews with 20 patients with AABP. Human analysis involved a structured 3-stage process-initial observations, line-by-line coding, and consensus discussions to refine themes. In contrast, artificial intelligence (AI) analysis with GPT-4 underwent two phases: (1) a naïve phase, where GPT-4 outputs were independently evaluated by a blinded reviewer to identify themes and subthemes and (2) a comparison phase, where AI-generated themes were compared with human-identified themes to assess agreement. We used a general qualitative description approach.
RESULTS: The study population (N=20) comprised predominantly White (17/20, 85%), married (12/20, 60%), heterosexual (19/20, 95%) men, with a mean age of 58.8 years and BMI of 41.1 kg/m2. Human qualitative analysis identified ""urinary issues"" in 95% (19/20) and GPT-4 in 75% (15/20) of interviews, with the subtheme ""spray or stream"" noted in 60% (12/20) and 35% (7/20), respectively. ""Sexual issues"" were prominent (19/20, 95% humans vs 16/20, 80% GPT-4), although humans identified a wider range of subthemes, including ""pain with sex or masturbation"" (7/20, 35%) and ""difficulty with sex or masturbation"" (4/20, 20%). Both analyses similarly highlighted ""mental health issues"" (11/20, 55%, both), although humans coded ""depression"" more frequently (10/20, 50% humans vs 4/20, 20% GPT-4). Humans frequently cited ""issues using public restrooms"" (12/20, 60%) as impacting social life, whereas GPT-4 emphasized ""struggles with romantic relationships"" (9/20, 45%). ""Hygiene issues"" were consistently recognized (14/20, 70% humans vs 13/20, 65% GPT-4). Humans uniquely identified ""contributing factors"" as a theme in all interviews. There was moderate agreement between human and GPT-4 coding (κ=0.401). Reliability assessments of GPT-4's analyses showed consistent coding for themes including ""body image struggles,"" ""chronic pain"" (10/10, 100%), and ""depression"" (9/10, 90%). Other themes like ""motivation for surgery"" and ""weight challenges"" were reliably coded (8/10, 80%), while less frequent themes were variably identified across multiple iterations.
CONCLUSIONS: Large language models including GPT-4 can effectively identify key themes in analyzing qualitative health care data, showing moderate agreement with human analysis. While human analysis provided a richer diversity of subthemes, the consistency of AI suggests its use as a complementary tool in qualitative research. With AI rapidly advancing, future studies should iterate analyses and circumvent token limitations by segmenting data, furthering the breadth and depth of large language model-driven qualitative analyses.","Comparing GPT-4 and Human Researchers in Health Care Data Analysis: Qualitative Description Study BACKGROUND: Large language models including GPT-4 (OpenAI) have opened new avenues in health care and qualitative research. Traditional qualitative methods are time-consuming and require expertise to capture nuance. Although large language models have demonstrated enhanced contextual understanding and inferencing compared with traditional natural language processing, their performance in qualitative analysis versus that of humans remains unexplored.
OBJECTIVE: We evaluated the effectiveness of GPT-4 versus human researchers in qualitative analysis of interviews with patients with adult-acquired buried penis (AABP).
METHODS: Qualitative data were obtained from semistructured interviews with 20 patients with AABP. Human analysis involved a structured 3-stage process-initial observations, line-by-line coding, and consensus discussions to refine themes. In contrast, artificial intelligence (AI) analysis with GPT-4 underwent two phases: (1) a naïve phase, where GPT-4 outputs were independently evaluated by a blinded reviewer to identify themes and subthemes and (2) a comparison phase, where AI-generated themes were compared with human-identified themes to assess agreement. We used a general qualitative description approach.
RESULTS: The study population (N=20) comprised predominantly White (17/20, 85%), married (12/20, 60%), heterosexual (19/20, 95%) men, with a mean age of 58.8 years and BMI of 41.1 kg/m2. Human qualitative analysis identified ""urinary issues"" in 95% (19/20) and GPT-4 in 75% (15/20) of interviews, with the subtheme ""spray or stream"" noted in 60% (12/20) and 35% (7/20), respectively. ""Sexual issues"" were prominent (19/20, 95% humans vs 16/20, 80% GPT-4), although humans identified a wider range of subthemes, including ""pain with sex or masturbation"" (7/20, 35%) and ""difficulty with sex or masturbation"" (4/20, 20%). Both analyses similarly highlighted ""mental health issues"" (11/20, 55%, both), although humans coded ""depression"" more frequently (10/20, 50% humans vs 4/20, 20% GPT-4). Humans frequently cited ""issues using public restrooms"" (12/20, 60%) as impacting social life, whereas GPT-4 emphasized ""struggles with romantic relationships"" (9/20, 45%). ""Hygiene issues"" were consistently recognized (14/20, 70% humans vs 13/20, 65% GPT-4). Humans uniquely identified ""contributing factors"" as a theme in all interviews. There was moderate agreement between human and GPT-4 coding (κ=0.401). Reliability assessments of GPT-4's analyses showed consistent coding for themes including ""body image struggles,"" ""chronic pain"" (10/10, 100%), and ""depression"" (9/10, 90%). Other themes like ""motivation for surgery"" and ""weight challenges"" were reliably coded (8/10, 80%), while less frequent themes were variably identified across multiple iterations.
CONCLUSIONS: Large language models including GPT-4 can effectively identify key themes in analyzing qualitative health care data, showing moderate agreement with human analysis. While human analysis provided a richer diversity of subthemes, the consistency of AI suggests its use as a complementary tool in qualitative research. With AI rapidly advancing, future studies should iterate analyses and circumvent token limitations by segmenting data, furthering the breadth and depth of large language model-driven qualitative analyses.","[-0.24913071  0.42092326  0.3482123  ...  0.50977033  0.5233008
 -0.33084032]",0.9014834,0.8131548,Text Mining,"natural language processing, language processing, large language model"
39090273,The policies on the use of large language models in radiological journals are lacking: a meta-research study,"Zhong J, Xing Y, Hu Y, Lu J, Yang J, Zhang G, Mao S, Chen H, Yin Q, Cen Q, Jiang R, Chu J, Song Y, Lu M, Ding D, Ge X, Zhang H, Yao W.",Insights Imaging. 2024 Aug 1;15(1):186. doi: 10.1186/s13244-024-01769-7.,Zhong J,Insights Imaging,2024,2024/08/01,PMC11294318,,10.1186/s13244-024-01769-7,"OBJECTIVE: To evaluate whether and how the radiological journals present their policies on the use of large language models (LLMs), and identify the journal characteristic variables that are associated with the presence.
METHODS: In this meta-research study, we screened Journals from the Radiology, Nuclear Medicine and Medical Imaging Category, 2022 Journal Citation Reports, excluding journals in non-English languages and relevant documents unavailable. We assessed their LLM use policies: (1) whether the policy is present; (2) whether the policy for the authors, the reviewers, and the editors is present; and (3) whether the policy asks the author to report the usage of LLMs, the name of LLMs, the section that used LLMs, the role of LLMs, the verification of LLMs, and the potential influence of LLMs. The association between the presence of policies and journal characteristic variables was evaluated.
RESULTS: The LLM use policies were presented in 43.9% (83/189) of journals, and those for the authors, the reviewers, and the editor were presented in 43.4% (82/189), 29.6% (56/189) and 25.9% (49/189) of journals, respectively. Many journals mentioned the aspects of the usage (43.4%, 82/189), the name (34.9%, 66/189), the verification (33.3%, 63/189), and the role (31.7%, 60/189) of LLMs, while the potential influence of LLMs (4.2%, 8/189), and the section that used LLMs (1.6%, 3/189) were seldomly touched. The publisher is related to the presence of LLM use policies (p < 0.001).
CONCLUSION: The presence of LLM use policies is suboptimal in radiological journals. A reporting guideline is encouraged to facilitate reporting quality and transparency.
CRITICAL RELEVANCE STATEMENT: It may facilitate the quality and transparency of the use of LLMs in scientific writing if a shared complete reporting guideline is developed by stakeholders and then endorsed by journals.
KEY POINTS: The policies on LLM use in radiological journals are unexplored. Some of the radiological journals presented policies on LLM use. A shared complete reporting guideline for LLM use is desired.","The policies on the use of large language models in radiological journals are lacking: a meta-research study OBJECTIVE: To evaluate whether and how the radiological journals present their policies on the use of large language models (LLMs), and identify the journal characteristic variables that are associated with the presence.
METHODS: In this meta-research study, we screened Journals from the Radiology, Nuclear Medicine and Medical Imaging Category, 2022 Journal Citation Reports, excluding journals in non-English languages and relevant documents unavailable. We assessed their LLM use policies: (1) whether the policy is present; (2) whether the policy for the authors, the reviewers, and the editors is present; and (3) whether the policy asks the author to report the usage of LLMs, the name of LLMs, the section that used LLMs, the role of LLMs, the verification of LLMs, and the potential influence of LLMs. The association between the presence of policies and journal characteristic variables was evaluated.
RESULTS: The LLM use policies were presented in 43.9% (83/189) of journals, and those for the authors, the reviewers, and the editor were presented in 43.4% (82/189), 29.6% (56/189) and 25.9% (49/189) of journals, respectively. Many journals mentioned the aspects of the usage (43.4%, 82/189), the name (34.9%, 66/189), the verification (33.3%, 63/189), and the role (31.7%, 60/189) of LLMs, while the potential influence of LLMs (4.2%, 8/189), and the section that used LLMs (1.6%, 3/189) were seldomly touched. The publisher is related to the presence of LLM use policies (p < 0.001).
CONCLUSION: The presence of LLM use policies is suboptimal in radiological journals. A reporting guideline is encouraged to facilitate reporting quality and transparency.
CRITICAL RELEVANCE STATEMENT: It may facilitate the quality and transparency of the use of LLMs in scientific writing if a shared complete reporting guideline is developed by stakeholders and then endorsed by journals.
KEY POINTS: The policies on LLM use in radiological journals are unexplored. Some of the radiological journals presented policies on LLM use. A shared complete reporting guideline for LLM use is desired.","[-0.27979603  0.34566128  0.12939145 ...  0.367217    0.06305098
 -0.08095933]",0.90007275,0.81500393,Text Mining,"large language model, LLM"
39379449,Scalable incident detection via natural language processing and probabilistic language models,"Walsh CG, Wilimitis D, Chen Q, Wright A, Kolli J, Robinson K, Ripperger MA, Johnson KB, Carrell D, Desai RJ, Mosholder A, Dharmarajan S, Adimadhyam S, Fabbri D, Stojanovic D, Matheny ME, Bejan CA.",Sci Rep. 2024 Oct 8;14(1):23429. doi: 10.1038/s41598-024-72756-7.,Walsh CG,Sci Rep,2024,2024/10/08,PMC11461638,,10.1038/s41598-024-72756-7,"Post marketing safety surveillance depends in part on the ability to detect concerning clinical events at scale. Spontaneous reporting might be an effective component of safety surveillance, but it requires awareness and understanding among healthcare professionals to achieve its potential. Reliance on readily available structured data such as diagnostic codes risks under-coding and imprecision. Clinical textual data might bridge these gaps, and natural language processing (NLP) has been shown to aid in scalable phenotyping across healthcare records in multiple clinical domains. In this study, we developed and validated a novel incident phenotyping approach using unstructured clinical textual data agnostic to Electronic Health Record (EHR) and note type. It's based on a published, validated approach (PheRe) used to ascertain social determinants of health and suicidality across entire healthcare records. To demonstrate generalizability, we validated this approach on two separate phenotypes that share common challenges with respect to accurate ascertainment: (1) suicide attempt; (2) sleep-related behaviors. With samples of 89,428 records and 35,863 records for suicide attempt and sleep-related behaviors, respectively, we conducted silver standard (diagnostic coding) and gold standard (manual chart review) validation. We showed Area Under the Precision-Recall Curve of ~ 0.77 (95% CI 0.75-0.78) for suicide attempt and AUPR ~ 0.31 (95% CI 0.28-0.34) for sleep-related behaviors. We also evaluated performance by coded race and demonstrated differences in performance by race differed across phenotypes. Scalable phenotyping models, like most healthcare AI, require algorithmovigilance and debiasing prior to implementation.","Scalable incident detection via natural language processing and probabilistic language models Post marketing safety surveillance depends in part on the ability to detect concerning clinical events at scale. Spontaneous reporting might be an effective component of safety surveillance, but it requires awareness and understanding among healthcare professionals to achieve its potential. Reliance on readily available structured data such as diagnostic codes risks under-coding and imprecision. Clinical textual data might bridge these gaps, and natural language processing (NLP) has been shown to aid in scalable phenotyping across healthcare records in multiple clinical domains. In this study, we developed and validated a novel incident phenotyping approach using unstructured clinical textual data agnostic to Electronic Health Record (EHR) and note type. It's based on a published, validated approach (PheRe) used to ascertain social determinants of health and suicidality across entire healthcare records. To demonstrate generalizability, we validated this approach on two separate phenotypes that share common challenges with respect to accurate ascertainment: (1) suicide attempt; (2) sleep-related behaviors. With samples of 89,428 records and 35,863 records for suicide attempt and sleep-related behaviors, respectively, we conducted silver standard (diagnostic coding) and gold standard (manual chart review) validation. We showed Area Under the Precision-Recall Curve of ~ 0.77 (95% CI 0.75-0.78) for suicide attempt and AUPR ~ 0.31 (95% CI 0.28-0.34) for sleep-related behaviors. We also evaluated performance by coded race and demonstrated differences in performance by race differed across phenotypes. Scalable phenotyping models, like most healthcare AI, require algorithmovigilance and debiasing prior to implementation.","[-0.08976084  0.72238094  0.32044032 ...  0.32425207 -0.16479652
 -0.4528624 ]",0.909842,0.8055847,Both,"natural language processing, NLP, language processing"
39269743,The Use of Natural Language Processing Methods in Reddit to Investigate Opioid Use: Scoping Review,"Almeida A, Patton T, Conway M, Gupta A, Strathdee SA, Bórquez A.",JMIR Infodemiology. 2024 Sep 13;4:e51156. doi: 10.2196/51156.,Almeida A,JMIR Infodemiology,2024,2024/09/13,PMC11437337,,10.2196/51156,"BACKGROUND: The growing availability of big data spontaneously generated by social media platforms allows us to leverage natural language processing (NLP) methods as valuable tools to understand the opioid crisis.
OBJECTIVE: We aimed to understand how NLP has been applied to Reddit (Reddit Inc) data to study opioid use.
METHODS: We systematically searched for peer-reviewed studies and conference abstracts in PubMed, Scopus, PsycINFO, ACL Anthology, IEEE Xplore, and Association for Computing Machinery data repositories up to July 19, 2022. Inclusion criteria were studies investigating opioid use, using NLP techniques to analyze the textual corpora, and using Reddit as the social media data source. We were specifically interested in mapping studies' overarching goals and findings, methodologies and software used, and main limitations.
RESULTS: In total, 30 studies were included, which were classified into 4 nonmutually exclusive overarching goal categories: methodological (n=6, 20% studies), infodemiology (n=22, 73% studies), infoveillance (n=7, 23% studies), and pharmacovigilance (n=3, 10% studies). NLP methods were used to identify content relevant to opioid use among vast quantities of textual data, to establish potential relationships between opioid use patterns or profiles and contextual factors or comorbidities, and to anticipate individuals' transitions between different opioid-related subreddits, likely revealing progression through opioid use stages. Most studies used an embedding technique (12/30, 40%), prediction or classification approach (12/30, 40%), topic modeling (9/30, 30%), and sentiment analysis (6/30, 20%). The most frequently used programming languages were Python (20/30, 67%) and R (2/30, 7%). Among the studies that reported limitations (20/30, 67%), the most cited was the uncertainty regarding whether redditors participating in these forums were representative of people who use opioids (8/20, 40%). The papers were very recent (28/30, 93%), from 2019 to 2022, with authors from a range of disciplines.
CONCLUSIONS: This scoping review identified a wide variety of NLP techniques and applications used to support surveillance and social media interventions addressing the opioid crisis. Despite the clear potential of these methods to enable the identification of opioid-relevant content in Reddit and its analysis, there are limits to the degree of interpretive meaning that they can provide. Moreover, we identified the need for standardized ethical guidelines to govern the use of Reddit data to safeguard the anonymity and privacy of people using these forums.","The Use of Natural Language Processing Methods in Reddit to Investigate Opioid Use: Scoping Review BACKGROUND: The growing availability of big data spontaneously generated by social media platforms allows us to leverage natural language processing (NLP) methods as valuable tools to understand the opioid crisis.
OBJECTIVE: We aimed to understand how NLP has been applied to Reddit (Reddit Inc) data to study opioid use.
METHODS: We systematically searched for peer-reviewed studies and conference abstracts in PubMed, Scopus, PsycINFO, ACL Anthology, IEEE Xplore, and Association for Computing Machinery data repositories up to July 19, 2022. Inclusion criteria were studies investigating opioid use, using NLP techniques to analyze the textual corpora, and using Reddit as the social media data source. We were specifically interested in mapping studies' overarching goals and findings, methodologies and software used, and main limitations.
RESULTS: In total, 30 studies were included, which were classified into 4 nonmutually exclusive overarching goal categories: methodological (n=6, 20% studies), infodemiology (n=22, 73% studies), infoveillance (n=7, 23% studies), and pharmacovigilance (n=3, 10% studies). NLP methods were used to identify content relevant to opioid use among vast quantities of textual data, to establish potential relationships between opioid use patterns or profiles and contextual factors or comorbidities, and to anticipate individuals' transitions between different opioid-related subreddits, likely revealing progression through opioid use stages. Most studies used an embedding technique (12/30, 40%), prediction or classification approach (12/30, 40%), topic modeling (9/30, 30%), and sentiment analysis (6/30, 20%). The most frequently used programming languages were Python (20/30, 67%) and R (2/30, 7%). Among the studies that reported limitations (20/30, 67%), the most cited was the uncertainty regarding whether redditors participating in these forums were representative of people who use opioids (8/20, 40%). The papers were very recent (28/30, 93%), from 2019 to 2022, with authors from a range of disciplines.
CONCLUSIONS: This scoping review identified a wide variety of NLP techniques and applications used to support surveillance and social media interventions addressing the opioid crisis. Despite the clear potential of these methods to enable the identification of opioid-relevant content in Reddit and its analysis, there are limits to the degree of interpretive meaning that they can provide. Moreover, we identified the need for standardized ethical guidelines to govern the use of Reddit data to safeguard the anonymity and privacy of people using these forums.","[-0.16081241  0.19570254  0.12798573 ...  0.7343638  -0.10560893
 -0.36822912]",0.9066003,0.8059511,Both,"natural language processing, NLP, language processing"
39185518,CarD-T: Interpreting Carcinomic Lexicon via Transformers,"O'Neill J, Reddy GA, Dhillon N, Tripathi O, Alexandrov L, Katira P.",medRxiv [Preprint]. 2024 Aug 31:2024.08.13.24311948. doi: 10.1101/2024.08.13.24311948.,O'Neill J,medRxiv,2024,2024/08/26,PMC11343268,,10.1101/2024.08.13.24311948,"The identification and classification of carcinogens is critical in cancer epidemiology, necessitating updated methodologies to manage the burgeoning biomedical literature. Current systems, like those run by the International Agency for Research on Cancer (IARC) and the National Toxicology Program (NTP), face challenges due to manual vetting and disparities in carcinogen classification spurred by the volume of emerging data. To address these issues, we introduced the Carcinogen Detection via Transformers (CarD-T) framework, a text analytics approach that combines transformer-based machine learning with probabilistic statistical analysis to efficiently nominate carcinogens from scientific texts. CarD-T uses Named Entity Recognition (NER) trained on PubMed abstracts featuring known carcinogens from IARC groups and includes a context classifier to enhance accuracy and manage computational demands. Using this method, journal publication data indexed with carcinogenicity & carcinogenesis Medical Subject Headings (MeSH) terms from the last 25 years was analyzed, identifying potential carcinogens. Training CarD-T on 60% of established carcinogens (Group 1 and 2A carcinogens, IARC designation), CarD-T correctly to identifies all of the remaining Group 1 and 2A designated carcinogens from the analyzed text. In addition, CarD-T nominates roughly 1500 more entities as potential carcinogens that have at least two publications citing evidence of carcinogenicity. Comparative assessment of CarD-T against GPT-4 model reveals a high recall (0.857 vs 0.705) and F1 score (0.875 vs 0.792), and comparable precision (0.894 vs 0.903). Additionally, CarD-T highlights 554 entities that show disputing evidence for carcinogenicity. These are further analyzed using Bayesian temporal Probabilistic Carcinogenic Denomination (PCarD) to provide probabilistic evaluations of their carcinogenic status based on evolving evidence. Our findings underscore that the CarD-T framework is not only robust and effective in identifying and nominating potential carcinogens within vast biomedical literature but also efficient on consumer GPUs. This integration of advanced NLP capabilities with vital epidemiological analysis significantly enhances the agility of public health responses to carcinogen identification, thereby setting a new benchmark for automated, scalable toxicological investigations.","CarD-T: Interpreting Carcinomic Lexicon via Transformers The identification and classification of carcinogens is critical in cancer epidemiology, necessitating updated methodologies to manage the burgeoning biomedical literature. Current systems, like those run by the International Agency for Research on Cancer (IARC) and the National Toxicology Program (NTP), face challenges due to manual vetting and disparities in carcinogen classification spurred by the volume of emerging data. To address these issues, we introduced the Carcinogen Detection via Transformers (CarD-T) framework, a text analytics approach that combines transformer-based machine learning with probabilistic statistical analysis to efficiently nominate carcinogens from scientific texts. CarD-T uses Named Entity Recognition (NER) trained on PubMed abstracts featuring known carcinogens from IARC groups and includes a context classifier to enhance accuracy and manage computational demands. Using this method, journal publication data indexed with carcinogenicity & carcinogenesis Medical Subject Headings (MeSH) terms from the last 25 years was analyzed, identifying potential carcinogens. Training CarD-T on 60% of established carcinogens (Group 1 and 2A carcinogens, IARC designation), CarD-T correctly to identifies all of the remaining Group 1 and 2A designated carcinogens from the analyzed text. In addition, CarD-T nominates roughly 1500 more entities as potential carcinogens that have at least two publications citing evidence of carcinogenicity. Comparative assessment of CarD-T against GPT-4 model reveals a high recall (0.857 vs 0.705) and F1 score (0.875 vs 0.792), and comparable precision (0.894 vs 0.903). Additionally, CarD-T highlights 554 entities that show disputing evidence for carcinogenicity. These are further analyzed using Bayesian temporal Probabilistic Carcinogenic Denomination (PCarD) to provide probabilistic evaluations of their carcinogenic status based on evolving evidence. Our findings underscore that the CarD-T framework is not only robust and effective in identifying and nominating potential carcinogens within vast biomedical literature but also efficient on consumer GPUs. This integration of advanced NLP capabilities with vital epidemiological analysis significantly enhances the agility of public health responses to carcinogen identification, thereby setting a new benchmark for automated, scalable toxicological investigations.","[-0.38518038  0.6447094  -0.17029312 ...  0.54679185 -0.03705858
 -0.11543154]",0.9134979,0.80404806,Text Mining,"NLP, text analytics, transformer"
39175355,Development of an Internet-based Product-related Child Injury Textual Data Platform (IPCITDP) in China,"Xiao W, Cheng P, Schwebel DC, Yang L, Zhao M, Zhao S, Hu G.",J Glob Health. 2024 Aug 23;14:04174. doi: 10.7189/jogh.14.04174.,Xiao W,J Glob Health,2024,2024/08/23,PMC11342019,,10.7189/jogh.14.04174,"BACKGROUND: Internet-based media stories provide valuable information for emerging risks of product-related child injury prevention and control, but critical methodological challenges and high costs of data acquisition and processing restrict practical use by stakeholders.
METHODS: We constructed a data platform through literature reviews and multi-round research group discussions. Developed components included standard search strategies, filtering criteria, textual document classification, information extraction standards and a keyword dictionary. We used ten thousand manually labelled media stories to validate the textual document classification model, which was established using the Bidirectional Encoder Representation from Transformers (BERT). Multiple information extraction methods based on natural language processing algorithms were adopted to extract data for 29 structured variables from media stories. They were evaluated through manual validation of 1000 media stories about product-related child injury. We mapped the geographic distribution of media sources and media-reported product-related child injury events.
RESULTS: We developed an internet-based product-related child injury textual data platform, IPCITDP, consisting of four layers - automatic data search and acquisition, data processing, data storage, and data application - concerning product-related child injury online media stories in China. Each layer occurred daily. External validation demonstrated high performance for the BERT classification model we established (accuracy = 0.9703) and the combined information extraction strategies (accuracy >0.70 for 25 variables). As of 31 December 2023, IPCITDP collected 35 275 eligible product-related child injury reports from 13 261 news media websites or social media platform accounts which were geographically located across all 31 mainland Chinese provinces and covered over 97% of prefecture-level cities. The injury cases in IPCITDP were typically reported several months or years earlier than official announcements about the product-related child injury risks. Our data platform added data concerning 15 supplementary variables that the national product-related injury surveillance system lacks. Two examples demonstrate the value of IPCITDP in supplementing existing data and providing early epidemiological detection of emerging signals concerning product-related child injury: magnetic beads and electric self-balancing scooters.
CONCLUSIONS: Our data platform provides injury data that can support early detection of new product-related child injury characteristics and supplement existing data sources to reduce the burden of product-related injury among Chinese children.","Development of an Internet-based Product-related Child Injury Textual Data Platform (IPCITDP) in China BACKGROUND: Internet-based media stories provide valuable information for emerging risks of product-related child injury prevention and control, but critical methodological challenges and high costs of data acquisition and processing restrict practical use by stakeholders.
METHODS: We constructed a data platform through literature reviews and multi-round research group discussions. Developed components included standard search strategies, filtering criteria, textual document classification, information extraction standards and a keyword dictionary. We used ten thousand manually labelled media stories to validate the textual document classification model, which was established using the Bidirectional Encoder Representation from Transformers (BERT). Multiple information extraction methods based on natural language processing algorithms were adopted to extract data for 29 structured variables from media stories. They were evaluated through manual validation of 1000 media stories about product-related child injury. We mapped the geographic distribution of media sources and media-reported product-related child injury events.
RESULTS: We developed an internet-based product-related child injury textual data platform, IPCITDP, consisting of four layers - automatic data search and acquisition, data processing, data storage, and data application - concerning product-related child injury online media stories in China. Each layer occurred daily. External validation demonstrated high performance for the BERT classification model we established (accuracy = 0.9703) and the combined information extraction strategies (accuracy >0.70 for 25 variables). As of 31 December 2023, IPCITDP collected 35 275 eligible product-related child injury reports from 13 261 news media websites or social media platform accounts which were geographically located across all 31 mainland Chinese provinces and covered over 97% of prefecture-level cities. The injury cases in IPCITDP were typically reported several months or years earlier than official announcements about the product-related child injury risks. Our data platform added data concerning 15 supplementary variables that the national product-related injury surveillance system lacks. Two examples demonstrate the value of IPCITDP in supplementing existing data and providing early epidemiological detection of emerging signals concerning product-related child injury: magnetic beads and electric self-balancing scooters.
CONCLUSIONS: Our data platform provides injury data that can support early detection of new product-related child injury characteristics and supplement existing data sources to reduce the burden of product-related injury among Chinese children.","[-0.067552    0.17590012 -0.19303855 ...  0.6049102  -0.01480758
 -0.39307818]",0.90443915,0.81738895,Both,"natural language processing, language processing, transformer"
38562836,"Health activism, vaccine, and mpox discourse: BERTopic based mixed-method analyses of tweets from sexual minority men and gender diverse (SMMGD) individuals in the U.S","Wang Y, O'Connor K, Flores I, Berdahl CT, Urbanowicz RJ, Stevens R, Bauermeister JA, Gonzalez-Hernandez G.",medRxiv [Preprint]. 2024 Mar 19:2024.03.19.24304519. doi: 10.1101/2024.03.19.24304519.,Wang Y,medRxiv,2024,2024/04/02,PMC10984054,,10.1101/2024.03.19.24304519,"OBJECTIVES: To synthesize discussions among sexual minority men and gender diverse (SMMGD) individuals on mpox, given limited representation of SMMGD voices in existing mpox literature.
METHODS: BERTopic (a topic modeling technique) was employed with human validations to analyze mpox-related tweets (n = 8,688; October 2020-September 2022) from 2,326 self-identified SMMGD individuals in the U.S.; followed by content analysis and geographic analysis.
RESULTS: BERTopic identified 11 topics: health activism (29.81%); mpox vaccination (25.81%) and adverse events (0.98%); sarcasm, jokes, emotional expressions (14.04%); COVID-19 and mpox (7.32%); government/public health response (6.12%); mpox symptoms (2.74%); case reports (2.21%); puns on the virus' naming (i.e., monkeypox; 0.86%); media publicity (0.68%); mpox in children (0.67%). Mpox health activism negatively correlated with LGB social climate index at U.S. state level, ρ = -0.322, p = 0.031.
CONCLUSIONS: SMMGD discussions on mpox encompassed utilitarian (e.g., vaccine access, case reports, mpox symptoms) and emotionally-charged themes-advocating against homophobia, misinformation, and stigma. Mpox health activism was more prevalent in states with lower LGB social acceptance.
PUBLIC HEALTH IMPLICATIONS: Findings illuminate SMMGD engagement with mpox discourse, underscoring the need for more inclusive health communication strategies in infectious disease outbreaks to control associated stigma.","Health activism, vaccine, and mpox discourse: BERTopic based mixed-method analyses of tweets from sexual minority men and gender diverse (SMMGD) individuals in the U.S OBJECTIVES: To synthesize discussions among sexual minority men and gender diverse (SMMGD) individuals on mpox, given limited representation of SMMGD voices in existing mpox literature.
METHODS: BERTopic (a topic modeling technique) was employed with human validations to analyze mpox-related tweets (n = 8,688; October 2020-September 2022) from 2,326 self-identified SMMGD individuals in the U.S.; followed by content analysis and geographic analysis.
RESULTS: BERTopic identified 11 topics: health activism (29.81%); mpox vaccination (25.81%) and adverse events (0.98%); sarcasm, jokes, emotional expressions (14.04%); COVID-19 and mpox (7.32%); government/public health response (6.12%); mpox symptoms (2.74%); case reports (2.21%); puns on the virus' naming (i.e., monkeypox; 0.86%); media publicity (0.68%); mpox in children (0.67%). Mpox health activism negatively correlated with LGB social climate index at U.S. state level, ρ = -0.322, p = 0.031.
CONCLUSIONS: SMMGD discussions on mpox encompassed utilitarian (e.g., vaccine access, case reports, mpox symptoms) and emotionally-charged themes-advocating against homophobia, misinformation, and stigma. Mpox health activism was more prevalent in states with lower LGB social acceptance.
PUBLIC HEALTH IMPLICATIONS: Findings illuminate SMMGD engagement with mpox discourse, underscoring the need for more inclusive health communication strategies in infectious disease outbreaks to control associated stigma.","[-0.15364936  0.11085124  0.21157451 ...  0.6255572   0.5015126
  0.04989206]",0.9171163,0.8073989,Other,Not Specified
38109888,Data-driven automated classification algorithms for acute health conditions: applying PheNorm to COVID-19 disease,"Smith JC, Williamson BD, Cronkite DJ, Park D, Whitaker JM, McLemore MF, Osmanski JT, Winter R, Ramaprasan A, Kelley A, Shea M, Wittayanukorn S, Stojanovic D, Zhao Y, Toh S, Johnson KB, Aronoff DM, Carrell DS.",J Am Med Inform Assoc. 2024 Feb 16;31(3):574-582. doi: 10.1093/jamia/ocad241.,Smith JC,J Am Med Inform Assoc,2024,2023/12/18,PMC10873852,,10.1093/jamia/ocad241,"OBJECTIVES: Automated phenotyping algorithms can reduce development time and operator dependence compared to manually developed algorithms. One such approach, PheNorm, has performed well for identifying chronic health conditions, but its performance for acute conditions is largely unknown. Herein, we implement and evaluate PheNorm applied to symptomatic COVID-19 disease to investigate its potential feasibility for rapid phenotyping of acute health conditions.
MATERIALS AND METHODS: PheNorm is a general-purpose automated approach to creating computable phenotype algorithms based on natural language processing, machine learning, and (low cost) silver-standard training labels. We applied PheNorm to cohorts of potential COVID-19 patients from 2 institutions and used gold-standard manual chart review data to investigate the impact on performance of alternative feature engineering options and implementing externally trained models without local retraining.
RESULTS: Models at each institution achieved AUC, sensitivity, and positive predictive value of 0.853, 0.879, 0.851 and 0.804, 0.976, and 0.885, respectively, at quantiles of model-predicted risk that maximize F1. We report performance metrics for all combinations of silver labels, feature engineering options, and models trained internally versus externally.
DISCUSSION: Phenotyping algorithms developed using PheNorm performed well at both institutions. Performance varied with different silver-standard labels and feature engineering options. Models developed locally at one site also worked well when implemented externally at the other site.
CONCLUSION: PheNorm models successfully identified an acute health condition, symptomatic COVID-19. The simplicity of the PheNorm approach allows it to be applied at multiple study sites with substantially reduced overhead compared to traditional approaches.","Data-driven automated classification algorithms for acute health conditions: applying PheNorm to COVID-19 disease OBJECTIVES: Automated phenotyping algorithms can reduce development time and operator dependence compared to manually developed algorithms. One such approach, PheNorm, has performed well for identifying chronic health conditions, but its performance for acute conditions is largely unknown. Herein, we implement and evaluate PheNorm applied to symptomatic COVID-19 disease to investigate its potential feasibility for rapid phenotyping of acute health conditions.
MATERIALS AND METHODS: PheNorm is a general-purpose automated approach to creating computable phenotype algorithms based on natural language processing, machine learning, and (low cost) silver-standard training labels. We applied PheNorm to cohorts of potential COVID-19 patients from 2 institutions and used gold-standard manual chart review data to investigate the impact on performance of alternative feature engineering options and implementing externally trained models without local retraining.
RESULTS: Models at each institution achieved AUC, sensitivity, and positive predictive value of 0.853, 0.879, 0.851 and 0.804, 0.976, and 0.885, respectively, at quantiles of model-predicted risk that maximize F1. We report performance metrics for all combinations of silver labels, feature engineering options, and models trained internally versus externally.
DISCUSSION: Phenotyping algorithms developed using PheNorm performed well at both institutions. Performance varied with different silver-standard labels and feature engineering options. Models developed locally at one site also worked well when implemented externally at the other site.
CONCLUSION: PheNorm models successfully identified an acute health condition, symptomatic COVID-19. The simplicity of the PheNorm approach allows it to be applied at multiple study sites with substantially reduced overhead compared to traditional approaches.","[ 0.06496048  0.9320771   0.14802003 ...  0.5751634   0.08368324
 -0.6348437 ]",0.9151709,0.806821,Computer Vision,"natural language processing, language processing"
37972522,"Development of a natural language processing model for deriving breast cancer quality indicators : A cross-sectional, multicenter study","Guével E, Priou S, Flicoteaux R, Lamé G, Bey R, Tannier X, Cohen A, Chatellier G, Daniel C, Tournigand C, Kempf E; AP-HP Cancer Group, a CRAB; initiative.",Rev Epidemiol Sante Publique. 2023 Dec;71(6):102189. doi: 10.1016/j.respe.2023.102189. Epub 2023 Nov 15.,Guével E,Rev Epidemiol Sante Publique,2023,2023/11/16,,,10.1016/j.respe.2023.102189,"OBJECTIVES: Medico-administrative data are promising to automate the calculation of Healthcare Quality and Safety Indicators. Nevertheless, not all relevant indicators can be calculated with this data alone. Our feasibility study objective is to analyze 1) the availability of data sources; 2) the availability of each indicator elementary variables, and 3) to apply natural language processing to automatically retrieve such information.
METHOD: We performed a multicenter cross-sectional observational feasibility study on the clinical data warehouse of Assistance Publique - Hôpitaux de Paris (AP-HP). We studied the management of breast cancer patients treated at AP-HP between January 2019 and June 2021, and the quality indicators published by the European Society of Breast Cancer Specialist, using claims data from the Programme de Médicalisation du Système d'Information (PMSI) and pathology reports. For each indicator, we calculated the number (%) of patients for whom all necessary data sources were available, and the number (%) of patients for whom all elementary variables were available in the sources, and for whom the related HQSI was computable. To extract useful data from the free text reports, we developed and validated dedicated rule-based algorithms, whose performance metrics were assessed with recall, precision, and f1-score.
RESULTS: Out of 5785 female patients diagnosed with a breast cancer (60.9 years, IQR [50.0-71.9]), 5,147 (89.0%) had procedures related to breast cancer recorded in the PMSI, and 3732 (72.5%) had at least one surgery. Out of the 34 key indicators, 9 could be calculated with the PMSI alone, and 6 others became so using the data from pathology reports. Ten elementary variables were needed to calculate the 6 indicators combining the PMSI and pathology reports. The necessary sources were available for 58.8% to 94.6% of patients, depending on the indicators. The extraction algorithms developed had an average accuracy of 76.5% (min-max [32.7%-93.3%]), an average precision of 77.7% [10.0%-97.4%] and an average sensitivity of 71.6% [2.8% to 100.0%]. Once these algorithms applied, the variables needed to calculate the indicators were extracted for 2% to 88% of patients, depending on the indicators.
DISCUSSION: The availability of medical reports in the electronic health records, of the elementary variables within the reports, and the performance of the extraction algorithms limit the population for which the indicators can be calculated.
CONCLUSIONS: The automated calculation of quality indicators from electronic health records is a prospect that comes up against many practical obstacles.","Development of a natural language processing model for deriving breast cancer quality indicators : A cross-sectional, multicenter study OBJECTIVES: Medico-administrative data are promising to automate the calculation of Healthcare Quality and Safety Indicators. Nevertheless, not all relevant indicators can be calculated with this data alone. Our feasibility study objective is to analyze 1) the availability of data sources; 2) the availability of each indicator elementary variables, and 3) to apply natural language processing to automatically retrieve such information.
METHOD: We performed a multicenter cross-sectional observational feasibility study on the clinical data warehouse of Assistance Publique - Hôpitaux de Paris (AP-HP). We studied the management of breast cancer patients treated at AP-HP between January 2019 and June 2021, and the quality indicators published by the European Society of Breast Cancer Specialist, using claims data from the Programme de Médicalisation du Système d'Information (PMSI) and pathology reports. For each indicator, we calculated the number (%) of patients for whom all necessary data sources were available, and the number (%) of patients for whom all elementary variables were available in the sources, and for whom the related HQSI was computable. To extract useful data from the free text reports, we developed and validated dedicated rule-based algorithms, whose performance metrics were assessed with recall, precision, and f1-score.
RESULTS: Out of 5785 female patients diagnosed with a breast cancer (60.9 years, IQR [50.0-71.9]), 5,147 (89.0%) had procedures related to breast cancer recorded in the PMSI, and 3732 (72.5%) had at least one surgery. Out of the 34 key indicators, 9 could be calculated with the PMSI alone, and 6 others became so using the data from pathology reports. Ten elementary variables were needed to calculate the 6 indicators combining the PMSI and pathology reports. The necessary sources were available for 58.8% to 94.6% of patients, depending on the indicators. The extraction algorithms developed had an average accuracy of 76.5% (min-max [32.7%-93.3%]), an average precision of 77.7% [10.0%-97.4%] and an average sensitivity of 71.6% [2.8% to 100.0%]. Once these algorithms applied, the variables needed to calculate the indicators were extracted for 2% to 88% of patients, depending on the indicators.
DISCUSSION: The availability of medical reports in the electronic health records, of the elementary variables within the reports, and the performance of the extraction algorithms limit the population for which the indicators can be calculated.
CONCLUSIONS: The automated calculation of quality indicators from electronic health records is a prospect that comes up against many practical obstacles.","[-0.0944256   0.7332806   0.06487891 ...  0.03489853  0.24182023
 -0.44922554]",0.9014708,0.80095404,Text Mining,"natural language processing, language processing"
37607963,Identifying COVID-19 cases and extracting patient reported symptoms from Reddit using natural language processing,"Guo M, Ma Y, Eworuke E, Khashei M, Song J, Zhao Y, Jin F.",Sci Rep. 2023 Aug 22;13(1):13721. doi: 10.1038/s41598-023-39986-7.,Guo M,Sci Rep,2023,2023/08/22,PMC10444846,,10.1038/s41598-023-39986-7,"We used social media data from ""covid19positive"" subreddit, from 03/2020 to 03/2022 to identify COVID-19 cases and extract their reported symptoms automatically using natural language processing (NLP). We trained a Bidirectional Encoder Representations from Transformers classification model with chunking to identify COVID-19 cases; also, we developed a novel QuadArm model, which incorporates Question-answering, dual-corpus expansion, Adaptive rotation clustering, and mapping, to extract symptoms. Our classification model achieved a 91.2% accuracy for the early period (03/2020-05/2020) and was applied to the Delta (07/2021-09/2021) and Omicron (12/2021-03/2022) periods for case identification. We identified 310, 8794, and 12,094 COVID-positive authors in the three periods, respectively. The top five common symptoms extracted in the early period were coughing (57%), fever (55%), loss of sense of smell (41%), headache (40%), and sore throat (40%). During the Delta period, these symptoms remained as the top five symptoms with percent authors reporting symptoms reduced to half or fewer than the early period. During the Omicron period, loss of sense of smell was reported less while sore throat was reported more. Our study demonstrated that NLP can be used to identify COVID-19 cases accurately and extracted symptoms efficiently.","Identifying COVID-19 cases and extracting patient reported symptoms from Reddit using natural language processing We used social media data from ""covid19positive"" subreddit, from 03/2020 to 03/2022 to identify COVID-19 cases and extract their reported symptoms automatically using natural language processing (NLP). We trained a Bidirectional Encoder Representations from Transformers classification model with chunking to identify COVID-19 cases; also, we developed a novel QuadArm model, which incorporates Question-answering, dual-corpus expansion, Adaptive rotation clustering, and mapping, to extract symptoms. Our classification model achieved a 91.2% accuracy for the early period (03/2020-05/2020) and was applied to the Delta (07/2021-09/2021) and Omicron (12/2021-03/2022) periods for case identification. We identified 310, 8794, and 12,094 COVID-positive authors in the three periods, respectively. The top five common symptoms extracted in the early period were coughing (57%), fever (55%), loss of sense of smell (41%), headache (40%), and sore throat (40%). During the Delta period, these symptoms remained as the top five symptoms with percent authors reporting symptoms reduced to half or fewer than the early period. During the Omicron period, loss of sense of smell was reported less while sore throat was reported more. Our study demonstrated that NLP can be used to identify COVID-19 cases accurately and extracted symptoms efficiently.","[ 0.08168177  0.58655775  0.29485768 ...  0.6988266   0.20077045
 -0.47015497]",0.9169934,0.8254276,Other,"natural language processing, NLP, language processing, transformer"
37575020,Identifying the most important data for research in the field of infectious diseases: thinking on the basis of artificial intelligence,"Téllez Santoyo A, Lopera C, Ladino Vásquez A, Seguí Fernández F, Grafiá Pérez I, Chumbita M, Aiello TF, Monzó P, Peyrony O, Puerta-Alcalde P, Cardozo C, Garcia-Pouton N, Castro P, Fernández Méndez S, Nicolas Arfelis JM, Soriano A, Garcia-Vidal C.",Rev Esp Quimioter. 2023 Dec;36(6):592-596. doi: 10.37201/req/032.2023. Epub 2023 Aug 12.,Téllez Santoyo A,Rev Esp Quimioter,2023,2023/08/14,PMC10710675,,10.37201/req/032.2023,"OBJECTIVE: Clinical data on which artificial intelligence (AI) algorithms are trained and tested provide the basis to improve diagnosis or treatment of infectious diseases (ID). We aimed to identify important data for ID research to prioritise efforts being undertaken in AI programmes.
METHODS: We searched for 1,000 articlesfrom high-impact ID journals on PubMed, selecting 288 of the latest articles from 10 top journals. We classified them into structured or unstructured data. Variables were homogenised and grouped into the following categories: epidemiology, admission, demographics, comorbidities, clinical manifestations, laboratory, microbiology, other diagnoses, treatment, outcomes and other non-categorizable variables.
RESULTS: 4,488 individual variables were collected, from the 288 articles. 3,670 (81.8%) variables were classified as structured data whilst 818 (18.2%) as unstructured data. From the structured data, 2,319 (63.2%) variables were classified as direct-retrievable from electronic health records-whilst 1,351 (36.8%) were indirect. The most frequent unstructured data were related to clinical manifestations and were repeated across articles. Data on demographics, comorbidities and microbiology constituted the most frequent group of variables.
CONCLUSIONS: This article identified that structured variables have comprised the most important data in research to generate knowledge in the field of ID. Extracting these data should be a priority when a medical centre intends to start an AI programme for ID. We also documented that the most important unstructured data in this field are those related to clinical manifestations. Such data could easily undergo some structuring with the use of semi-structured medical records focusing on a few symptoms.","Identifying the most important data for research in the field of infectious diseases: thinking on the basis of artificial intelligence OBJECTIVE: Clinical data on which artificial intelligence (AI) algorithms are trained and tested provide the basis to improve diagnosis or treatment of infectious diseases (ID). We aimed to identify important data for ID research to prioritise efforts being undertaken in AI programmes.
METHODS: We searched for 1,000 articlesfrom high-impact ID journals on PubMed, selecting 288 of the latest articles from 10 top journals. We classified them into structured or unstructured data. Variables were homogenised and grouped into the following categories: epidemiology, admission, demographics, comorbidities, clinical manifestations, laboratory, microbiology, other diagnoses, treatment, outcomes and other non-categorizable variables.
RESULTS: 4,488 individual variables were collected, from the 288 articles. 3,670 (81.8%) variables were classified as structured data whilst 818 (18.2%) as unstructured data. From the structured data, 2,319 (63.2%) variables were classified as direct-retrievable from electronic health records-whilst 1,351 (36.8%) were indirect. The most frequent unstructured data were related to clinical manifestations and were repeated across articles. Data on demographics, comorbidities and microbiology constituted the most frequent group of variables.
CONCLUSIONS: This article identified that structured variables have comprised the most important data in research to generate knowledge in the field of ID. Extracting these data should be a priority when a medical centre intends to start an AI programme for ID. We also documented that the most important unstructured data in this field are those related to clinical manifestations. Such data could easily undergo some structuring with the use of semi-structured medical records focusing on a few symptoms.","[ 0.1549401   0.70600194  0.32612437 ...  0.5641966  -0.08513455
 -0.13271596]",0.91990054,0.80480146,Both,Not Specified
37458501,RDBridge: a knowledge graph of rare diseases based on large-scale text mining,"Xing H, Zhang D, Cai P, Zhang R, Hu QN.",Bioinformatics. 2023 Jul 1;39(7):btad440. doi: 10.1093/bioinformatics/btad440.,Xing H,Bioinformatics,2023,2023/07/17,PMC10368801,,10.1093/bioinformatics/btad440,"MOTIVATION: Despite low prevalence, rare diseases affect 300 million people worldwide. Research on pathogenesis and drug development lags due to limited commercial potential, insufficient epidemiological data, and a dearth of publications. The unique characteristics of rare diseases, including limited annotated data, intricate processes for extracting pertinent entity relationships, and difficulties in standardizing data, represent challenges for text mining.
RESULTS: We developed a rare disease data acquisition framework using text mining and knowledge graphs and constructed the most comprehensive rare disease knowledge graph to date, Rare Disease Bridge (RDBridge). RDBridge offers search functions for genes, potential drugs, pathways, literature, and medical imaging data that will support mechanistic research, drug development, diagnosis, and treatment for rare diseases.
AVAILABILITY AND IMPLEMENTATION: RDBridge is freely available at http://rdb.lifesynther.com/.","RDBridge: a knowledge graph of rare diseases based on large-scale text mining MOTIVATION: Despite low prevalence, rare diseases affect 300 million people worldwide. Research on pathogenesis and drug development lags due to limited commercial potential, insufficient epidemiological data, and a dearth of publications. The unique characteristics of rare diseases, including limited annotated data, intricate processes for extracting pertinent entity relationships, and difficulties in standardizing data, represent challenges for text mining.
RESULTS: We developed a rare disease data acquisition framework using text mining and knowledge graphs and constructed the most comprehensive rare disease knowledge graph to date, Rare Disease Bridge (RDBridge). RDBridge offers search functions for genes, potential drugs, pathways, literature, and medical imaging data that will support mechanistic research, drug development, diagnosis, and treatment for rare diseases.
AVAILABILITY AND IMPLEMENTATION: RDBridge is freely available at http://rdb.lifesynther.com/.","[-0.3089402   0.49418172  0.13319312 ...  0.28302252 -0.5558983
 -0.16269523]",0.90118736,0.8026473,Text Mining,text mining
36787355,Can accurate demographic information about people who use prescription medications nonmedically be derived from Twitter?,"Yang YC, Al-Garadi MA, Love JS, Cooper HLF, Perrone J, Sarker A.",Proc Natl Acad Sci U S A. 2023 Feb 21;120(8):e2207391120. doi: 10.1073/pnas.2207391120. Epub 2023 Feb 14.,Yang YC,Proc Natl Acad Sci U S A,2023,2023/02/14,PMC9974473,,10.1073/pnas.2207391120,"Traditional substance use (SU) surveillance methods, such as surveys, incur substantial lags. Due to the continuously evolving trends in SU, insights obtained via such methods are often outdated. Social media-based sources have been proposed for obtaining timely insights, but methods leveraging such data cannot typically provide fine-grained statistics about subpopulations, unlike traditional approaches. We address this gap by developing methods for automatically characterizing a large Twitter nonmedical prescription medication use (NPMU) cohort (n = 288,562) in terms of age-group, race, and gender. Our natural language processing and machine learning methods for automated cohort characterization achieved 0.88 precision (95% CI:0.84 to 0.92) for age-group, 0.90 (95% CI: 0.85 to 0.95) for race, and 94% accuracy (95% CI: 92 to 97) for gender, when evaluated against manually annotated gold-standard data. We compared automatically derived statistics for NPMU of tranquilizers, stimulants, and opioids from Twitter with statistics reported in the National Survey on Drug Use and Health (NSDUH) and the National Emergency Department Sample (NEDS). Distributions automatically estimated from Twitter were mostly consistent with the NSDUH [Spearman r: race: 0.98 (P &lt; 0.005); age-group: 0.67 (P &lt; 0.005); gender: 0.66 (P = 0.27)] and NEDS, with 34/65 (52.3%) of the Twitter-based estimates lying within 95% CIs of estimates from the traditional sources. Explainable differences (e.g., overrepresentation of younger people) were found for age-group-related statistics. Our study demonstrates that accurate subpopulation-specific estimates about SU, particularly NPMU, may be automatically derived from Twitter to obtain earlier insights about targeted subpopulations compared to traditional surveillance approaches.","Can accurate demographic information about people who use prescription medications nonmedically be derived from Twitter? Traditional substance use (SU) surveillance methods, such as surveys, incur substantial lags. Due to the continuously evolving trends in SU, insights obtained via such methods are often outdated. Social media-based sources have been proposed for obtaining timely insights, but methods leveraging such data cannot typically provide fine-grained statistics about subpopulations, unlike traditional approaches. We address this gap by developing methods for automatically characterizing a large Twitter nonmedical prescription medication use (NPMU) cohort (n = 288,562) in terms of age-group, race, and gender. Our natural language processing and machine learning methods for automated cohort characterization achieved 0.88 precision (95% CI:0.84 to 0.92) for age-group, 0.90 (95% CI: 0.85 to 0.95) for race, and 94% accuracy (95% CI: 92 to 97) for gender, when evaluated against manually annotated gold-standard data. We compared automatically derived statistics for NPMU of tranquilizers, stimulants, and opioids from Twitter with statistics reported in the National Survey on Drug Use and Health (NSDUH) and the National Emergency Department Sample (NEDS). Distributions automatically estimated from Twitter were mostly consistent with the NSDUH [Spearman r: race: 0.98 (P &lt; 0.005); age-group: 0.67 (P &lt; 0.005); gender: 0.66 (P = 0.27)] and NEDS, with 34/65 (52.3%) of the Twitter-based estimates lying within 95% CIs of estimates from the traditional sources. Explainable differences (e.g., overrepresentation of younger people) were found for age-group-related statistics. Our study demonstrates that accurate subpopulation-specific estimates about SU, particularly NPMU, may be automatically derived from Twitter to obtain earlier insights about targeted subpopulations compared to traditional surveillance approaches.","[-0.21514332  0.26897204  0.02647984 ...  0.56769186  0.07215579
 -0.41720456]",0.9040207,0.8219047,Other,"natural language processing, language processing"
36716158,Integrating third-party apps with electronic health records to support COVID-19 response,"Johnson C, Barker W.",Am J Manag Care. 2023 Jan 1;29(1):e8-e12. doi: 10.37765/ajmc.2023.89308.,Johnson C,Am J Manag Care,2023,2023/01/30,,,10.37765/ajmc.2023.89308,"OBJECTIVES: To (1) track the integration of telehealth- and COVID-19-related apps with electronic health records (EHRs) over time, (2) identify the primary functionality of apps designed to support the COVID-19 response, and (3) examine whether apps available prior to the pandemic added new telehealth- or COVID-19-related functionalities during the pandemic.
STUDY DESIGN: Data were collected from public EHR app galleries on a monthly basis from December 31, 2019, through June 1, 2021.
METHODS: Apps were identified as relating to COVID-19 or telehealth using text analysis of the app marketing materials. Descriptive analyses were conducted to characterize telehealth- and COVID-19-related apps discovered through the app galleries, identify their primary functionality, and examine whether any apps added new telehealth- or COVID-19-related functionalities during the pandemic.
RESULTS: The number of COVID-19-related apps increased from 0 in March 2020 to 19 a month later and continued to grow to 62 as of June 2021. The number of telehealth-related apps more than doubled from prepandemic levels (n = 41) to a total of 87 apps by June 2021. These apps were 2 times more likely to contain specialized capabilities used to support COVID-19 response efforts, such as secure messaging, vaccine administration, and laboratory testing, compared with all apps listed in the EHR app galleries.
CONCLUSIONS: These findings demonstrate the potential of integrating third-party apps into EHRs to expand the range of tools that health care providers can use to diagnose, treat, and communicate with patients.","Integrating third-party apps with electronic health records to support COVID-19 response OBJECTIVES: To (1) track the integration of telehealth- and COVID-19-related apps with electronic health records (EHRs) over time, (2) identify the primary functionality of apps designed to support the COVID-19 response, and (3) examine whether apps available prior to the pandemic added new telehealth- or COVID-19-related functionalities during the pandemic.
STUDY DESIGN: Data were collected from public EHR app galleries on a monthly basis from December 31, 2019, through June 1, 2021.
METHODS: Apps were identified as relating to COVID-19 or telehealth using text analysis of the app marketing materials. Descriptive analyses were conducted to characterize telehealth- and COVID-19-related apps discovered through the app galleries, identify their primary functionality, and examine whether any apps added new telehealth- or COVID-19-related functionalities during the pandemic.
RESULTS: The number of COVID-19-related apps increased from 0 in March 2020 to 19 a month later and continued to grow to 62 as of June 2021. The number of telehealth-related apps more than doubled from prepandemic levels (n = 41) to a total of 87 apps by June 2021. These apps were 2 times more likely to contain specialized capabilities used to support COVID-19 response efforts, such as secure messaging, vaccine administration, and laboratory testing, compared with all apps listed in the EHR app galleries.
CONCLUSIONS: These findings demonstrate the potential of integrating third-party apps into EHRs to expand the range of tools that health care providers can use to diagnose, treat, and communicate with patients.","[ 0.13804549  0.5578244   0.24719667 ...  0.40039116  0.24218412
 -0.00860525]",0.9063482,0.8089354,Other,text analysis
36583929,"Characterizing the Prevalence of Obesity Misinformation, Factual Content, Stigma, and Positivity on the Social Media Platform Reddit Between 2011 and 2019: Infodemiology Study","Pollack CC, Emond JA, O'Malley AJ, Byrd A, Green P, Miller KE, Vosoughi S, Gilbert-Diamond D, Onega T.",J Med Internet Res. 2022 Dec 30;24(12):e36729. doi: 10.2196/36729.,Pollack CC,J Med Internet Res,2022,2022/12/30,PMC9840103,,10.2196/36729,"BACKGROUND: Reddit is a popular social media platform that has faced scrutiny for inflammatory language against those with obesity, yet there has been no comprehensive analysis of its obesity-related content.
OBJECTIVE: We aimed to quantify the presence of 4 types of obesity-related content on Reddit (misinformation, facts, stigma, and positivity) and identify psycholinguistic features that may be enriched within each one.
METHODS: All sentences (N=764,179) containing ""obese"" or ""obesity"" from top-level comments (n=689,447) made on non-age-restricted subreddits (ie, smaller communities within Reddit) between 2011 and 2019 that contained one of a series of keywords were evaluated. Four types of common natural language processing features were extracted: bigram term frequency-inverse document frequency, word embeddings derived from Bidirectional Encoder Representations from Transformers, sentiment from the Valence Aware Dictionary for Sentiment Reasoning, and psycholinguistic features from the Linguistic Inquiry and Word Count Program. These features were used to train an Extreme Gradient Boosting machine learning classifier to label each sentence as 1 of the 4 content categories or other. Two-part hurdle models for semicontinuous data (which use logistic regression to assess the odds of a 0 result and linear regression for continuous data) were used to evaluate whether select psycholinguistic features presented differently in misinformation (compared with facts) or stigma (compared with positivity).
RESULTS: After removing ambiguous sentences, 0.47% (3610/764,179) of the sentences were labeled as misinformation, 1.88% (14,366/764,179) were labeled as stigma, 1.94% (14,799/764,179) were labeled as positivity, and 8.93% (68,276/764,179) were labeled as facts. Each category had markers that distinguished it from other categories within the data as well as an external corpus. For example, misinformation had a higher average percent of negations (β=3.71, 95% CI 3.53-3.90; P<.001) but a lower average number of words >6 letters (β=-1.47, 95% CI -1.85 to -1.10; P<.001) relative to facts. Stigma had a higher proportion of swear words (β=1.83, 95% CI 1.62-2.04; P<.001) but a lower proportion of first-person singular pronouns (β=-5.30, 95% CI -5.44 to -5.16; P<.001) relative to positivity.
CONCLUSIONS: There are distinct psycholinguistic properties between types of obesity-related content on Reddit that can be leveraged to rapidly identify deleterious content with minimal human intervention and provide insights into how the Reddit population perceives patients with obesity. Future work should assess whether these properties are shared across languages and other social media platforms.","Characterizing the Prevalence of Obesity Misinformation, Factual Content, Stigma, and Positivity on the Social Media Platform Reddit Between 2011 and 2019: Infodemiology Study BACKGROUND: Reddit is a popular social media platform that has faced scrutiny for inflammatory language against those with obesity, yet there has been no comprehensive analysis of its obesity-related content.
OBJECTIVE: We aimed to quantify the presence of 4 types of obesity-related content on Reddit (misinformation, facts, stigma, and positivity) and identify psycholinguistic features that may be enriched within each one.
METHODS: All sentences (N=764,179) containing ""obese"" or ""obesity"" from top-level comments (n=689,447) made on non-age-restricted subreddits (ie, smaller communities within Reddit) between 2011 and 2019 that contained one of a series of keywords were evaluated. Four types of common natural language processing features were extracted: bigram term frequency-inverse document frequency, word embeddings derived from Bidirectional Encoder Representations from Transformers, sentiment from the Valence Aware Dictionary for Sentiment Reasoning, and psycholinguistic features from the Linguistic Inquiry and Word Count Program. These features were used to train an Extreme Gradient Boosting machine learning classifier to label each sentence as 1 of the 4 content categories or other. Two-part hurdle models for semicontinuous data (which use logistic regression to assess the odds of a 0 result and linear regression for continuous data) were used to evaluate whether select psycholinguistic features presented differently in misinformation (compared with facts) or stigma (compared with positivity).
RESULTS: After removing ambiguous sentences, 0.47% (3610/764,179) of the sentences were labeled as misinformation, 1.88% (14,366/764,179) were labeled as stigma, 1.94% (14,799/764,179) were labeled as positivity, and 8.93% (68,276/764,179) were labeled as facts. Each category had markers that distinguished it from other categories within the data as well as an external corpus. For example, misinformation had a higher average percent of negations (β=3.71, 95% CI 3.53-3.90; P<.001) but a lower average number of words >6 letters (β=-1.47, 95% CI -1.85 to -1.10; P<.001) relative to facts. Stigma had a higher proportion of swear words (β=1.83, 95% CI 1.62-2.04; P<.001) but a lower proportion of first-person singular pronouns (β=-5.30, 95% CI -5.44 to -5.16; P<.001) relative to positivity.
CONCLUSIONS: There are distinct psycholinguistic properties between types of obesity-related content on Reddit that can be leveraged to rapidly identify deleterious content with minimal human intervention and provide insights into how the Reddit population perceives patients with obesity. Future work should assess whether these properties are shared across languages and other social media platforms.","[-0.04426321  0.18669409  0.05852687 ...  0.67499477  0.29040968
 -0.30931962]",0.90194017,0.8092394,Other,"natural language processing, language processing, transformer"
35776071,Continuous development of the semantic search engine preVIEW: from COVID-19 to long COVID,"Langnickel L, Darms J, Heldt K, Ducks D, Fluck J.",Database (Oxford). 2022 Jul 1;2022:baac048. doi: 10.1093/database/baac048.,Langnickel L,Database (Oxford),2022,2022/07/01,PMC9248388,,10.1093/database/baac048,"preVIEW is a freely available semantic search engine for Coronavirus disease (COVID-19)-related preprint publications. Currently, it contains >43 800 documents indexed with >4000 semantic concepts, annotated automatically. During the last 2 years, the dynamic situation of the corona crisis has demanded dynamic development. Whereas new semantic concepts have been added over time-such as the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants of interest-the service has been also extended with several features improving the usability and user friendliness. Most importantly, the user is now able to give feedback on detected semantic concepts, i.e. a user can mark annotations as true positives or false positives. In addition, we expanded our methods to construct search queries. The presented version of preVIEW also includes links to the peer-reviewed journal articles, if available. With the described system, we participated in the BioCreative VII interactive text-mining track and retrieved promising user-in-the-loop feedback. Additionally, as the occurrence of long-term symptoms after an infection with the virus SARS-CoV-2-called long COVID-is getting more and more attention, we have recently developed and incorporated a long COVID classifier based on state-of-the-art methods and manually curated data by experts. The service is freely accessible under https://preview.zbmed.de.","Continuous development of the semantic search engine preVIEW: from COVID-19 to long COVID preVIEW is a freely available semantic search engine for Coronavirus disease (COVID-19)-related preprint publications. Currently, it contains >43 800 documents indexed with >4000 semantic concepts, annotated automatically. During the last 2 years, the dynamic situation of the corona crisis has demanded dynamic development. Whereas new semantic concepts have been added over time-such as the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) variants of interest-the service has been also extended with several features improving the usability and user friendliness. Most importantly, the user is now able to give feedback on detected semantic concepts, i.e. a user can mark annotations as true positives or false positives. In addition, we expanded our methods to construct search queries. The presented version of preVIEW also includes links to the peer-reviewed journal articles, if available. With the described system, we participated in the BioCreative VII interactive text-mining track and retrieved promising user-in-the-loop feedback. Additionally, as the occurrence of long-term symptoms after an infection with the virus SARS-CoV-2-called long COVID-is getting more and more attention, we have recently developed and incorporated a long COVID classifier based on state-of-the-art methods and manually curated data by experts. The service is freely accessible under https://preview.zbmed.de.","[-0.08770257  0.28303024  0.04175769 ...  0.29176962 -0.11792027
 -0.24811403]",0.9060234,0.8403056,Both,Not Specified
35750878,Preparing for the next pandemic via transfer learning from existing diseases with hierarchical multi-modal BERT: a study on COVID-19 outcome prediction,"Agarwal K, Choudhury S, Tipirneni S, Mukherjee P, Ham C, Tamang S, Baker M, Tang S, Kocaman V, Gevaert O, Rallo R, Reddy CK.",Sci Rep. 2022 Jun 24;12(1):10748. doi: 10.1038/s41598-022-13072-w.,Agarwal K,Sci Rep,2022,2022/06/24,PMC9232529,,10.1038/s41598-022-13072-w,"Developing prediction models for emerging infectious diseases from relatively small numbers of cases is a critical need for improving pandemic preparedness. Using COVID-19 as an exemplar, we propose a transfer learning methodology for developing predictive models from multi-modal electronic healthcare records by leveraging information from more prevalent diseases with shared clinical characteristics. Our novel hierarchical, multi-modal model ([Formula: see text]) integrates baseline risk factors from the natural language processing of clinical notes at admission, time-series measurements of biomarkers obtained from laboratory tests, and discrete diagnostic, procedure and drug codes. We demonstrate the alignment of [Formula: see text]'s predictions with well-established clinical knowledge about COVID-19 through univariate and multivariate risk factor driven sub-cohort analysis. [Formula: see text]'s superior performance over state-of-the-art methods shows that leveraging patient data across modalities and transferring prior knowledge from similar disorders is critical for accurate prediction of patient outcomes, and this approach may serve as an important tool in the early response to future pandemics.","Preparing for the next pandemic via transfer learning from existing diseases with hierarchical multi-modal BERT: a study on COVID-19 outcome prediction Developing prediction models for emerging infectious diseases from relatively small numbers of cases is a critical need for improving pandemic preparedness. Using COVID-19 as an exemplar, we propose a transfer learning methodology for developing predictive models from multi-modal electronic healthcare records by leveraging information from more prevalent diseases with shared clinical characteristics. Our novel hierarchical, multi-modal model ([Formula: see text]) integrates baseline risk factors from the natural language processing of clinical notes at admission, time-series measurements of biomarkers obtained from laboratory tests, and discrete diagnostic, procedure and drug codes. We demonstrate the alignment of [Formula: see text]'s predictions with well-established clinical knowledge about COVID-19 through univariate and multivariate risk factor driven sub-cohort analysis. [Formula: see text]'s superior performance over state-of-the-art methods shows that leveraging patient data across modalities and transferring prior knowledge from similar disorders is critical for accurate prediction of patient outcomes, and this approach may serve as an important tool in the early response to future pandemics.","[ 0.17027865  0.7237899   0.49751726 ...  0.7810706  -0.42865872
 -0.62504953]",0.9043361,0.80740607,Both,"natural language processing, language processing"
35671409,COVID-19 Vaccine Fact-Checking Posts on Facebook: Observational Study,"Xue H, Gong X, Stevens H.",J Med Internet Res. 2022 Jun 21;24(6):e38423. doi: 10.2196/38423.,Xue H,J Med Internet Res,2022,2022/06/07,PMC9217154,,10.2196/38423,"BACKGROUND: Effective interventions aimed at correcting COVID-19 vaccine misinformation, known as fact-checking messages, are needed to combat the mounting antivaccine infodemic and alleviate vaccine hesitancy.
OBJECTIVE: This work investigates (1) the changes in the public's attitude toward COVID-19 vaccines over time, (2) the effectiveness of COVID-19 vaccine fact-checking information on social media engagement and attitude change, and (3) the emotional and linguistic features of the COVID-19 vaccine fact-checking information ecosystem.
METHODS: We collected a data set of 12,553 COVID-19 vaccine fact-checking Facebook posts and their associated comments (N=122,362) from January 2020 to March 2022 and conducted a series of natural language processing and statistical analyses to investigate trends in public attitude toward the vaccine in COVID-19 vaccine fact-checking posts and comments, and emotional and linguistic features of the COVID-19 fact-checking information ecosystem.
RESULTS: The percentage of fact-checking posts relative to all COVID-19 vaccine posts peaked in May 2020 and then steadily decreased as the pandemic progressed (r=-0.92, df=21, t=-10.94, 95% CI -0.97 to -0.82, P<.001). The salience of COVID-19 vaccine entities was significantly lower in comments (mean 0.03, SD 0.03, t=39.28, P<.001) than in posts (mean 0.09, SD 0.11). Third-party fact checkers have been playing a more important role in more fact-checking over time (r=0.63, df=25, t=4.06, 95% CI 0.33-0.82, P<.001). COVID-19 vaccine fact-checking posts continued to be more analytical (r=0.81, df=25, t=6.88, 95% CI 0.62-0.91, P<.001) and more confident (r=0.59, df=25, t=3.68, 95% CI 0.27-0.79, P=.001) over time. Although comments did not exhibit a significant increase in confidence over time, tentativeness in comments significantly decreased (r=-0.62, df=25, t=-3.94, 95% CI -0.81 to -0.31, P=.001). In addition, although hospitals receive less engagement than other information sources, the comments expressed more positive attitudinal valence in comments compared to other information sources (b=0.06, 95% CI 0.00-0.12, t=2.03, P=.04).
CONCLUSIONS: The percentage of fact-checking posts relative to all posts about the vaccine steadily decreased after May 2020. As the pandemic progressed, third-party fact checkers played a larger role in posting fact-checking COVID-19 vaccine posts. COVID-19 vaccine fact-checking posts continued to be more analytical and more confident over time, reflecting increased confidence in posts. Similarly, tentativeness in comments decreased; this likewise suggests that public uncertainty diminished over time. COVID-19 fact-checking vaccine posts from hospitals yielded more positive attitudes toward vaccination than other information sources. At the same time, hospitals received less engagement than other information sources. This suggests that hospitals should invest more in generating engaging public health campaigns on social media.","COVID-19 Vaccine Fact-Checking Posts on Facebook: Observational Study BACKGROUND: Effective interventions aimed at correcting COVID-19 vaccine misinformation, known as fact-checking messages, are needed to combat the mounting antivaccine infodemic and alleviate vaccine hesitancy.
OBJECTIVE: This work investigates (1) the changes in the public's attitude toward COVID-19 vaccines over time, (2) the effectiveness of COVID-19 vaccine fact-checking information on social media engagement and attitude change, and (3) the emotional and linguistic features of the COVID-19 vaccine fact-checking information ecosystem.
METHODS: We collected a data set of 12,553 COVID-19 vaccine fact-checking Facebook posts and their associated comments (N=122,362) from January 2020 to March 2022 and conducted a series of natural language processing and statistical analyses to investigate trends in public attitude toward the vaccine in COVID-19 vaccine fact-checking posts and comments, and emotional and linguistic features of the COVID-19 fact-checking information ecosystem.
RESULTS: The percentage of fact-checking posts relative to all COVID-19 vaccine posts peaked in May 2020 and then steadily decreased as the pandemic progressed (r=-0.92, df=21, t=-10.94, 95% CI -0.97 to -0.82, P<.001). The salience of COVID-19 vaccine entities was significantly lower in comments (mean 0.03, SD 0.03, t=39.28, P<.001) than in posts (mean 0.09, SD 0.11). Third-party fact checkers have been playing a more important role in more fact-checking over time (r=0.63, df=25, t=4.06, 95% CI 0.33-0.82, P<.001). COVID-19 vaccine fact-checking posts continued to be more analytical (r=0.81, df=25, t=6.88, 95% CI 0.62-0.91, P<.001) and more confident (r=0.59, df=25, t=3.68, 95% CI 0.27-0.79, P=.001) over time. Although comments did not exhibit a significant increase in confidence over time, tentativeness in comments significantly decreased (r=-0.62, df=25, t=-3.94, 95% CI -0.81 to -0.31, P=.001). In addition, although hospitals receive less engagement than other information sources, the comments expressed more positive attitudinal valence in comments compared to other information sources (b=0.06, 95% CI 0.00-0.12, t=2.03, P=.04).
CONCLUSIONS: The percentage of fact-checking posts relative to all posts about the vaccine steadily decreased after May 2020. As the pandemic progressed, third-party fact checkers played a larger role in posting fact-checking COVID-19 vaccine posts. COVID-19 vaccine fact-checking posts continued to be more analytical and more confident over time, reflecting increased confidence in posts. Similarly, tentativeness in comments decreased; this likewise suggests that public uncertainty diminished over time. COVID-19 fact-checking vaccine posts from hospitals yielded more positive attitudes toward vaccination than other information sources. At the same time, hospitals received less engagement than other information sources. This suggests that hospitals should invest more in generating engaging public health campaigns on social media.","[ 0.03645379 -0.01891197  0.3015798  ...  0.59389824  0.33000183
 -0.04697459]",0.9115548,0.8207857,Other,"natural language processing, language processing"
35667199,ARTCDP: An automated data platform for monitoring emerging patterns concerning road traffic crashes in China,"Cheng P, Xiao W, Ning P, Li L, Rao Z, Yang L, Schwebel DC, Yang Y, Huang Y, Hu G.",Accid Anal Prev. 2022 Sep;174:106727. doi: 10.1016/j.aap.2022.106727. Epub 2022 Jun 3.,Cheng P,Accid Anal Prev,2022,2022/06/06,,,10.1016/j.aap.2022.106727,"Online media reports provide valuable information for road traffic injury prevention, but technical challenges concerning data acquisition and processing limit analysis and interpretation of such data. Integrating injury epidemiology theory and big data technology, we developed a data platform consisting of four layers (data acquisition, data processing, application and data storage) to automatically collect reports from online Chinese media concerning road traffic crashes every 24 h. We built a text classification model using 20,000 manually annotated news stories based on the Bidirectional Encoder Representations from Transformers (BERT) and then used natural language processing algorithms to extract data concerning 27 structured variables from the news sources. The accuracy of the BERT-based text classification model was 0.9271, with information extraction accuracy exceeding 80% for 22 variables. As of November 30, 2021, the data platform collected 244,650 eligible media reports covering all 333 prefecture-level divisions in China. These reports were from 37,073 websites or social media accounts, which were geographically located in all 31 provinces and over 98% of prefecture-level divisions. Data availability varied greatly from 0.9% to 100% across the 27 structured variables. Additionally, the platform identified 645,787 potentially relevant keywords when applying natural language processing techniques to the textual media reports. Platform data were highly correlated with road police data in province-based road traffic crash statistics (crashes, r<sub>s</sub> = 0.799; non-fatal injuries, r<sub>s</sub> = 0.802; deaths, r<sub>s</sub> = 0.775). In particular, the platform offers valuable data (like crashes involving electric vehicles) that are not included in official road traffic crash statistics. The new automated data platform shows great potential for timely detection of emerging characteristics of road traffic crashes. Further research is needed to improve the platform and apply it to real-time monitoring and analysis of road traffic injuries.","ARTCDP: An automated data platform for monitoring emerging patterns concerning road traffic crashes in China Online media reports provide valuable information for road traffic injury prevention, but technical challenges concerning data acquisition and processing limit analysis and interpretation of such data. Integrating injury epidemiology theory and big data technology, we developed a data platform consisting of four layers (data acquisition, data processing, application and data storage) to automatically collect reports from online Chinese media concerning road traffic crashes every 24 h. We built a text classification model using 20,000 manually annotated news stories based on the Bidirectional Encoder Representations from Transformers (BERT) and then used natural language processing algorithms to extract data concerning 27 structured variables from the news sources. The accuracy of the BERT-based text classification model was 0.9271, with information extraction accuracy exceeding 80% for 22 variables. As of November 30, 2021, the data platform collected 244,650 eligible media reports covering all 333 prefecture-level divisions in China. These reports were from 37,073 websites or social media accounts, which were geographically located in all 31 provinces and over 98% of prefecture-level divisions. Data availability varied greatly from 0.9% to 100% across the 27 structured variables. Additionally, the platform identified 645,787 potentially relevant keywords when applying natural language processing techniques to the textual media reports. Platform data were highly correlated with road police data in province-based road traffic crash statistics (crashes, r<sub>s</sub> = 0.799; non-fatal injuries, r<sub>s</sub> = 0.802; deaths, r<sub>s</sub> = 0.775). In particular, the platform offers valuable data (like crashes involving electric vehicles) that are not included in official road traffic crash statistics. The new automated data platform shows great potential for timely detection of emerging characteristics of road traffic crashes. Further research is needed to improve the platform and apply it to real-time monitoring and analysis of road traffic injuries.","[-0.24921998 -0.01509438 -0.1813537  ...  0.5398348  -0.01181215
 -0.36815494]",0.907683,0.8108067,Both,"natural language processing, language processing, transformer"
35614412,Text mining for identifying the nature of online questions about non-suicidal self-injury,"Kim MS, Yu J.",BMC Public Health. 2022 May 25;22(1):1041. doi: 10.1186/s12889-022-13480-7.,Kim MS,BMC Public Health,2022,2022/05/25,PMC9131529,,10.1186/s12889-022-13480-7,"OBJECTIVE: The internet provides convenient access to information about non-suicidal self-injury (NSSI) owing to its accessibility and anonymity. This study aimed to explore the distribution of topics regarding NSSI posted on the internet and yearly trends in the derived topics using text mining.
METHODS: We searched for the keyword ""non-suicidal self-injury"" (Ja-Hae in Korean) in the Naver Q&A using the statistical package R. We analyzed 7893 NSSI-related questions posted between 2009 and 2018. Text mining was performed using latent Dirichlet allocation (LDA) on the dataset to determine associations between phrases and thus identify common themes in posts about NSSI.
RESULTS: In the LDA, we selected the following 10 most common topics: anger, family troubles, collecting information on NSSI, stress, concerns regarding NSSI scarring, ways to help a non-suicidal self-injurious friend, depression, medical advice, ways to perform or stop NSSI, and prejudices and thoughts regarding non-suicidal self-injurious people.
CONCLUSIONS: This study provides valuable information on the nature of NSSI questions posted online. In future research, developing websites that provide NSSI information and support or guidance on effectively communicating with NSSI is necessary.","Text mining for identifying the nature of online questions about non-suicidal self-injury OBJECTIVE: The internet provides convenient access to information about non-suicidal self-injury (NSSI) owing to its accessibility and anonymity. This study aimed to explore the distribution of topics regarding NSSI posted on the internet and yearly trends in the derived topics using text mining.
METHODS: We searched for the keyword ""non-suicidal self-injury"" (Ja-Hae in Korean) in the Naver Q&A using the statistical package R. We analyzed 7893 NSSI-related questions posted between 2009 and 2018. Text mining was performed using latent Dirichlet allocation (LDA) on the dataset to determine associations between phrases and thus identify common themes in posts about NSSI.
RESULTS: In the LDA, we selected the following 10 most common topics: anger, family troubles, collecting information on NSSI, stress, concerns regarding NSSI scarring, ways to help a non-suicidal self-injurious friend, depression, medical advice, ways to perform or stop NSSI, and prejudices and thoughts regarding non-suicidal self-injurious people.
CONCLUSIONS: This study provides valuable information on the nature of NSSI questions posted online. In future research, developing websites that provide NSSI information and support or guidance on effectively communicating with NSSI is necessary.","[-0.22590812 -0.07925989  0.12492383 ...  0.36685953  0.04204123
 -0.3177495 ]",0.906566,0.8011683,Text Mining,text mining
35559806,Identifying research themes and trends in the top 20 cancer journals through textual analysis,"Zengul AG, Zengul FD, Ozaydin B, Oner N, Fiveash JB.",J Cancer Policy. 2021 Dec;30:100313. doi: 10.1016/j.jcpo.2021.100313. Epub 2021 Nov 3.,Zengul AG,J Cancer Policy,2021,2022/05/13,,,10.1016/j.jcpo.2021.100313,"BACKGROUND: The rapid growth in cancer research continues expanding the literature. Text mining approaches help make sense of large bodies of scientific literature and integrate the mounting data into the health care delivery system. Our objective is to generate a comprehensive understanding of the themes and trends in cancer research.
METHODS: We employed a three-step text mining process of corpus generation and term-list creation and analysis, including latent semantic analysis for dimension reduction and factor analysis for topic identification to analyze 93,423 abstracts from the top 20 cancer/oncology journals for the period between 1999 and 2020.
RESULTS: We identified 14 distinct topics in cancer literature. The results revealed the uptrend topics - including cell signaling (T-2), immunotherapy (T-3), clinical trials (T-5), disparities and epidemiology (T-7), cancer practice and policy (T-8), outcome research (T-9), and molecular therapeutics (T-10). - and downtrend topics such as cell death (T-1), early phase clinical trials (T-4), angiogenesis (T-6), cancer screening (T-12), and transplant (T-13). The topics of biomarkers(T-11) and cancer genetics(T-16) remained relatively stable. While the topics of angiogenesis (n = 10,490) and cell death (n = 10,258) included the highest number of abstracts, biomarkers (n = 3203), and cancer genetic (n = 4322) themes included the least number of articles. These findings suggest that despite having the lowest numbers of publications, certain topics such as cancer genetic (T-16) and biomarkers (T-11) have been exhibiting a stable trend and drawing a steady amount of attention from cancer researchers over the past 20 years.
CONCLUSION: Findings of this study contribute explanatory insight about themes and trends in cancer research, which can help researchers and stakeholders to identify areas for future studies.
POLICY SUMMARY STATEMENT: The findings indicate the increasing efforts to improve cancer practice and cancer care through policy efforts. Therefore, policymakers and other stakeholders may use the findings in prioritization and funding of specific topics.","Identifying research themes and trends in the top 20 cancer journals through textual analysis BACKGROUND: The rapid growth in cancer research continues expanding the literature. Text mining approaches help make sense of large bodies of scientific literature and integrate the mounting data into the health care delivery system. Our objective is to generate a comprehensive understanding of the themes and trends in cancer research.
METHODS: We employed a three-step text mining process of corpus generation and term-list creation and analysis, including latent semantic analysis for dimension reduction and factor analysis for topic identification to analyze 93,423 abstracts from the top 20 cancer/oncology journals for the period between 1999 and 2020.
RESULTS: We identified 14 distinct topics in cancer literature. The results revealed the uptrend topics - including cell signaling (T-2), immunotherapy (T-3), clinical trials (T-5), disparities and epidemiology (T-7), cancer practice and policy (T-8), outcome research (T-9), and molecular therapeutics (T-10). - and downtrend topics such as cell death (T-1), early phase clinical trials (T-4), angiogenesis (T-6), cancer screening (T-12), and transplant (T-13). The topics of biomarkers(T-11) and cancer genetics(T-16) remained relatively stable. While the topics of angiogenesis (n = 10,490) and cell death (n = 10,258) included the highest number of abstracts, biomarkers (n = 3203), and cancer genetic (n = 4322) themes included the least number of articles. These findings suggest that despite having the lowest numbers of publications, certain topics such as cancer genetic (T-16) and biomarkers (T-11) have been exhibiting a stable trend and drawing a steady amount of attention from cancer researchers over the past 20 years.
CONCLUSION: Findings of this study contribute explanatory insight about themes and trends in cancer research, which can help researchers and stakeholders to identify areas for future studies.
POLICY SUMMARY STATEMENT: The findings indicate the increasing efforts to improve cancer practice and cancer care through policy efforts. Therefore, policymakers and other stakeholders may use the findings in prioritization and funding of specific topics.",[0.00280868 0.536258   0.09892123 ... 0.5388608  0.25249726 0.02101145],0.9306774,0.81920874,Both,text mining
35142636,Monitoring COVID-19 on Social Media: Development of an End-to-End Natural Language Processing Pipeline Using a Novel Triage and Diagnosis Approach,"Hasan A, Levene M, Weston D, Fromson R, Koslover N, Levene T.",J Med Internet Res. 2022 Feb 28;24(2):e30397. doi: 10.2196/30397.,Hasan A,J Med Internet Res,2022,2022/02/10,PMC8887561,,10.2196/30397,"BACKGROUND: The COVID-19 pandemic has created a pressing need for integrating information from disparate sources in order to assist decision makers. Social media is important in this respect; however, to make sense of the textual information it provides and be able to automate the processing of large amounts of data, natural language processing methods are needed. Social media posts are often noisy, yet they may provide valuable insights regarding the severity and prevalence of the disease in the population. Here, we adopt a triage and diagnosis approach to analyzing social media posts using machine learning techniques for the purpose of disease detection and surveillance. We thus obtain useful prevalence and incidence statistics to identify disease symptoms and their severities, motivated by public health concerns.
OBJECTIVE: This study aims to develop an end-to-end natural language processing pipeline for triage and diagnosis of COVID-19 from patient-authored social media posts in order to provide researchers and public health practitioners with additional information on the symptoms, severity, and prevalence of the disease rather than to provide an actionable decision at the individual level.
METHODS: The text processing pipeline first extracted COVID-19 symptoms and related concepts, such as severity, duration, negations, and body parts, from patients' posts using conditional random fields. An unsupervised rule-based algorithm was then applied to establish relations between concepts in the next step of the pipeline. The extracted concepts and relations were subsequently used to construct 2 different vector representations of each post. These vectors were separately applied to build support vector machine learning models to triage patients into 3 categories and diagnose them for COVID-19.
RESULTS: We reported macro- and microaveraged F<sub>1</sub> scores in the range of 71%-96% and 61%-87%, respectively, for the triage and diagnosis of COVID-19 when the models were trained on human-labeled data. Our experimental results indicated that similar performance can be achieved when the models are trained using predicted labels from concept extraction and rule-based classifiers, thus yielding end-to-end machine learning. In addition, we highlighted important features uncovered by our diagnostic machine learning models and compared them with the most frequent symptoms revealed in another COVID-19 data set. In particular, we found that the most important features are not always the most frequent ones.
CONCLUSIONS: Our preliminary results show that it is possible to automatically triage and diagnose patients for COVID-19 from social media natural language narratives, using a machine learning pipeline in order to provide information on the severity and prevalence of the disease for use within health surveillance systems.","Monitoring COVID-19 on Social Media: Development of an End-to-End Natural Language Processing Pipeline Using a Novel Triage and Diagnosis Approach BACKGROUND: The COVID-19 pandemic has created a pressing need for integrating information from disparate sources in order to assist decision makers. Social media is important in this respect; however, to make sense of the textual information it provides and be able to automate the processing of large amounts of data, natural language processing methods are needed. Social media posts are often noisy, yet they may provide valuable insights regarding the severity and prevalence of the disease in the population. Here, we adopt a triage and diagnosis approach to analyzing social media posts using machine learning techniques for the purpose of disease detection and surveillance. We thus obtain useful prevalence and incidence statistics to identify disease symptoms and their severities, motivated by public health concerns.
OBJECTIVE: This study aims to develop an end-to-end natural language processing pipeline for triage and diagnosis of COVID-19 from patient-authored social media posts in order to provide researchers and public health practitioners with additional information on the symptoms, severity, and prevalence of the disease rather than to provide an actionable decision at the individual level.
METHODS: The text processing pipeline first extracted COVID-19 symptoms and related concepts, such as severity, duration, negations, and body parts, from patients' posts using conditional random fields. An unsupervised rule-based algorithm was then applied to establish relations between concepts in the next step of the pipeline. The extracted concepts and relations were subsequently used to construct 2 different vector representations of each post. These vectors were separately applied to build support vector machine learning models to triage patients into 3 categories and diagnose them for COVID-19.
RESULTS: We reported macro- and microaveraged F<sub>1</sub> scores in the range of 71%-96% and 61%-87%, respectively, for the triage and diagnosis of COVID-19 when the models were trained on human-labeled data. Our experimental results indicated that similar performance can be achieved when the models are trained using predicted labels from concept extraction and rule-based classifiers, thus yielding end-to-end machine learning. In addition, we highlighted important features uncovered by our diagnostic machine learning models and compared them with the most frequent symptoms revealed in another COVID-19 data set. In particular, we found that the most important features are not always the most frequent ones.
CONCLUSIONS: Our preliminary results show that it is possible to automatically triage and diagnose patients for COVID-19 from social media natural language narratives, using a machine learning pipeline in order to provide information on the severity and prevalence of the disease for use within health surveillance systems.","[ 0.30492234  0.4616949   0.16417068 ...  0.7614087  -0.25850725
 -0.6452053 ]",0.90339816,0.80078053,Both,"machine learning model, natural language processing, language processing"
34664389,"COVID19 Disease Map, a computational knowledge repository of virus-host interaction mechanisms","Ostaszewski M, Niarakis A, Mazein A, Kuperstein I, Phair R, Orta-Resendiz A, Singh V, Aghamiri SS, Acencio ML, Glaab E, Ruepp A, Fobo G, Montrone C, Brauner B, Frishman G, Monraz Gómez LC, Somers J, Hoch M, Kumar Gupta S, Scheel J, Borlinghaus H, Czauderna T, Schreiber F, Montagud A, Ponce de Leon M, Funahashi A, Hiki Y, Hiroi N, Yamada TG, Dräger A, Renz A, Naveez M, Bocskei Z, Messina F, Börnigen D, Fergusson L, Conti M, Rameil M, Nakonecnij V, Vanhoefer J, Schmiester L, Wang M, Ackerman EE, Shoemaker JE, Zucker J, Oxford K, Teuton J, Kocakaya E, Summak GY, Hanspers K, Kutmon M, Coort S, Eijssen L, Ehrhart F, Rex DAB, Slenter D, Martens M, Pham N, Haw R, Jassal B, Matthews L, Orlic-Milacic M, Senff Ribeiro A, Rothfels K, Shamovsky V, Stephan R, Sevilla C, Varusai T, Ravel JM, Fraser R, Ortseifen V, Marchesi S, Gawron P, Smula E, Heirendt L, Satagopam V, Wu G, Riutta A, Golebiewski M, Owen S, Goble C, Hu X, Overall RW, Maier D, Bauch A, Gyori BM, Bachman JA, Vega C, Grouès V, Vazquez M, Porras P, Licata L, Iannuccelli M, Sacco F, Nesterova A, Yuryev A, de Waard A, Turei D, Luna A, Babur O, et al.",Mol Syst Biol. 2021 Oct;17(10):e10387. doi: 10.15252/msb.202110387.,Ostaszewski M,Mol Syst Biol,2021,2021/10/19,PMC8524328,,10.15252/msb.202110387,"We need to effectively combine the knowledge from surging literature with complex datasets to propose mechanistic models of SARS-CoV-2 infection, improving data interpretation and predicting key targets of intervention. Here, we describe a large-scale community effort to build an open access, interoperable and computable repository of COVID-19 molecular mechanisms. The COVID-19 Disease Map (C19DMap) is a graphical, interactive representation of disease-relevant molecular mechanisms linking many knowledge sources. Notably, it is a computational resource for graph-based analyses and disease modelling. To this end, we established a framework of tools, platforms and guidelines necessary for a multifaceted community of biocurators, domain experts, bioinformaticians and computational biologists. The diagrams of the C19DMap, curated from the literature, are integrated with relevant interaction and text mining databases. We demonstrate the application of network analysis and modelling approaches by concrete examples to highlight new testable hypotheses. This framework helps to find signatures of SARS-CoV-2 predisposition, treatment response or prioritisation of drug candidates. Such an approach may help deal with new waves of COVID-19 or similar pandemics in the long-term perspective.","COVID19 Disease Map, a computational knowledge repository of virus-host interaction mechanisms We need to effectively combine the knowledge from surging literature with complex datasets to propose mechanistic models of SARS-CoV-2 infection, improving data interpretation and predicting key targets of intervention. Here, we describe a large-scale community effort to build an open access, interoperable and computable repository of COVID-19 molecular mechanisms. The COVID-19 Disease Map (C19DMap) is a graphical, interactive representation of disease-relevant molecular mechanisms linking many knowledge sources. Notably, it is a computational resource for graph-based analyses and disease modelling. To this end, we established a framework of tools, platforms and guidelines necessary for a multifaceted community of biocurators, domain experts, bioinformaticians and computational biologists. The diagrams of the C19DMap, curated from the literature, are integrated with relevant interaction and text mining databases. We demonstrate the application of network analysis and modelling approaches by concrete examples to highlight new testable hypotheses. This framework helps to find signatures of SARS-CoV-2 predisposition, treatment response or prioritisation of drug candidates. Such an approach may help deal with new waves of COVID-19 or similar pandemics in the long-term perspective.","[-0.05421336  1.0457239   0.14507994 ...  0.66247743 -0.6556716
 -0.16335978]",0.91229385,0.81217766,Both,text mining
34613399,Extracting social determinants of health from electronic health records using natural language processing: a systematic review,"Patra BG, Sharma MM, Vekaria V, Adekkanattu P, Patterson OV, Glicksberg B, Lepow LA, Ryu E, Biernacka JM, Furmanchuk A, George TJ, Hogan W, Wu Y, Yang X, Bian J, Weissman M, Wickramaratne P, Mann JJ, Olfson M, Campion TR, Weiner M, Pathak J.",J Am Med Inform Assoc. 2021 Nov 25;28(12):2716-2727. doi: 10.1093/jamia/ocab170.,Patra BG,J Am Med Inform Assoc,2021,2021/10/06,PMC8633615,,10.1093/jamia/ocab170,"OBJECTIVE: Social determinants of health (SDoH) are nonclinical dispositions that impact patient health risks and clinical outcomes. Leveraging SDoH in clinical decision-making can potentially improve diagnosis, treatment planning, and patient outcomes. Despite increased interest in capturing SDoH in electronic health records (EHRs), such information is typically locked in unstructured clinical notes. Natural language processing (NLP) is the key technology to extract SDoH information from clinical text and expand its utility in patient care and research. This article presents a systematic review of the state-of-the-art NLP approaches and tools that focus on identifying and extracting SDoH data from unstructured clinical text in EHRs.
MATERIALS AND METHODS: A broad literature search was conducted in February 2021 using 3 scholarly databases (ACL Anthology, PubMed, and Scopus) following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 6402 publications were initially identified, and after applying the study inclusion criteria, 82 publications were selected for the final review.
RESULTS: Smoking status (n = 27), substance use (n = 21), homelessness (n = 20), and alcohol use (n = 15) are the most frequently studied SDoH categories. Homelessness (n = 7) and other less-studied SDoH (eg, education, financial problems, social isolation and support, family problems) are mostly identified using rule-based approaches. In contrast, machine learning approaches are popular for identifying smoking status (n = 13), substance use (n = 9), and alcohol use (n = 9).
CONCLUSION: NLP offers significant potential to extract SDoH data from narrative clinical notes, which in turn can aid in the development of screening tools, risk prediction models, and clinical decision support systems.","Extracting social determinants of health from electronic health records using natural language processing: a systematic review OBJECTIVE: Social determinants of health (SDoH) are nonclinical dispositions that impact patient health risks and clinical outcomes. Leveraging SDoH in clinical decision-making can potentially improve diagnosis, treatment planning, and patient outcomes. Despite increased interest in capturing SDoH in electronic health records (EHRs), such information is typically locked in unstructured clinical notes. Natural language processing (NLP) is the key technology to extract SDoH information from clinical text and expand its utility in patient care and research. This article presents a systematic review of the state-of-the-art NLP approaches and tools that focus on identifying and extracting SDoH data from unstructured clinical text in EHRs.
MATERIALS AND METHODS: A broad literature search was conducted in February 2021 using 3 scholarly databases (ACL Anthology, PubMed, and Scopus) following Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines. A total of 6402 publications were initially identified, and after applying the study inclusion criteria, 82 publications were selected for the final review.
RESULTS: Smoking status (n = 27), substance use (n = 21), homelessness (n = 20), and alcohol use (n = 15) are the most frequently studied SDoH categories. Homelessness (n = 7) and other less-studied SDoH (eg, education, financial problems, social isolation and support, family problems) are mostly identified using rule-based approaches. In contrast, machine learning approaches are popular for identifying smoking status (n = 13), substance use (n = 9), and alcohol use (n = 9).
CONCLUSION: NLP offers significant potential to extract SDoH data from narrative clinical notes, which in turn can aid in the development of screening tools, risk prediction models, and clinical decision support systems.","[-0.05886379  0.35951692  0.3099     ...  0.5439843  -0.1286154
 -0.3962798 ]",0.9037064,0.8164453,Text Mining,"natural language processing, NLP, language processing"
34465169,Artificial Intelligence in Action: Addressing the COVID-19 Pandemic with Natural Language Processing,"Chen Q, Leaman R, Allot A, Luo L, Wei CH, Yan S, Lu Z.",Annu Rev Biomed Data Sci. 2021 Jul 20;4:313-339. doi: 10.1146/annurev-biodatasci-021821-061045. Epub 2021 May 14.,Chen Q,Annu Rev Biomed Data Sci,2021,2021/09/01,,,10.1146/annurev-biodatasci-021821-061045,"The COVID-19 (coronavirus disease 2019) pandemic has had a significant impact on society, both because of the serious health effects of COVID-19 and because of public health measures implemented to slow its spread. Many of these difficulties are fundamentally information needs; attempts to address these needs have caused an information overload for both researchers and the public. Natural language processing (NLP)-the branch of artificial intelligence that interprets human language-can be applied to address many of the information needs made urgent by the COVID-19 pandemic. This review surveys approximately 150 NLP studies and more than 50 systems and datasets addressing the COVID-19 pandemic. We detail work on four core NLP tasks: information retrieval, named entity recognition, literature-based discovery, and question answering. We also describe work that directly addresses aspects of the pandemic through four additional tasks: topic modeling, sentiment and emotion analysis, caseload forecasting, and misinformation detection. We conclude by discussing observable trends and remaining challenges.","Artificial Intelligence in Action: Addressing the COVID-19 Pandemic with Natural Language Processing The COVID-19 (coronavirus disease 2019) pandemic has had a significant impact on society, both because of the serious health effects of COVID-19 and because of public health measures implemented to slow its spread. Many of these difficulties are fundamentally information needs; attempts to address these needs have caused an information overload for both researchers and the public. Natural language processing (NLP)-the branch of artificial intelligence that interprets human language-can be applied to address many of the information needs made urgent by the COVID-19 pandemic. This review surveys approximately 150 NLP studies and more than 50 systems and datasets addressing the COVID-19 pandemic. We detail work on four core NLP tasks: information retrieval, named entity recognition, literature-based discovery, and question answering. We also describe work that directly addresses aspects of the pandemic through four additional tasks: topic modeling, sentiment and emotion analysis, caseload forecasting, and misinformation detection. We conclude by discussing observable trends and remaining challenges.","[ 0.24399903  0.40694895  0.08034199 ...  0.3342487  -0.32833987
 -0.3434799 ]",0.9091286,0.81803584,Both,"natural language processing, NLP, language processing"
33739287,"Revealing Opinions for COVID-19 Questions Using a Context Retriever, Opinion Aggregator, and Question-Answering Model: Model Development Study","Lu ZH, Wang JX, Li X.",J Med Internet Res. 2021 Mar 19;23(3):e22860. doi: 10.2196/22860.,Lu ZH,J Med Internet Res,2021,2021/03/19,PMC7984426,,10.2196/22860,"BACKGROUND: COVID-19 has challenged global public health because it is highly contagious and can be lethal. Numerous ongoing and recently published studies about the disease have emerged. However, the research regarding COVID-19 is largely ongoing and inconclusive.
OBJECTIVE: A potential way to accelerate COVID-19 research is to use existing information gleaned from research into other viruses that belong to the coronavirus family. Our objective is to develop a natural language processing method for answering factoid questions related to COVID-19 using published articles as knowledge sources.
METHODS: Given a question, first, a BM25-based context retriever model is implemented to select the most relevant passages from previously published articles. Second, for each selected context passage, an answer is obtained using a pretrained bidirectional encoder representations from transformers (BERT) question-answering model. Third, an opinion aggregator, which is a combination of a biterm topic model and k-means clustering, is applied to the task of aggregating all answers into several opinions.
RESULTS: We applied the proposed pipeline to extract answers, opinions, and the most frequent words related to six questions from the COVID-19 Open Research Dataset Challenge. By showing the longitudinal distributions of the opinions, we uncovered the trends of opinions and popular words in the articles published in the five time periods assessed: before 1990, 1990-1999, 2000-2009, 2010-2018, and since 2019. The changes in opinions and popular words agree with several distinct characteristics and challenges of COVID-19, including a higher risk for senior people and people with pre-existing medical conditions; high contagion and rapid transmission; and a more urgent need for screening and testing. The opinions and popular words also provide additional insights for the COVID-19-related questions.
CONCLUSIONS: Compared with other methods of literature retrieval and answer generation, opinion aggregation using our method leads to more interpretable, robust, and comprehensive question-specific literature reviews. The results demonstrate the usefulness of the proposed method in answering COVID-19-related questions with main opinions and capturing the trends of research about COVID-19 and other relevant strains of coronavirus in recent years.","Revealing Opinions for COVID-19 Questions Using a Context Retriever, Opinion Aggregator, and Question-Answering Model: Model Development Study BACKGROUND: COVID-19 has challenged global public health because it is highly contagious and can be lethal. Numerous ongoing and recently published studies about the disease have emerged. However, the research regarding COVID-19 is largely ongoing and inconclusive.
OBJECTIVE: A potential way to accelerate COVID-19 research is to use existing information gleaned from research into other viruses that belong to the coronavirus family. Our objective is to develop a natural language processing method for answering factoid questions related to COVID-19 using published articles as knowledge sources.
METHODS: Given a question, first, a BM25-based context retriever model is implemented to select the most relevant passages from previously published articles. Second, for each selected context passage, an answer is obtained using a pretrained bidirectional encoder representations from transformers (BERT) question-answering model. Third, an opinion aggregator, which is a combination of a biterm topic model and k-means clustering, is applied to the task of aggregating all answers into several opinions.
RESULTS: We applied the proposed pipeline to extract answers, opinions, and the most frequent words related to six questions from the COVID-19 Open Research Dataset Challenge. By showing the longitudinal distributions of the opinions, we uncovered the trends of opinions and popular words in the articles published in the five time periods assessed: before 1990, 1990-1999, 2000-2009, 2010-2018, and since 2019. The changes in opinions and popular words agree with several distinct characteristics and challenges of COVID-19, including a higher risk for senior people and people with pre-existing medical conditions; high contagion and rapid transmission; and a more urgent need for screening and testing. The opinions and popular words also provide additional insights for the COVID-19-related questions.
CONCLUSIONS: Compared with other methods of literature retrieval and answer generation, opinion aggregation using our method leads to more interpretable, robust, and comprehensive question-specific literature reviews. The results demonstrate the usefulness of the proposed method in answering COVID-19-related questions with main opinions and capturing the trends of research about COVID-19 and other relevant strains of coronavirus in recent years.","[ 0.2279684   0.10120169  0.15263642 ...  0.6857311   0.04886466
 -0.5524326 ]",0.90439284,0.80493706,Both,"natural language processing, language processing, transformer"
33544689,Lexicon Development for COVID-19-related Concepts Using Open-source Word Embedding Sources: An Intrinsic and Extrinsic Evaluation,"Parikh S, Davoudi A, Yu S, Giraldo C, Schriver E, Mowery D.",JMIR Med Inform. 2021 Feb 22;9(2):e21679. doi: 10.2196/21679.,Parikh S,JMIR Med Inform,2021,2021/02/05,PMC7901592,,10.2196/21679,"BACKGROUND: Scientists are developing new computational methods and prediction models to better clinically understand COVID-19 prevalence, treatment efficacy, and patient outcomes. These efforts could be improved by leveraging documented COVID-19-related symptoms, findings, and disorders from clinical text sources in an electronic health record. Word embeddings can identify terms related to these clinical concepts from both the biomedical and nonbiomedical domains, and are being shared with the open-source community at large. However, it's unclear how useful openly available word embeddings are for developing lexicons for COVID-19-related concepts.
OBJECTIVE: Given an initial lexicon of COVID-19-related terms, this study aims to characterize the returned terms by similarity across various open-source word embeddings and determine common semantic and syntactic patterns between the COVID-19 queried terms and returned terms specific to the word embedding source.
METHODS: We compared seven openly available word embedding sources. Using a series of COVID-19-related terms for associated symptoms, findings, and disorders, we conducted an interannotator agreement study to determine how accurately the most similar returned terms could be classified according to semantic types by three annotators. We conducted a qualitative study of COVID-19 queried terms and their returned terms to detect informative patterns for constructing lexicons. We demonstrated the utility of applying such learned synonyms to discharge summaries by reporting the proportion of patients identified by concept among three patient cohorts: pneumonia (n=6410), acute respiratory distress syndrome (n=8647), and COVID-19 (n=2397).
RESULTS: We observed high pairwise interannotator agreement (Cohen kappa) for symptoms (0.86-0.99), findings (0.93-0.99), and disorders (0.93-0.99). Word embedding sources generated based on characters tend to return more synonyms (mean count of 7.2 synonyms) compared to token-based embedding sources (mean counts range from 2.0 to 3.4). Word embedding sources queried using a qualifier term (eg, dry cough or muscle pain) more often returned qualifiers of the similar semantic type (eg, ""dry"" returns consistency qualifiers like ""wet"" and ""runny"") compared to a single term (eg, cough or pain) queries. A higher proportion of patients had documented fever (0.61-0.84), cough (0.41-0.55), shortness of breath (0.40-0.59), and hypoxia (0.51-0.56) retrieved than other clinical features. Terms for dry cough returned a higher proportion of patients with COVID-19 (0.07) than the pneumonia (0.05) and acute respiratory distress syndrome (0.03) populations.
CONCLUSIONS: Word embeddings are valuable technology for learning related terms, including synonyms. When leveraging openly available word embedding sources, choices made for the construction of the word embeddings can significantly influence the words learned.","Lexicon Development for COVID-19-related Concepts Using Open-source Word Embedding Sources: An Intrinsic and Extrinsic Evaluation BACKGROUND: Scientists are developing new computational methods and prediction models to better clinically understand COVID-19 prevalence, treatment efficacy, and patient outcomes. These efforts could be improved by leveraging documented COVID-19-related symptoms, findings, and disorders from clinical text sources in an electronic health record. Word embeddings can identify terms related to these clinical concepts from both the biomedical and nonbiomedical domains, and are being shared with the open-source community at large. However, it's unclear how useful openly available word embeddings are for developing lexicons for COVID-19-related concepts.
OBJECTIVE: Given an initial lexicon of COVID-19-related terms, this study aims to characterize the returned terms by similarity across various open-source word embeddings and determine common semantic and syntactic patterns between the COVID-19 queried terms and returned terms specific to the word embedding source.
METHODS: We compared seven openly available word embedding sources. Using a series of COVID-19-related terms for associated symptoms, findings, and disorders, we conducted an interannotator agreement study to determine how accurately the most similar returned terms could be classified according to semantic types by three annotators. We conducted a qualitative study of COVID-19 queried terms and their returned terms to detect informative patterns for constructing lexicons. We demonstrated the utility of applying such learned synonyms to discharge summaries by reporting the proportion of patients identified by concept among three patient cohorts: pneumonia (n=6410), acute respiratory distress syndrome (n=8647), and COVID-19 (n=2397).
RESULTS: We observed high pairwise interannotator agreement (Cohen kappa) for symptoms (0.86-0.99), findings (0.93-0.99), and disorders (0.93-0.99). Word embedding sources generated based on characters tend to return more synonyms (mean count of 7.2 synonyms) compared to token-based embedding sources (mean counts range from 2.0 to 3.4). Word embedding sources queried using a qualifier term (eg, dry cough or muscle pain) more often returned qualifiers of the similar semantic type (eg, ""dry"" returns consistency qualifiers like ""wet"" and ""runny"") compared to a single term (eg, cough or pain) queries. A higher proportion of patients had documented fever (0.61-0.84), cough (0.41-0.55), shortness of breath (0.40-0.59), and hypoxia (0.51-0.56) retrieved than other clinical features. Terms for dry cough returned a higher proportion of patients with COVID-19 (0.07) than the pneumonia (0.05) and acute respiratory distress syndrome (0.03) populations.
CONCLUSIONS: Word embeddings are valuable technology for learning related terms, including synonyms. When leveraging openly available word embedding sources, choices made for the construction of the word embeddings can significantly influence the words learned.","[-0.10409089  0.28705463  0.19039829 ...  0.46165928  0.05232855
 -0.4585464 ]",0.9003979,0.8148334,Text Mining,Not Specified
33419870,Network graph representation of COVID-19 scientific publications to aid knowledge discovery,"Cernile G, Heritage T, Sebire NJ, Gordon B, Schwering T, Kazemlou S, Borecki Y.",BMJ Health Care Inform. 2021 Jan;28(1):e100254. doi: 10.1136/bmjhci-2020-100254.,Cernile G,BMJ Health Care Inform,2021,2021/01/09,PMC7798427,,10.1136/bmjhci-2020-100254,"INTRODUCTION: Numerous scientific journal articles related to COVID-19 have been rapidly published, making navigation and understanding of relationships difficult.
METHODS: A graph network was constructed from the publicly available COVID-19 Open Research Dataset (CORD-19) of COVID-19-related publications using an engine leveraging medical knowledge bases to identify discrete medical concepts and an open-source tool (Gephi) to visualise the network.
RESULTS: The network shows connections between diseases, medications and procedures identified from the title and abstract of 195 958 COVID-19-related publications (CORD-19 Dataset). Connections between terms with few publications, those unconnected to the main network and those irrelevant were not displayed. Nodes were coloured by knowledge base and the size of the node related to the number of publications containing the term. The data set and visualisations were made publicly accessible via a webtool.
CONCLUSION: Knowledge management approaches (text mining and graph networks) can effectively allow rapid navigation and exploration of entity inter-relationships to improve understanding of diseases such as COVID-19.","Network graph representation of COVID-19 scientific publications to aid knowledge discovery INTRODUCTION: Numerous scientific journal articles related to COVID-19 have been rapidly published, making navigation and understanding of relationships difficult.
METHODS: A graph network was constructed from the publicly available COVID-19 Open Research Dataset (CORD-19) of COVID-19-related publications using an engine leveraging medical knowledge bases to identify discrete medical concepts and an open-source tool (Gephi) to visualise the network.
RESULTS: The network shows connections between diseases, medications and procedures identified from the title and abstract of 195 958 COVID-19-related publications (CORD-19 Dataset). Connections between terms with few publications, those unconnected to the main network and those irrelevant were not displayed. Nodes were coloured by knowledge base and the size of the node related to the number of publications containing the term. The data set and visualisations were made publicly accessible via a webtool.
CONCLUSION: Knowledge management approaches (text mining and graph networks) can effectively allow rapid navigation and exploration of entity inter-relationships to improve understanding of diseases such as COVID-19.","[-0.15819538  0.23997544 -0.03501101 ...  0.7311377  -0.21490853
 -0.22516388]",0.90130705,0.8270813,Both,text mining
33348764,Automated Classification of Online Sources for Infectious Disease Occurrences Using Machine-Learning-Based Natural Language Processing Approaches,"Kim M, Chae K, Lee S, Jang HJ, Kim S.",Int J Environ Res Public Health. 2020 Dec 17;17(24):9467. doi: 10.3390/ijerph17249467.,Kim M,Int J Environ Res Public Health,2020,2020/12/22,PMC7766498,,10.3390/ijerph17249467,"Collecting valid information from electronic sources to detect the potential outbreak of infectious disease is time-consuming and labor-intensive. The automated identification of relevant information using machine learning is necessary to respond to a potential disease outbreak. A total of 2864 documents were collected from various websites and subsequently manually categorized and labeled by two reviewers. Accurate labels for the training and test data were provided based on a reviewer consensus. Two machine learning algorithms-ConvNet and bidirectional long short-term memory (BiLSTM)-and two classification methods-DocClass and SenClass-were used for classifying the documents. The precision, recall, F1, accuracy, and area under the curve were measured to evaluate the performance of each model. ConvNet yielded higher average, min, and max accuracies (87.6%, 85.2%, and 91.1%, respectively) than BiLSTM with DocClass, while BiLSTM performed better than ConvNet with SenClass with average, min, and max accuracies of 92.8%, 92.6%, and 93.3%, respectively. The performance of BiLSTM with SenClass yielded an overall accuracy of 92.9% in classifying infectious disease occurrences. Machine learning had a compatible performance with a human expert given a particular text extraction system. This study suggests that analyzing information from the website using machine learning can achieve significant accuracies in the presence of abundant articles/documents.","Automated Classification of Online Sources for Infectious Disease Occurrences Using Machine-Learning-Based Natural Language Processing Approaches Collecting valid information from electronic sources to detect the potential outbreak of infectious disease is time-consuming and labor-intensive. The automated identification of relevant information using machine learning is necessary to respond to a potential disease outbreak. A total of 2864 documents were collected from various websites and subsequently manually categorized and labeled by two reviewers. Accurate labels for the training and test data were provided based on a reviewer consensus. Two machine learning algorithms-ConvNet and bidirectional long short-term memory (BiLSTM)-and two classification methods-DocClass and SenClass-were used for classifying the documents. The precision, recall, F1, accuracy, and area under the curve were measured to evaluate the performance of each model. ConvNet yielded higher average, min, and max accuracies (87.6%, 85.2%, and 91.1%, respectively) than BiLSTM with DocClass, while BiLSTM performed better than ConvNet with SenClass with average, min, and max accuracies of 92.8%, 92.6%, and 93.3%, respectively. The performance of BiLSTM with SenClass yielded an overall accuracy of 92.9% in classifying infectious disease occurrences. Machine learning had a compatible performance with a human expert given a particular text extraction system. This study suggests that analyzing information from the website using machine learning can achieve significant accuracies in the presence of abundant articles/documents.","[-0.12013509  0.33441597  0.16872133 ...  0.50803    -0.04915206
 -0.5802146 ]",0.9073132,0.8253573,Text Mining,"LSTM, natural language processing, language processing"
33320803,Identifying naloxone administrations in electronic health record data using a text-mining tool,"Derington CG, Mueller SR, Glanz JM, Binswanger IA.",Subst Abus. 2021;42(4):806-812. doi: 10.1080/08897077.2020.1856288. Epub 2020 Dec 15.,Derington CG,Subst Abus,2021,2020/12/15,PMC8203755,NIHMS1664560,10.1080/08897077.2020.1856288,"Background: Effective and efficient methods are needed to identify naloxone administrations within electronic health record (EHR) data to conduct overdose surveillance and research. The objective of this study was to develop and validate a text-mining tool to identify naloxone administrations in EHR data. Methods: Clinical notes stored in databases between January 2017 and March 2018 were used to iteratively develop a text-mining tool to identify naloxone administrations. The first iteration of the tool used broad search terms. Then, after reviewing clinical notes of overdose encounters, we developed a list of phrases that described naloxone administrations to inform iteration two. While validating iteration two, additional phrases were found, which were then added to inform the final iteration. The comparator was an administrative code query extracted from the EHR. Medical record review was used to identify true positives. The primary outcome was the positive predictive values (PPV) of the second iteration, final iteration, and administrative code query. Results: Iteration two, the final iteration, and the administrative code had PPVs of 84.3% (95% confidence interval [CI] 78.6-89.0%), 83.8% (95% CI 78.6-88.2%), and 57.1% (95% CI 47.1-66.8%), respectively. Both iterations of the tool had a significantly higher PPV than the administrative code (p &lt; 0.001). Conclusions: A text-mining tool improved the identification of naloxone administrations in EHR data from less than 60% with the administrative code to greater than 80% with both versions of the tool. Text-mining tools can inform the use of more sophisticated informatics methods, which often require significant time, resource, and expertise investment.","Identifying naloxone administrations in electronic health record data using a text-mining tool Background: Effective and efficient methods are needed to identify naloxone administrations within electronic health record (EHR) data to conduct overdose surveillance and research. The objective of this study was to develop and validate a text-mining tool to identify naloxone administrations in EHR data. Methods: Clinical notes stored in databases between January 2017 and March 2018 were used to iteratively develop a text-mining tool to identify naloxone administrations. The first iteration of the tool used broad search terms. Then, after reviewing clinical notes of overdose encounters, we developed a list of phrases that described naloxone administrations to inform iteration two. While validating iteration two, additional phrases were found, which were then added to inform the final iteration. The comparator was an administrative code query extracted from the EHR. Medical record review was used to identify true positives. The primary outcome was the positive predictive values (PPV) of the second iteration, final iteration, and administrative code query. Results: Iteration two, the final iteration, and the administrative code had PPVs of 84.3% (95% confidence interval [CI] 78.6-89.0%), 83.8% (95% CI 78.6-88.2%), and 57.1% (95% CI 47.1-66.8%), respectively. Both iterations of the tool had a significantly higher PPV than the administrative code (p &lt; 0.001). Conclusions: A text-mining tool improved the identification of naloxone administrations in EHR data from less than 60% with the administrative code to greater than 80% with both versions of the tool. Text-mining tools can inform the use of more sophisticated informatics methods, which often require significant time, resource, and expertise investment.","[-0.20675994  0.39130095  0.26029792 ...  0.26939982 -0.10018138
 -0.40486944]",0.9098159,0.80665815,Other,Not Specified
33166390,COVID19 Drug Repository: text-mining the literature in search of putative COVID19 therapeutics,"Tworowski D, Gorohovski A, Mukherjee S, Carmi G, Levy E, Detroja R, Mukherjee SB, Frenkel-Morgenstern M.",Nucleic Acids Res. 2021 Jan 8;49(D1):D1113-D1121. doi: 10.1093/nar/gkaa969.,Tworowski D,Nucleic Acids Res,2021,2020/11/09,PMC7778969,,10.1093/nar/gkaa969,"The recent outbreak of COVID-19 has generated an enormous amount of Big Data. To date, the COVID-19 Open Research Dataset (CORD-19), lists ∼130,000 articles from the WHO COVID-19 database, PubMed Central, medRxiv, and bioRxiv, as collected by Semantic Scholar. According to LitCovid (11 August 2020), ∼40,300 COVID19-related articles are currently listed in PubMed. It has been shown in clinical settings that the analysis of past research results and the mining of available data can provide novel opportunities for the successful application of currently approved therapeutics and their combinations for the treatment of conditions caused by a novel SARS-CoV-2 infection. As such, effective responses to the pandemic require the development of efficient applications, methods and algorithms for data navigation, text-mining, clustering, classification, analysis, and reasoning. Thus, our COVID19 Drug Repository represents a modular platform for drug data navigation and analysis, with an emphasis on COVID-19-related information currently being reported. The COVID19 Drug Repository enables users to focus on different levels of complexity, starting from general information about (FDA-) approved drugs, PubMed references, clinical trials, recipes as well as the descriptions of molecular mechanisms of drugs' action. Our COVID19 drug repository provide a most updated world-wide collection of drugs that has been repurposed for COVID19 treatments around the world.","COVID19 Drug Repository: text-mining the literature in search of putative COVID19 therapeutics The recent outbreak of COVID-19 has generated an enormous amount of Big Data. To date, the COVID-19 Open Research Dataset (CORD-19), lists ∼130,000 articles from the WHO COVID-19 database, PubMed Central, medRxiv, and bioRxiv, as collected by Semantic Scholar. According to LitCovid (11 August 2020), ∼40,300 COVID19-related articles are currently listed in PubMed. It has been shown in clinical settings that the analysis of past research results and the mining of available data can provide novel opportunities for the successful application of currently approved therapeutics and their combinations for the treatment of conditions caused by a novel SARS-CoV-2 infection. As such, effective responses to the pandemic require the development of efficient applications, methods and algorithms for data navigation, text-mining, clustering, classification, analysis, and reasoning. Thus, our COVID19 Drug Repository represents a modular platform for drug data navigation and analysis, with an emphasis on COVID-19-related information currently being reported. The COVID19 Drug Repository enables users to focus on different levels of complexity, starting from general information about (FDA-) approved drugs, PubMed references, clinical trials, recipes as well as the descriptions of molecular mechanisms of drugs' action. Our COVID19 drug repository provide a most updated world-wide collection of drugs that has been repurposed for COVID19 treatments around the world.","[-0.16936046  0.8214723   0.17513587 ...  0.5901923  -0.49582243
 -0.06878632]",0.91998476,0.80169725,Text Mining,Not Specified
32448248,Implementation and comparison of two text mining methods with a standard pharmacovigilance method for signal detection of medication errors,"Eskildsen NK, Eriksson R, Christensen SB, Aghassipour TS, Bygsø MJ, Brunak S, Hansen SL.",BMC Med Inform Decis Mak. 2020 May 24;20(1):94. doi: 10.1186/s12911-020-1097-0.,Eskildsen NK,BMC Med Inform Decis Mak,2020,2020/05/26,PMC7245808,,10.1186/s12911-020-1097-0,"BACKGROUND: Medication errors have been identified as the most common preventable cause of adverse events. The lack of granularity in medication error terminology has led pharmacovigilance experts to rely on information in individual case safety reports' (ICSRs) codes and narratives for signal detection, which is both time consuming and labour intensive. Thus, there is a need for complementary methods for the detection of medication errors from ICSRs. The aim of this study is to evaluate the utility of two natural language processing text mining methods as complementary tools to the traditional approach followed by pharmacovigilance experts for medication error signal detection.
METHODS: The safety surveillance advisor (SSA) method, I2E text mining and University of Copenhagen Center for Protein Research (CPR) text mining, were evaluated for their ability to extract cases containing a type of medication error where patients extracted insulin from a prefilled pen or cartridge by a syringe. A total of 154,209 ICSRs were retrieved from Novo Nordisk's safety database from January 1987 to February 2018. Each method was evaluated by recall (sensitivity) and precision (positive predictive value).
RESULTS: We manually annotated 2533 ICSRs to investigate whether these contained the sought medication error. All these ICSRs were then analysed using the three methods. The recall was 90.4, 88.1 and 78.5% for the CPR text mining, the SSA method and the I2E text mining, respectively. Precision was low for all three methods ranging from 3.4% for the SSA method to 1.9 and 1.6% for the CPR and I2E text mining methods, respectively.
CONCLUSIONS: Text mining methods can, with advantage, be used for the detection of complex signals relying on information found in unstructured text (e.g., ICSR narratives) as standardised and both less labour-intensive and time-consuming methods compared to traditional pharmacovigilance methods. The employment of text mining in pharmacovigilance need not be limited to the surveillance of potential medication errors but can be used for the ongoing regulatory requests, e.g., obligations in risk management plans and may thus be utilised broadly for signal detection and ongoing surveillance activities.","Implementation and comparison of two text mining methods with a standard pharmacovigilance method for signal detection of medication errors BACKGROUND: Medication errors have been identified as the most common preventable cause of adverse events. The lack of granularity in medication error terminology has led pharmacovigilance experts to rely on information in individual case safety reports' (ICSRs) codes and narratives for signal detection, which is both time consuming and labour intensive. Thus, there is a need for complementary methods for the detection of medication errors from ICSRs. The aim of this study is to evaluate the utility of two natural language processing text mining methods as complementary tools to the traditional approach followed by pharmacovigilance experts for medication error signal detection.
METHODS: The safety surveillance advisor (SSA) method, I2E text mining and University of Copenhagen Center for Protein Research (CPR) text mining, were evaluated for their ability to extract cases containing a type of medication error where patients extracted insulin from a prefilled pen or cartridge by a syringe. A total of 154,209 ICSRs were retrieved from Novo Nordisk's safety database from January 1987 to February 2018. Each method was evaluated by recall (sensitivity) and precision (positive predictive value).
RESULTS: We manually annotated 2533 ICSRs to investigate whether these contained the sought medication error. All these ICSRs were then analysed using the three methods. The recall was 90.4, 88.1 and 78.5% for the CPR text mining, the SSA method and the I2E text mining, respectively. Precision was low for all three methods ranging from 3.4% for the SSA method to 1.9 and 1.6% for the CPR and I2E text mining methods, respectively.
CONCLUSIONS: Text mining methods can, with advantage, be used for the detection of complex signals relying on information found in unstructured text (e.g., ICSR narratives) as standardised and both less labour-intensive and time-consuming methods compared to traditional pharmacovigilance methods. The employment of text mining in pharmacovigilance need not be limited to the surveillance of potential medication errors but can be used for the ongoing regulatory requests, e.g., obligations in risk management plans and may thus be utilised broadly for signal detection and ongoing surveillance activities.","[ 0.08504513  0.2253695   0.0416465  ...  0.1689403  -0.38623166
 -0.27496135]",0.9204173,0.8063208,Both,"natural language processing, text mining, language processing"
32369038,Use of Machine Learning Techniques for Case-Detection of Varicella Zoster Using Routinely Collected Textual Ambulatory Records: Pilot Observational Study,"Lanera C, Berchialla P, Baldi I, Lorenzoni G, Tramontan L, Scamarcia A, Cantarutti L, Giaquinto C, Gregori D.",JMIR Med Inform. 2020 May 5;8(5):e14330. doi: 10.2196/14330.,Lanera C,JMIR Med Inform,2020,2020/05/06,PMC7238079,,10.2196/14330,"BACKGROUND: The detection of infectious diseases through the analysis of free text on electronic health reports (EHRs) can provide prompt and accurate background information for the implementation of preventative measures, such as advertising and monitoring the effectiveness of vaccination campaigns.
OBJECTIVE: The purpose of this paper is to compare machine learning techniques in their application to EHR analysis for disease detection.
METHODS: The Pedianet database was used as a data source for a real-world scenario on the identification of cases of varicella. The models' training and test sets were based on two different Italian regions' (Veneto and Sicilia) data sets of 7631 patients and 1,230,355 records, and 2347 patients and 569,926 records, respectively, for whom a gold standard of varicella diagnosis was available. Elastic-net regularized generalized linear model (GLMNet), maximum entropy (MAXENT), and LogitBoost (boosting) algorithms were implemented in a supervised environment and 5-fold cross-validated. The document-term matrix generated by the training set involves a dictionary of 1,871,532 tokens. The analysis was conducted on a subset of 29,096 tokens, corresponding to a matrix with no more than a 99% sparsity ratio.
RESULTS: The highest predictive values were achieved through boosting (positive predicative value [PPV] 63.1, 95% CI 42.7-83.5 and negative predicative value [NPV] 98.8, 95% CI 98.3-99.3). GLMNet delivered superior predictive capability compared to MAXENT (PPV 24.5% and NPV 98.3% vs PPV 11.0% and NPV 98.0%). MAXENT and GLMNet predictions weakly agree with each other (agreement coefficient 1 [AC1]=0.60, 95% CI 0.58-0.62), as well as with LogitBoost (MAXENT: AC1=0.64, 95% CI 0.63-0.66 and GLMNet: AC1=0.53, 95% CI 0.51-0.55).
CONCLUSIONS: Boosting has demonstrated promising performance in large-scale EHR-based infectious disease identification.","Use of Machine Learning Techniques for Case-Detection of Varicella Zoster Using Routinely Collected Textual Ambulatory Records: Pilot Observational Study BACKGROUND: The detection of infectious diseases through the analysis of free text on electronic health reports (EHRs) can provide prompt and accurate background information for the implementation of preventative measures, such as advertising and monitoring the effectiveness of vaccination campaigns.
OBJECTIVE: The purpose of this paper is to compare machine learning techniques in their application to EHR analysis for disease detection.
METHODS: The Pedianet database was used as a data source for a real-world scenario on the identification of cases of varicella. The models' training and test sets were based on two different Italian regions' (Veneto and Sicilia) data sets of 7631 patients and 1,230,355 records, and 2347 patients and 569,926 records, respectively, for whom a gold standard of varicella diagnosis was available. Elastic-net regularized generalized linear model (GLMNet), maximum entropy (MAXENT), and LogitBoost (boosting) algorithms were implemented in a supervised environment and 5-fold cross-validated. The document-term matrix generated by the training set involves a dictionary of 1,871,532 tokens. The analysis was conducted on a subset of 29,096 tokens, corresponding to a matrix with no more than a 99% sparsity ratio.
RESULTS: The highest predictive values were achieved through boosting (positive predicative value [PPV] 63.1, 95% CI 42.7-83.5 and negative predicative value [NPV] 98.8, 95% CI 98.3-99.3). GLMNet delivered superior predictive capability compared to MAXENT (PPV 24.5% and NPV 98.3% vs PPV 11.0% and NPV 98.0%). MAXENT and GLMNet predictions weakly agree with each other (agreement coefficient 1 [AC1]=0.60, 95% CI 0.58-0.62), as well as with LogitBoost (MAXENT: AC1=0.64, 95% CI 0.63-0.66 and GLMNet: AC1=0.53, 95% CI 0.51-0.55).
CONCLUSIONS: Boosting has demonstrated promising performance in large-scale EHR-based infectious disease identification.","[-0.16664992  0.40990752  0.19225606 ...  0.4903848   0.30473858
 -0.47470802]",0.90201646,0.8009567,Other,Not Specified
31647520,SMARTS: the social media-based addiction recovery and intervention targeting server,"Jha D, Singh R.",Bioinformatics. 2019 Oct 24:btz800. doi: 10.1093/bioinformatics/btz800. Online ahead of print.,Jha D,Bioinformatics,2019,2019/10/25,,,10.1093/bioinformatics/btz800,"MOTIVATION: Substance abuse and addiction is a significant contemporary health crisis. Modeling its epidemiology and designing effective interventions requires real-time data analysis along with the means to contextualize addiction patterns across the individual-to-community scale. In this context, social media platforms have begun to receive significant attention as a novel source of real-time user-reported information. However, the ability of epidemiologists to use such information is significantly stymied by the lack of publicly available algorithms and software for addiction information extraction, analysis and modeling.
RESULTS: SMARTS is a public, open source, web-based application that addresses the aforementioned deficiency. SMARTS is designed to analyze data from two popular social media forums, namely, Reddit and Twitter and can be used to study the effect of various intoxicants including, opioids, weed, kratom, alcohol, and cigarettes. The SMARTS software analyzes social media posts using natural language processing, and machine learning to characterize drug use at both the individual- and population-levels. Included in SMARTS is a predictive modeling functionality that can, with high accuracy, identify individuals open to addiction recovery interventions. SMARTS also supports extraction, analysis and visualization of a number of key informational and demographic characteristics including post topics and sentiment, drug- and recovery-term usage, geolocation, and age. Finally, the distributions of the aforementioned characteristics as derived from a set of 170,097 drug users are provided as part of SMARTS and can be used by researchers as a reference.
AVAILABILITY: The SMARTS web server and source code are available at: http://haddock9.sfsu.edu/.
SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.","SMARTS: the social media-based addiction recovery and intervention targeting server MOTIVATION: Substance abuse and addiction is a significant contemporary health crisis. Modeling its epidemiology and designing effective interventions requires real-time data analysis along with the means to contextualize addiction patterns across the individual-to-community scale. In this context, social media platforms have begun to receive significant attention as a novel source of real-time user-reported information. However, the ability of epidemiologists to use such information is significantly stymied by the lack of publicly available algorithms and software for addiction information extraction, analysis and modeling.
RESULTS: SMARTS is a public, open source, web-based application that addresses the aforementioned deficiency. SMARTS is designed to analyze data from two popular social media forums, namely, Reddit and Twitter and can be used to study the effect of various intoxicants including, opioids, weed, kratom, alcohol, and cigarettes. The SMARTS software analyzes social media posts using natural language processing, and machine learning to characterize drug use at both the individual- and population-levels. Included in SMARTS is a predictive modeling functionality that can, with high accuracy, identify individuals open to addiction recovery interventions. SMARTS also supports extraction, analysis and visualization of a number of key informational and demographic characteristics including post topics and sentiment, drug- and recovery-term usage, geolocation, and age. Finally, the distributions of the aforementioned characteristics as derived from a set of 170,097 drug users are provided as part of SMARTS and can be used by researchers as a reference.
AVAILABILITY: The SMARTS web server and source code are available at: http://haddock9.sfsu.edu/.
SUPPLEMENTARY INFORMATION: Supplementary data are available at Bioinformatics online.","[ 0.15991071  0.3445462  -0.10571897 ...  1.1084859  -0.27792624
 -0.52277476]",0.911231,0.83096075,Both,"natural language processing, language processing"
31618226,Enhancing timeliness of drug overdose mortality surveillance: A machine learning approach,"Ward PJ, Rock PJ, Slavova S, Young AM, Bunn TL, Kavuluru R.",PLoS One. 2019 Oct 16;14(10):e0223318. doi: 10.1371/journal.pone.0223318. eCollection 2019.,Ward PJ,PLoS One,2019,2019/10/17,PMC6795484,,10.1371/journal.pone.0223318,"BACKGROUND: Timely data is key to effective public health responses to epidemics. Drug overdose deaths are identified in surveillance systems through ICD-10 codes present on death certificates. ICD-10 coding takes time, but free-text information is available on death certificates prior to ICD-10 coding. The objective of this study was to develop a machine learning method to classify free-text death certificates as drug overdoses to provide faster drug overdose mortality surveillance.
METHODS: Using 2017-2018 Kentucky death certificate data, free-text fields were tokenized and features were created from these tokens using natural language processing (NLP). Word, bigram, and trigram features were created as well as features indicating the part-of-speech of each word. These features were then used to train machine learning classifiers on 2017 data. The resulting models were tested on 2018 Kentucky data and compared to a simple rule-based classification approach. Documented code for this method is available for reuse and extensions: https://github.com/pjward5656/dcnlp.
RESULTS: The top scoring machine learning model achieved 0.96 positive predictive value (PPV) and 0.98 sensitivity for an F-score of 0.97 in identification of fatal drug overdoses on test data. This machine learning model achieved significantly higher performance for sensitivity (p<0.001) than the rule-based approach. Additional feature engineering may improve the model's prediction. This model can be deployed on death certificates as soon as the free-text is available, eliminating the time needed to code the death certificates.
CONCLUSION: Machine learning using natural language processing is a relatively new approach in the context of surveillance of health conditions. This method presents an accessible application of machine learning that improves the timeliness of drug overdose mortality surveillance. As such, it can be employed to inform public health responses to the drug overdose epidemic in near-real time as opposed to several weeks following events.","Enhancing timeliness of drug overdose mortality surveillance: A machine learning approach BACKGROUND: Timely data is key to effective public health responses to epidemics. Drug overdose deaths are identified in surveillance systems through ICD-10 codes present on death certificates. ICD-10 coding takes time, but free-text information is available on death certificates prior to ICD-10 coding. The objective of this study was to develop a machine learning method to classify free-text death certificates as drug overdoses to provide faster drug overdose mortality surveillance.
METHODS: Using 2017-2018 Kentucky death certificate data, free-text fields were tokenized and features were created from these tokens using natural language processing (NLP). Word, bigram, and trigram features were created as well as features indicating the part-of-speech of each word. These features were then used to train machine learning classifiers on 2017 data. The resulting models were tested on 2018 Kentucky data and compared to a simple rule-based classification approach. Documented code for this method is available for reuse and extensions: https://github.com/pjward5656/dcnlp.
RESULTS: The top scoring machine learning model achieved 0.96 positive predictive value (PPV) and 0.98 sensitivity for an F-score of 0.97 in identification of fatal drug overdoses on test data. This machine learning model achieved significantly higher performance for sensitivity (p<0.001) than the rule-based approach. Additional feature engineering may improve the model's prediction. This model can be deployed on death certificates as soon as the free-text is available, eliminating the time needed to code the death certificates.
CONCLUSION: Machine learning using natural language processing is a relatively new approach in the context of surveillance of health conditions. This method presents an accessible application of machine learning that improves the timeliness of drug overdose mortality surveillance. As such, it can be employed to inform public health responses to the drug overdose epidemic in near-real time as opposed to several weeks following events.","[-0.06491399  0.42000487  0.14274076 ...  0.38016284 -0.09419174
 -0.45859492]",0.9095619,0.800741,Text Mining,"machine learning model, natural language processing, NLP, language processing"
31020755,Identifying and classifying opioid-related overdoses: A validation study,"Green CA, Perrin NA, Hazlehurst B, Janoff SL, DeVeaugh-Geiss A, Carrell DS, Grijalva CG, Liang C, Enger CL, Coplan PM.",Pharmacoepidemiol Drug Saf. 2019 Aug;28(8):1127-1137. doi: 10.1002/pds.4772. Epub 2019 Apr 24.,Green CA,Pharmacoepidemiol Drug Saf,2019,2019/04/26,PMC6767606,,10.1002/pds.4772,"PURPOSE: The study aims to develop and validate algorithms to identify and classify opioid overdoses using claims and other coded data, and clinical text extracted from electronic health records using natural language processing (NLP).
METHODS: Primary data were derived from Kaiser Permanente Northwest (2008-2014), an integrated health care system (~n > 475 000 unique individuals per year). Data included International Classification of Diseases, Ninth Revision (ICD-9) codes for nonfatal diagnoses, International Classification of Diseases, Tenth Revision (ICD-10) codes for fatal events, clinical notes, and prescription medication records. We assessed sensitivity, specificity, positive predictive value, and negative predictive value for algorithms relative to medical chart review and conducted assessments of algorithm portability in Kaiser Permanente Washington, Tennessee State Medicaid, and Optum.
RESULTS: Code-based algorithm performance was excellent for opioid-related overdoses (sensitivity = 97.2%, specificity = 84.6%) and classification of heroin-involved overdoses (sensitivity = 91.8%, specificity = 99.0%). Performance was acceptable for code-based suicide/suicide attempt classifications (sensitivity = 70.7%, specificity = 90.5%); sensitivity improved with NLP (sensitivity = 78.7%, specificity = 91.0%). Performance was acceptable for the code-based substance abuse-involved classification (sensitivity = 75.3%, specificity = 79.5%); sensitivity improved with the NLP-enhanced algorithm (sensitivity = 80.5%, specificity = 76.3%). The opioid-related overdose algorithm performed well across portability assessment sites, with sensitivity greater than 96% and specificity greater than 84%. Cross-site sensitivity for heroin-involved overdose was greater than 87%, specificity greater than or equal to 99%.
CONCLUSIONS: Code-based algorithms developed to detect opioid-related overdoses and classify them according to heroin involvement perform well. Algorithms for classifying suicides/attempts and abuse-related opioid overdoses perform adequately for use for research, particularly given the complexity of classifying such overdoses. The NLP-enhanced algorithms for suicides/suicide attempts and abuse-related overdoses perform significantly better than code-based algorithms and are appropriate for use in settings that have data and capacity to use NLP.","Identifying and classifying opioid-related overdoses: A validation study PURPOSE: The study aims to develop and validate algorithms to identify and classify opioid overdoses using claims and other coded data, and clinical text extracted from electronic health records using natural language processing (NLP).
METHODS: Primary data were derived from Kaiser Permanente Northwest (2008-2014), an integrated health care system (~n > 475 000 unique individuals per year). Data included International Classification of Diseases, Ninth Revision (ICD-9) codes for nonfatal diagnoses, International Classification of Diseases, Tenth Revision (ICD-10) codes for fatal events, clinical notes, and prescription medication records. We assessed sensitivity, specificity, positive predictive value, and negative predictive value for algorithms relative to medical chart review and conducted assessments of algorithm portability in Kaiser Permanente Washington, Tennessee State Medicaid, and Optum.
RESULTS: Code-based algorithm performance was excellent for opioid-related overdoses (sensitivity = 97.2%, specificity = 84.6%) and classification of heroin-involved overdoses (sensitivity = 91.8%, specificity = 99.0%). Performance was acceptable for code-based suicide/suicide attempt classifications (sensitivity = 70.7%, specificity = 90.5%); sensitivity improved with NLP (sensitivity = 78.7%, specificity = 91.0%). Performance was acceptable for the code-based substance abuse-involved classification (sensitivity = 75.3%, specificity = 79.5%); sensitivity improved with the NLP-enhanced algorithm (sensitivity = 80.5%, specificity = 76.3%). The opioid-related overdose algorithm performed well across portability assessment sites, with sensitivity greater than 96% and specificity greater than 84%. Cross-site sensitivity for heroin-involved overdose was greater than 87%, specificity greater than or equal to 99%.
CONCLUSIONS: Code-based algorithms developed to detect opioid-related overdoses and classify them according to heroin involvement perform well. Algorithms for classifying suicides/attempts and abuse-related opioid overdoses perform adequately for use for research, particularly given the complexity of classifying such overdoses. The NLP-enhanced algorithms for suicides/suicide attempts and abuse-related overdoses perform significantly better than code-based algorithms and are appropriate for use in settings that have data and capacity to use NLP.","[-0.13994063  0.595825    0.20924857 ...  0.16968623  0.23558955
 -0.3609073 ]",0.9054419,0.8064663,Other,"natural language processing, NLP, language processing"
30875704,Leveraging Electronic Dental Record Data to Classify Patients Based on Their Smoking Intensity,"Patel J, Siddiqui Z, Krishnan A, Thyvalikakath TP.",Methods Inf Med. 2018 Nov;57(5-06):253-260. doi: 10.1055/s-0039-1681088. Epub 2019 Mar 15.,Patel J,Methods Inf Med,2018,2019/03/16,,,10.1055/s-0039-1681088,"BACKGROUND: Smoking is an established risk factor for oral diseases and, therefore, dental clinicians routinely assess and record their patients' detailed smoking status. Researchers have successfully extracted smoking history from electronic health records (EHRs) using text mining methods. However, they could not retrieve patients' smoking intensity due to its limited availability in the EHR. The presence of detailed smoking information in the electronic dental record (EDR) often under a separate section allows retrieving this information with less preprocessing.
OBJECTIVE: To determine patients' detailed smoking status based on smoking intensity from the EDR.
METHODS: First, the authors created a reference standard of 3,296 unique patients' smoking histories from the EDR that classified patients based on their smoking intensity. Next, they trained three machine learning classifiers (support vector machine, random forest, and naïve Bayes) using the training set (2,176) and evaluated performances on test set (1,120) using precision (P), recall (R), and F-measure (F). Finally, they applied the best classifier to classify smoking status from an additional 3,114 patients' smoking histories.
RESULTS: Support vector machine performed best to classify patients into smokers, nonsmokers, and unknowns (P, R, F: 98%); intermittent smoker (P: 95%, R: 98%, F: 96%); past smoker (P, R, F: 89%); light smoker (P, R, F: 87%); smokers with unknown intensity (P: 76%, R: 86%, F: 81%), and intermediate smoker (P: 90%, R: 88%, F: 89%). It performed moderately to differentiate heavy smokers (P: 90%, R: 44%, F: 60%). EDR could be a valuable source for obtaining patients' detailed smoking information.
CONCLUSION: EDR data could serve as a valuable source for obtaining patients' detailed smoking information based on their smoking intensity that may not be readily available in the EHR.","Leveraging Electronic Dental Record Data to Classify Patients Based on Their Smoking Intensity BACKGROUND: Smoking is an established risk factor for oral diseases and, therefore, dental clinicians routinely assess and record their patients' detailed smoking status. Researchers have successfully extracted smoking history from electronic health records (EHRs) using text mining methods. However, they could not retrieve patients' smoking intensity due to its limited availability in the EHR. The presence of detailed smoking information in the electronic dental record (EDR) often under a separate section allows retrieving this information with less preprocessing.
OBJECTIVE: To determine patients' detailed smoking status based on smoking intensity from the EDR.
METHODS: First, the authors created a reference standard of 3,296 unique patients' smoking histories from the EDR that classified patients based on their smoking intensity. Next, they trained three machine learning classifiers (support vector machine, random forest, and naïve Bayes) using the training set (2,176) and evaluated performances on test set (1,120) using precision (P), recall (R), and F-measure (F). Finally, they applied the best classifier to classify smoking status from an additional 3,114 patients' smoking histories.
RESULTS: Support vector machine performed best to classify patients into smokers, nonsmokers, and unknowns (P, R, F: 98%); intermittent smoker (P: 95%, R: 98%, F: 96%); past smoker (P, R, F: 89%); light smoker (P, R, F: 87%); smokers with unknown intensity (P: 76%, R: 86%, F: 81%), and intermediate smoker (P: 90%, R: 88%, F: 89%). It performed moderately to differentiate heavy smokers (P: 90%, R: 44%, F: 60%). EDR could be a valuable source for obtaining patients' detailed smoking information.
CONCLUSION: EDR data could serve as a valuable source for obtaining patients' detailed smoking information based on their smoking intensity that may not be readily available in the EHR.","[-0.13513257  0.54252136  0.01380225 ...  0.5838598   0.15203117
 -0.46623117]",0.91408205,0.80569106,Other,text mining
29754799,Extracting cancer mortality statistics from death certificates: A hybrid machine learning and rule-based approach for common and rare cancers,"Koopman B, Zuccon G, Nguyen A, Bergheim A, Grayson N.",Artif Intell Med. 2018 Jul;89:1-9. doi: 10.1016/j.artmed.2018.04.011. Epub 2018 May 10.,Koopman B,Artif Intell Med,2018,2018/05/15,,,10.1016/j.artmed.2018.04.011,"OBJECTIVE: Death certificates are an invaluable source of cancer mortality statistics. However, this value can only be realised if accurate, quantitative data can be extracted from certificates-an aim hampered by both the volume and variable quality of certificates written in natural language. This paper proposes an automatic classification system for identifying all cancer related causes of death from death certificates.
METHODS: Detailed features, including terms, n-grams and SNOMED CT concepts were extracted from a collection of 447,336 death certificates. The features were used as input to two different classification sub-systems: a machine learning sub-system using Support Vector Machines (SVMs) and a rule-based sub-system. A fusion sub-system then combines the results from SVMs and rules into a single final classification. A held-out test set was used to evaluate the effectiveness of the classifiers according to precision, recall and F-measure.
RESULTS: The system was highly effective at determining the type of cancers for both common cancers (F-measure of 0.85) and rare cancers (F-measure of 0.7). In general, rules performed superior to SVMs; however, the fusion method that combined the two was the most effective.
CONCLUSION: The system proposed in this study provides automatic identification and characterisation of cancers from large collections of free-text death certificates. This allows organisations such as Cancer Registries to monitor and report on cancer mortality in a timely and accurate manner. In addition, the methods and findings are generally applicable beyond cancer classification and to other sources of medical text besides death certificates.","Extracting cancer mortality statistics from death certificates: A hybrid machine learning and rule-based approach for common and rare cancers OBJECTIVE: Death certificates are an invaluable source of cancer mortality statistics. However, this value can only be realised if accurate, quantitative data can be extracted from certificates-an aim hampered by both the volume and variable quality of certificates written in natural language. This paper proposes an automatic classification system for identifying all cancer related causes of death from death certificates.
METHODS: Detailed features, including terms, n-grams and SNOMED CT concepts were extracted from a collection of 447,336 death certificates. The features were used as input to two different classification sub-systems: a machine learning sub-system using Support Vector Machines (SVMs) and a rule-based sub-system. A fusion sub-system then combines the results from SVMs and rules into a single final classification. A held-out test set was used to evaluate the effectiveness of the classifiers according to precision, recall and F-measure.
RESULTS: The system was highly effective at determining the type of cancers for both common cancers (F-measure of 0.85) and rare cancers (F-measure of 0.7). In general, rules performed superior to SVMs; however, the fusion method that combined the two was the most effective.
CONCLUSION: The system proposed in this study provides automatic identification and characterisation of cancers from large collections of free-text death certificates. This allows organisations such as Cancer Registries to monitor and report on cancer mortality in a timely and accurate manner. In addition, the methods and findings are generally applicable beyond cancer classification and to other sources of medical text besides death certificates.","[-0.27554774  0.9300336   0.18252619 ...  0.46890935 -0.01941174
 -0.7403837 ]",0.9055812,0.8013444,Both,Not Specified
29420741,Use of text-mining methods to improve efficiency in the calculation of drug exposure to support pharmacoepidemiology studies,"McTaggart S, Nangle C, Caldwell J, Alvarez-Madrazo S, Colhoun H, Bennie M.",Int J Epidemiol. 2018 Apr 1;47(2):617-624. doi: 10.1093/ije/dyx264.,McTaggart S,Int J Epidemiol,2018,2018/02/09,PMC5913611,,10.1093/ije/dyx264,"BACKGROUND: Efficient generation of structured dose instructions that enable researchers to calculate drug exposure is central to pharmacoepidemiology studies. Our aim was to design and test an algorithm to codify dose instructions, applied to the NHS Scotland Prescribing Information System (PIS) that records about 100 million prescriptions per annum.
METHODS: A natural language processing (NLP) algorithm was developed that enabled free-text dose instructions to be represented by three attributes - quantity, frequency and qualifier - specified by three, three and two variables, respectively. A sample of 15 593 distinct dose instructions was used to test, validate and refine the algorithm. The final algorithm used a zero-assumption approach and was then applied to the full dataset.
RESULTS: The initial algorithm generated structured output for 13 152 (84.34%) of the 15 593 sample dose instructions, and reviewers identified 767 (5.83%) incorrect translations, giving an accuracy of 94.17%. Following subsequent refinement of the algorithm rules, application to the full dataset of 458 227 687 prescriptions (99.67% had dose instructions represented by 4 964 083 distinct instructions) generated a structured output for 92.3% of dose instruction texts. This varied by therapeutic area (from 86.7% for the central nervous system to 96.8% for the cardiovascular system).
CONCLUSIONS: We created an NLP algorithm, operational at scale, to produce structured output that gives data users maximum flexibility to formulate, test and apply their own assumptions according to the medicines under investigation. Text mining approaches can provide a solution to the safe and efficient management and provisioning of large volumes of data generated through our health systems.","Use of text-mining methods to improve efficiency in the calculation of drug exposure to support pharmacoepidemiology studies BACKGROUND: Efficient generation of structured dose instructions that enable researchers to calculate drug exposure is central to pharmacoepidemiology studies. Our aim was to design and test an algorithm to codify dose instructions, applied to the NHS Scotland Prescribing Information System (PIS) that records about 100 million prescriptions per annum.
METHODS: A natural language processing (NLP) algorithm was developed that enabled free-text dose instructions to be represented by three attributes - quantity, frequency and qualifier - specified by three, three and two variables, respectively. A sample of 15 593 distinct dose instructions was used to test, validate and refine the algorithm. The final algorithm used a zero-assumption approach and was then applied to the full dataset.
RESULTS: The initial algorithm generated structured output for 13 152 (84.34%) of the 15 593 sample dose instructions, and reviewers identified 767 (5.83%) incorrect translations, giving an accuracy of 94.17%. Following subsequent refinement of the algorithm rules, application to the full dataset of 458 227 687 prescriptions (99.67% had dose instructions represented by 4 964 083 distinct instructions) generated a structured output for 92.3% of dose instruction texts. This varied by therapeutic area (from 86.7% for the central nervous system to 96.8% for the cardiovascular system).
CONCLUSIONS: We created an NLP algorithm, operational at scale, to produce structured output that gives data users maximum flexibility to formulate, test and apply their own assumptions according to the medicines under investigation. Text mining approaches can provide a solution to the safe and efficient management and provisioning of large volumes of data generated through our health systems.","[-0.25432032  0.25544795  0.03529864 ...  0.24545906 -0.27135018
 -0.516917  ]",0.9046091,0.82039595,Both,"natural language processing, text mining, NLP, language processing"
28578250,Increases in synthetic cannabinoids-related harms: Results from a longitudinal web-based content analysis,"Lamy FR, Daniulaityte R, Nahhas RW, Barratt MJ, Smith AG, Sheth A, Martins SS, Boyer EW, Carlson RG.",Int J Drug Policy. 2017 Jun;44:121-129. doi: 10.1016/j.drugpo.2017.05.007. Epub 2017 Jun 1.,Lamy FR,Int J Drug Policy,2017,2017/06/05,PMC5545681,NIHMS876447,10.1016/j.drugpo.2017.05.007,"BACKGROUND: Synthetic Cannabinoid Receptor Agonists (SCRA), also known as ""K2"" or ""Spice,"" have drawn considerable attention due to their potential of abuse and harmful consequences. More research is needed to understand user experiences of SCRA-related effects. We use semi-automated information processing techniques through eDrugTrends platform to examine SCRA-related effects and their variations through a longitudinal content analysis of web-forum data.
METHOD: English language posts from three drug-focused web-forums were extracted and analyzed between January 1st 2008 and September 30th 2015. Search terms are based on the Drug Use Ontology (DAO) created for this study (189 SCRA-related and 501 effect-related terms). EDrugTrends NLP-based text processing tools were used to extract posts mentioning SCRA and their effects. Generalized linear regression was used to fit restricted cubic spline functions of time to test whether the proportion of drug-related posts that mention SCRA (and no other drug) and the proportion of these ""SCRA-only"" posts that mention SCRA effects have changed over time, with an adjustment for multiple testing.
RESULTS: 19,052 SCRA-related posts (Bluelight (n=2782), Forum A (n=3882), and Forum B (n=12,388)) posted by 2543 international users were extracted. The most frequently mentioned effects were ""getting high"" (44.0%), ""hallucinations"" (10.8%), and ""anxiety"" (10.2%). The frequency of SCRA-only posts declined steadily over the study period. The proportions of SCRA-only posts mentioning positive effects (e.g., ""High"" and ""Euphoria"") steadily decreased, while the proportions of SCRA-only posts mentioning negative effects (e.g., ""Anxiety,"" 'Nausea,"" ""Overdose"") increased over the same period.
CONCLUSION: This study's findings indicate that the proportion of negative effects mentioned in web forum posts and linked to SCRA has increased over time, suggesting that recent generations of SCRA generate more harms. This is also one of the first studies to conduct automated content analysis of web forum data related to illicit drug use.","Increases in synthetic cannabinoids-related harms: Results from a longitudinal web-based content analysis BACKGROUND: Synthetic Cannabinoid Receptor Agonists (SCRA), also known as ""K2"" or ""Spice,"" have drawn considerable attention due to their potential of abuse and harmful consequences. More research is needed to understand user experiences of SCRA-related effects. We use semi-automated information processing techniques through eDrugTrends platform to examine SCRA-related effects and their variations through a longitudinal content analysis of web-forum data.
METHOD: English language posts from three drug-focused web-forums were extracted and analyzed between January 1st 2008 and September 30th 2015. Search terms are based on the Drug Use Ontology (DAO) created for this study (189 SCRA-related and 501 effect-related terms). EDrugTrends NLP-based text processing tools were used to extract posts mentioning SCRA and their effects. Generalized linear regression was used to fit restricted cubic spline functions of time to test whether the proportion of drug-related posts that mention SCRA (and no other drug) and the proportion of these ""SCRA-only"" posts that mention SCRA effects have changed over time, with an adjustment for multiple testing.
RESULTS: 19,052 SCRA-related posts (Bluelight (n=2782), Forum A (n=3882), and Forum B (n=12,388)) posted by 2543 international users were extracted. The most frequently mentioned effects were ""getting high"" (44.0%), ""hallucinations"" (10.8%), and ""anxiety"" (10.2%). The frequency of SCRA-only posts declined steadily over the study period. The proportions of SCRA-only posts mentioning positive effects (e.g., ""High"" and ""Euphoria"") steadily decreased, while the proportions of SCRA-only posts mentioning negative effects (e.g., ""Anxiety,"" 'Nausea,"" ""Overdose"") increased over the same period.
CONCLUSION: This study's findings indicate that the proportion of negative effects mentioned in web forum posts and linked to SCRA has increased over time, suggesting that recent generations of SCRA generate more harms. This is also one of the first studies to conduct automated content analysis of web forum data related to illicit drug use.","[-0.12143207  0.16005495 -0.14336826 ...  0.64358836  0.13753444
 -0.09242628]",0.92548096,0.8026765,Other,NLP
28210429,KIWI: A technology for public health event monitoring and early warning signal detection,Mukhi SN.,Online J Public Health Inform. 2016 Dec 28;8(3):e208. doi: 10.5210/ojphi.v8i3.6937. eCollection 2016.,Mukhi SN,Online J Public Health Inform,2016,2017/02/18,PMC5302468,,10.5210/ojphi.v8i3.6937,"OBJECTIVES: To introduce the Canadian Network for Public Health Intelligence's new Knowledge Integration using Web-based Intelligence (KIWI) technology, and to pefrom preliminary evaluation of the KIWI technology using a case study. The purpose of this new technology is to support surveillance activities by monitoring unstructured data sources for the early detection and awareness of potential public health threats.
METHODS: A prototype of the KIWI technology, adapted for zoonotic and emerging diseases, was piloted by end-users with expertise in the field of public health and zoonotic/emerging disease surveillance. The technology was assessed using variables such as geographic coverage, user participation, and others; categorized by high-level attributes from evaluation guidelines for internet based surveillance systems. Special attention was given to the evaluation of the system's automated sense-making algorithm, which used variables such as sensitivity, specificity, and predictive values. Event-based surveillance evaluation was not applied to its full capacity as such an evaluation is beyond the scope of this paper.
RESULTS: KIWI was piloted with user participation = 85.0% and geographic coverage within monitored sources = 83.9% of countries. The pilots, which focused on zoonotic and emerging diseases, lasted a combined total of 65 days and resulted in the collection of 3243 individual information pieces (IIP) and 2 community reported events (CRE) for processing. Ten sources were monitored during the second phase of the pilot, which resulted in 545 anticipatory intelligence signals (AIS). KIWI's automated sense-making algorithm (SMA) had sensitivity = 63.9% (95% CI: 60.2-67.5%), specificity = 88.6% (95% CI: 87.3-89.8%), positive predictive value = 59.8% (95% CI: 56.1-63.4%), and negative predictive value = 90.3% (95% CI: 89.0-91.4%).
DISCUSSION: Literature suggests the need for internet based monitoring and surveillance systems that are customizable, integrated into collaborative networks of public health professionals, and incorporated into national surveillance activities. Results show that the KIWI technology is well posied to address some of the suggested challenges. A limitation of this study is that sample size for pilot participation was small for capturing overall readiness of integrating KIWI into regular surveillance activities.
CONCLUSIONS: KIWI is a customizable technology developed within an already thriving collaborative platform used by public health professionals, and performs well as a tool for discipline-specific event monitoring and early warning signal detection.","KIWI: A technology for public health event monitoring and early warning signal detection OBJECTIVES: To introduce the Canadian Network for Public Health Intelligence's new Knowledge Integration using Web-based Intelligence (KIWI) technology, and to pefrom preliminary evaluation of the KIWI technology using a case study. The purpose of this new technology is to support surveillance activities by monitoring unstructured data sources for the early detection and awareness of potential public health threats.
METHODS: A prototype of the KIWI technology, adapted for zoonotic and emerging diseases, was piloted by end-users with expertise in the field of public health and zoonotic/emerging disease surveillance. The technology was assessed using variables such as geographic coverage, user participation, and others; categorized by high-level attributes from evaluation guidelines for internet based surveillance systems. Special attention was given to the evaluation of the system's automated sense-making algorithm, which used variables such as sensitivity, specificity, and predictive values. Event-based surveillance evaluation was not applied to its full capacity as such an evaluation is beyond the scope of this paper.
RESULTS: KIWI was piloted with user participation = 85.0% and geographic coverage within monitored sources = 83.9% of countries. The pilots, which focused on zoonotic and emerging diseases, lasted a combined total of 65 days and resulted in the collection of 3243 individual information pieces (IIP) and 2 community reported events (CRE) for processing. Ten sources were monitored during the second phase of the pilot, which resulted in 545 anticipatory intelligence signals (AIS). KIWI's automated sense-making algorithm (SMA) had sensitivity = 63.9% (95% CI: 60.2-67.5%), specificity = 88.6% (95% CI: 87.3-89.8%), positive predictive value = 59.8% (95% CI: 56.1-63.4%), and negative predictive value = 90.3% (95% CI: 89.0-91.4%).
DISCUSSION: Literature suggests the need for internet based monitoring and surveillance systems that are customizable, integrated into collaborative networks of public health professionals, and incorporated into national surveillance activities. Results show that the KIWI technology is well posied to address some of the suggested challenges. A limitation of this study is that sample size for pilot participation was small for capturing overall readiness of integrating KIWI into regular surveillance activities.
CONCLUSIONS: KIWI is a customizable technology developed within an already thriving collaborative platform used by public health professionals, and performs well as a tool for discipline-specific event monitoring and early warning signal detection.","[ 0.31237736  0.5106884  -0.15242133 ...  0.5532381  -0.22084594
 -0.3141545 ]",0.92070055,0.8145418,Both,Not Specified
25991273,A health analytics semantic ETL service for obesity surveillance,"Poulymenopoulou M, Papakonstantinou D, Malamateniou F, Vassilacopoulos G.",Stud Health Technol Inform. 2015;210:840-4.,Poulymenopoulou M,Stud Health Technol Inform,2015,2015/05/21,,,,"The increasingly large amount of data produced in healthcare (e.g. collected through health information systems such as electronic medical records - EMRs or collected through novel data sources such as personal health records - PHRs, social media, web resources) enable the creation of detailed records about people's health, sentiments and activities (e.g. physical activity, diet, sleep quality) that can be used in the public health area among others. However, despite the transformative potential of big data in public health surveillance there are several challenges in integrating big data. In this paper, the interoperability challenge is tackled and a semantic Extract Transform Load (ETL) service is proposed that seeks to semantically annotate big data to result into valuable data for analysis. This service is considered as part of a health analytics engine on the cloud that interacts with existing healthcare information exchange networks, like the Integrating the Healthcare Enterprise (IHE), PHRs, sensors, mobile applications, and other web resources to retrieve patient health, behavioral and daily activity data. The semantic ETL service aims at semantically integrating big data for use by analytic mechanisms. An illustrative implementation of the service on big data which is potentially relevant to human obesity, enables using appropriate analytic techniques (e.g. machine learning, text mining) that are expected to assist in identifying patterns and contributing factors (e.g. genetic background, social, environmental) for this social phenomenon and, hence, drive health policy changes and promote healthy behaviors where residents live, work, learn, shop and play.","A health analytics semantic ETL service for obesity surveillance The increasingly large amount of data produced in healthcare (e.g. collected through health information systems such as electronic medical records - EMRs or collected through novel data sources such as personal health records - PHRs, social media, web resources) enable the creation of detailed records about people's health, sentiments and activities (e.g. physical activity, diet, sleep quality) that can be used in the public health area among others. However, despite the transformative potential of big data in public health surveillance there are several challenges in integrating big data. In this paper, the interoperability challenge is tackled and a semantic Extract Transform Load (ETL) service is proposed that seeks to semantically annotate big data to result into valuable data for analysis. This service is considered as part of a health analytics engine on the cloud that interacts with existing healthcare information exchange networks, like the Integrating the Healthcare Enterprise (IHE), PHRs, sensors, mobile applications, and other web resources to retrieve patient health, behavioral and daily activity data. The semantic ETL service aims at semantically integrating big data for use by analytic mechanisms. An illustrative implementation of the service on big data which is potentially relevant to human obesity, enables using appropriate analytic techniques (e.g. machine learning, text mining) that are expected to assist in identifying patterns and contributing factors (e.g. genetic background, social, environmental) for this social phenomenon and, hence, drive health policy changes and promote healthy behaviors where residents live, work, learn, shop and play.","[ 0.2388246   0.66009754  0.14712235 ...  0.70818204 -0.21182153
 -0.60966927]",0.9015615,0.8086856,Both,text mining
25402257,The use of natural language processing of infusion notes to identify outpatient infusions,"Nelson SD, Lu CC, Teng CC, Leng J, Cannon GW, He T, Zeng Q, Halwani A, Sauer B.",Pharmacoepidemiol Drug Saf. 2015 Jan;24(1):86-92. doi: 10.1002/pds.3720. Epub 2014 Nov 17.,Nelson SD,Pharmacoepidemiol Drug Saf,2015,2014/11/18,,,10.1002/pds.3720,"PURPOSE: Outpatient infusions are commonly missing in Veterans Health Affairs (VHA) pharmacy dispensing data sets. Currently, Healthcare Common Procedure Coding System (HCPCS) codes are used to identify outpatient infusions, but concerns exist if they correctly capture all infusions and infusion-related data such as dose and date of administration. We developed natural language processing (NLP) software to extract infusion information from medical text infusion notes. The objective was to compare the sensitivity of three approaches to identify infliximab administration dates and infusion doses against a reference standard established from the Veterans Affairs rheumatoid arthritis (VARA) registry.
METHODS: We compared the sensitivity and positive predictive value (PPV) of NLP to that of HCPCS codes in identifying the correct date and dose of infliximab infusions against a human extracted reference standard.
RESULTS: The sensitivity was 0.606 (0.585-0.627) for HCPCS alone, 0.858 (0.842-0.873) for NLP alone, and 0.923 (0.911-0.934) for the two methods combined, with a PPV of 0.735 (0.716-0.754), 0.976 (0.969-0.983), and 0.957 (0.948-0.965) for each method, respectively. The mean dose of infliximab was 433 mg in the reference standard, 337 mg from HCPCS, 434 mg from NLP, and 426 mg from the combined method.
CONCLUSIONS: HCPCS codes alone are not sufficient to accurately identify infliximab infusion dates and doses in the VHA system. The use of NLP significantly improved the sensitivity and PPV for estimating infusion dates and doses, especially when combined with HCPCS codes.","The use of natural language processing of infusion notes to identify outpatient infusions PURPOSE: Outpatient infusions are commonly missing in Veterans Health Affairs (VHA) pharmacy dispensing data sets. Currently, Healthcare Common Procedure Coding System (HCPCS) codes are used to identify outpatient infusions, but concerns exist if they correctly capture all infusions and infusion-related data such as dose and date of administration. We developed natural language processing (NLP) software to extract infusion information from medical text infusion notes. The objective was to compare the sensitivity of three approaches to identify infliximab administration dates and infusion doses against a reference standard established from the Veterans Affairs rheumatoid arthritis (VARA) registry.
METHODS: We compared the sensitivity and positive predictive value (PPV) of NLP to that of HCPCS codes in identifying the correct date and dose of infliximab infusions against a human extracted reference standard.
RESULTS: The sensitivity was 0.606 (0.585-0.627) for HCPCS alone, 0.858 (0.842-0.873) for NLP alone, and 0.923 (0.911-0.934) for the two methods combined, with a PPV of 0.735 (0.716-0.754), 0.976 (0.969-0.983), and 0.957 (0.948-0.965) for each method, respectively. The mean dose of infliximab was 433 mg in the reference standard, 337 mg from HCPCS, 434 mg from NLP, and 426 mg from the combined method.
CONCLUSIONS: HCPCS codes alone are not sufficient to accurately identify infliximab infusion dates and doses in the VHA system. The use of NLP significantly improved the sensitivity and PPV for estimating infusion dates and doses, especially when combined with HCPCS codes.","[-0.32174647  0.33198673  0.22601552 ...  0.08336312 -0.03822357
 -0.19465035]",0.91512835,0.8147337,Other,"natural language processing, NLP, language processing"
23554109,Natural Language Processing to identify pneumonia from radiology reports,"Dublin S, Baldwin E, Walker RL, Christensen LM, Haug PJ, Jackson ML, Nelson JC, Ferraro J, Carrell D, Chapman WW.",Pharmacoepidemiol Drug Saf. 2013 Aug;22(8):834-41. doi: 10.1002/pds.3418. Epub 2013 Apr 1.,Dublin S,Pharmacoepidemiol Drug Saf,2013,2013/04/05,PMC3811072,NIHMS471897,10.1002/pds.3418,"PURPOSE: This study aimed to develop Natural Language Processing (NLP) approaches to supplement manual outcome validation, specifically to validate pneumonia cases from chest radiograph reports.
METHODS: We trained one NLP system, ONYX, using radiograph reports from children and adults that were previously manually reviewed. We then assessed its validity on a test set of 5000 reports. We aimed to substantially decrease manual review, not replace it entirely, and so, we classified reports as follows: (1) consistent with pneumonia; (2) inconsistent with pneumonia; or (3) requiring manual review because of complex features. We developed processes tailored either to optimize accuracy or to minimize manual review. Using logistic regression, we jointly modeled sensitivity and specificity of ONYX in relation to patient age, comorbidity, and care setting. We estimated positive and negative predictive value (PPV and NPV) assuming pneumonia prevalence in the source data.
RESULTS: Tailored for accuracy, ONYX identified 25% of reports as requiring manual review (34% of true pneumonias and 18% of non-pneumonias). For the remainder, ONYX's sensitivity was 92% (95% CI 90-93%), specificity 87% (86-88%), PPV 74% (72-76%), and NPV 96% (96-97%). Tailored to minimize manual review, ONYX classified 12% as needing manual review. For the remainder, ONYX had sensitivity 75% (72-77%), specificity 95% (94-96%), PPV 86% (83-88%), and NPV 91% (90-91%).
CONCLUSIONS: For pneumonia validation, ONYX can replace almost 90% of manual review while maintaining low to moderate misclassification rates. It can be tailored for different outcomes and study needs and thus warrants exploration in other settings.","Natural Language Processing to identify pneumonia from radiology reports PURPOSE: This study aimed to develop Natural Language Processing (NLP) approaches to supplement manual outcome validation, specifically to validate pneumonia cases from chest radiograph reports.
METHODS: We trained one NLP system, ONYX, using radiograph reports from children and adults that were previously manually reviewed. We then assessed its validity on a test set of 5000 reports. We aimed to substantially decrease manual review, not replace it entirely, and so, we classified reports as follows: (1) consistent with pneumonia; (2) inconsistent with pneumonia; or (3) requiring manual review because of complex features. We developed processes tailored either to optimize accuracy or to minimize manual review. Using logistic regression, we jointly modeled sensitivity and specificity of ONYX in relation to patient age, comorbidity, and care setting. We estimated positive and negative predictive value (PPV and NPV) assuming pneumonia prevalence in the source data.
RESULTS: Tailored for accuracy, ONYX identified 25% of reports as requiring manual review (34% of true pneumonias and 18% of non-pneumonias). For the remainder, ONYX's sensitivity was 92% (95% CI 90-93%), specificity 87% (86-88%), PPV 74% (72-76%), and NPV 96% (96-97%). Tailored to minimize manual review, ONYX classified 12% as needing manual review. For the remainder, ONYX had sensitivity 75% (72-77%), specificity 95% (94-96%), PPV 86% (83-88%), and NPV 91% (90-91%).
CONCLUSIONS: For pneumonia validation, ONYX can replace almost 90% of manual review while maintaining low to moderate misclassification rates. It can be tailored for different outcomes and study needs and thus warrants exploration in other settings.","[ 0.05279308  0.76249385  0.17298408 ...  0.1666095   0.2943431
 -0.45051163]",0.9078666,0.8014163,Other,"natural language processing, NLP, language processing"
20616993,Text and structural data mining of influenza mentions in Web and social media,"Corley CD, Cook DJ, Mikler AR, Singh KP.",Int J Environ Res Public Health. 2010 Feb;7(2):596-615. doi: 10.3390/ijerph7020596. Epub 2010 Feb 22.,Corley CD,Int J Environ Res Public Health,2010,2010/07/10,PMC2872292,,10.3390/ijerph7020596,"Text and structural data mining of web and social media (WSM) provides a novel disease surveillance resource and can identify online communities for targeted public health communications (PHC) to assure wide dissemination of pertinent information. WSM that mention influenza are harvested over a 24-week period, 5 October 2008 to 21 March 2009. Link analysis reveals communities for targeted PHC. Text mining is shown to identify trends in flu posts that correlate to real-world influenza-like illness patient report data. We also bring to bear a graph-based data mining technique to detect anomalies among flu blogs connected by publisher type, links, and user-tags.","Text and structural data mining of influenza mentions in Web and social media Text and structural data mining of web and social media (WSM) provides a novel disease surveillance resource and can identify online communities for targeted public health communications (PHC) to assure wide dissemination of pertinent information. WSM that mention influenza are harvested over a 24-week period, 5 October 2008 to 21 March 2009. Link analysis reveals communities for targeted PHC. Text mining is shown to identify trends in flu posts that correlate to real-world influenza-like illness patient report data. We also bring to bear a graph-based data mining technique to detect anomalies among flu blogs connected by publisher type, links, and user-tags.","[-0.09073303  0.27349576  0.15146773 ...  0.90949684 -0.40152395
 -0.20752236]",0.93159854,0.8176628,Both,text mining
17238751,EpiPortal: an electronic decision support system for infection control,"Wajngurt D, Hong F, Chaudhry R, Hyman S, Ross B, Fracaro M.",AMIA Annu Symp Proc. 2006;2006:1132.,Wajngurt D,AMIA Annu Symp Proc,2006,2007/01/24,PMC1839617,,,"A comprehensive, electronic hospital epidemiology decision support system serves diverse users but its primary user is the infection control professional (ICP). Utilizing off-the-shelf components and accepted standards enables the system to be open, vendor-independent and ICP-controlled. Its development can flexibly respond to the evolving nature of infection control practice.","EpiPortal: an electronic decision support system for infection control A comprehensive, electronic hospital epidemiology decision support system serves diverse users but its primary user is the infection control professional (ICP). Utilizing off-the-shelf components and accepted standards enables the system to be open, vendor-independent and ICP-controlled. Its development can flexibly respond to the evolving nature of infection control practice.","[ 0.3718747   0.64566183  0.3062088  ...  0.29157746 -0.26264247
 -0.00245619]",0.90257204,0.81792176,Other,Not Specified
39383158,Modeling health risks using neural network ensembles,"Smith BM, Criminisi A, Sorek N, Harari Y, Sood N, Heymsfield SB.",PLoS One. 2024 Oct 9;19(10):e0308922. doi: 10.1371/journal.pone.0308922. eCollection 2024.,Smith BM,PLoS One,2024,2024/10/09,PMC11463747,,10.1371/journal.pone.0308922,"This study aims to demonstrate that demographics combined with biometrics can be used to predict obesity related chronic disease risk and produce a health risk score that outperforms body mass index (BMI)-the most commonly used biomarker for obesity. We propose training an ensemble of small neural networks to fuse demographics and biometrics inputs. The categorical outputs of the networks are then turned into a multi-dimensional risk map, which associates diverse inputs with stratified, output health risk. Our ensemble model is optimized and validated on disjoint subsets of nationally representative data (N~100,000) from the National Health and Nutrition Examination Survey (NHANES). To broaden applicability of the proposed method, we consider only non-invasive inputs that can be easily measured through modern devices. Our results show that: (a) neural networks can predict individual conditions (e.g., diabetes, hypertension) or the union of multiple (e.g., nine) health conditions; (b) Softmax model outputs can be used to stratify individual- or any-condition risk; (c) ensembles of neural networks improve generalizability; (d) multiple-input models outperform BMI (e.g., 75.1% area under the receiver operator curve for eight-input, any-condition models compared to 64.2% for BMI); (e) small neural networks are as effective as larger ones for the inference tasks considered; the proposed models are small enough that they can be expressed as human-readable equations, and they can be adapted to clinical settings to identify high-risk, undiagnosed populations.","Modeling health risks using neural network ensembles This study aims to demonstrate that demographics combined with biometrics can be used to predict obesity related chronic disease risk and produce a health risk score that outperforms body mass index (BMI)-the most commonly used biomarker for obesity. We propose training an ensemble of small neural networks to fuse demographics and biometrics inputs. The categorical outputs of the networks are then turned into a multi-dimensional risk map, which associates diverse inputs with stratified, output health risk. Our ensemble model is optimized and validated on disjoint subsets of nationally representative data (N~100,000) from the National Health and Nutrition Examination Survey (NHANES). To broaden applicability of the proposed method, we consider only non-invasive inputs that can be easily measured through modern devices. Our results show that: (a) neural networks can predict individual conditions (e.g., diabetes, hypertension) or the union of multiple (e.g., nine) health conditions; (b) Softmax model outputs can be used to stratify individual- or any-condition risk; (c) ensembles of neural networks improve generalizability; (d) multiple-input models outperform BMI (e.g., 75.1% area under the receiver operator curve for eight-input, any-condition models compared to 64.2% for BMI); (e) small neural networks are as effective as larger ones for the inference tasks considered; the proposed models are small enough that they can be expressed as human-readable equations, and they can be adapted to clinical settings to identify high-risk, undiagnosed populations.","[-0.07363383  0.8922657   0.32556507 ...  0.8535912   0.01052005
 -0.57331777]",0.91086596,0.81485844,Other,neural network
38420033,A multidimensional comparative study of help-seeking messages on Weibo under different stages of COVID-19 pandemic in China,"Jiang J, Yao C, Song X.",Front Public Health. 2024 Feb 14;12:1320146. doi: 10.3389/fpubh.2024.1320146. eCollection 2024.,Jiang J,Front Public Health,2024,2024/02/29,PMC10899892,,10.3389/fpubh.2024.1320146,"OBJECTIVE: During the COVID-19 pandemic, people posted help-seeking messages on Weibo, a mainstream social media in China, to solve practical problems. As viruses, policies, and perceptions have all changed, help-seeking behavior on Weibo has been shown to evolve in this paper.
METHODS: We compare and analyze the help-seeking messages from three dimensions: content categories, time distribution, and retweeting influencing factors. First, we crawled the help-seeking messages from Weibo, and successively used CNN and xlm-roberta-large models for text classification to analyze the changes of help-seeking messages in different stages from the content categories dimension. Subsequently, we studied the time distribution of help-seeking messages and calculated the time lag using TLCC algorithm. Finally, we analyze the changes of the retweeting influencing factors of help-seeking messages in different stages by negative binomial regression.
RESULTS: (1) Help-seekers in different periods have different emphasis on content. (2) There is a significant correlation between new daily help-seeking messages and new confirmed cases in the middle stage (1/1/2022-5/20/2022), with a 16-day time lag, but there is no correlation in the latter stage (12/10/2022-2/25/2023). (3) In all the periods, pictures or videos, and the length of the text have a significant positive effect on the number of retweets of help-seeking messages, but other factors do not have exactly the same effect on the retweeting volume.
CONCLUSION: This paper demonstrates the evolution of help-seeking messages during different stages of the COVID-19 pandemic in three dimensions: content categories, time distribution, and retweeting influencing factors, which are worthy of reference for decision-makers and help-seekers, as well as provide thinking for subsequent studies.","A multidimensional comparative study of help-seeking messages on Weibo under different stages of COVID-19 pandemic in China OBJECTIVE: During the COVID-19 pandemic, people posted help-seeking messages on Weibo, a mainstream social media in China, to solve practical problems. As viruses, policies, and perceptions have all changed, help-seeking behavior on Weibo has been shown to evolve in this paper.
METHODS: We compare and analyze the help-seeking messages from three dimensions: content categories, time distribution, and retweeting influencing factors. First, we crawled the help-seeking messages from Weibo, and successively used CNN and xlm-roberta-large models for text classification to analyze the changes of help-seeking messages in different stages from the content categories dimension. Subsequently, we studied the time distribution of help-seeking messages and calculated the time lag using TLCC algorithm. Finally, we analyze the changes of the retweeting influencing factors of help-seeking messages in different stages by negative binomial regression.
RESULTS: (1) Help-seekers in different periods have different emphasis on content. (2) There is a significant correlation between new daily help-seeking messages and new confirmed cases in the middle stage (1/1/2022-5/20/2022), with a 16-day time lag, but there is no correlation in the latter stage (12/10/2022-2/25/2023). (3) In all the periods, pictures or videos, and the length of the text have a significant positive effect on the number of retweets of help-seeking messages, but other factors do not have exactly the same effect on the retweeting volume.
CONCLUSION: This paper demonstrates the evolution of help-seeking messages during different stages of the COVID-19 pandemic in three dimensions: content categories, time distribution, and retweeting influencing factors, which are worthy of reference for decision-makers and help-seekers, as well as provide thinking for subsequent studies.","[ 0.06384382  0.21372773  0.16701828 ...  0.7383644  -0.00291801
 -0.35052723]",0.9116404,0.82008165,Both,CNN
38271404,In-hospital real-time prediction of COVID-19 severity regardless of disease phase using electronic health records,"Park H, Choi CM, Kim SH, Kim SH, Kim DK, Jeong JB.",PLoS One. 2024 Jan 25;19(1):e0294362. doi: 10.1371/journal.pone.0294362. eCollection 2024.,Park H,PLoS One,2024,2024/01/25,PMC10810421,,10.1371/journal.pone.0294362,"Coronavirus disease 2019 (COVID-19) has strained healthcare systems worldwide. Predicting COVID-19 severity could optimize resource allocation, like oxygen devices and intensive care. If machine learning model could forecast the severity of COVID-19 patients, hospital resource allocation would be more comfortable. This study evaluated machine learning models using electronic records from 3,996 COVID-19 patients to forecast mild, moderate, or severe disease up to 2 days in advance. A deep neural network (DNN) model achieved 91.8% accuracy, 0.96 AUROC, and 0.90 AUPRC for 2-day predictions, regardless of disease phase. Tree-based models like random forest achieved slightly better metrics (random forest: 94.1% of accuracy, 0.98 AUROC, 0.95 AUPRC; Gradient boost: 94.1% of accuracy, 0.98 AUROC, 0.94 AUPRC), prioritizing treatment factors like steroid use. However, the DNN relied more on fixed patient factors like demographics and symptoms in aspect to SHAP value importance. Since treatment patterns vary between hospitals, the DNN may be more generalizable than tree-based models (random forest, gradient boost model). The results demonstrate accurate short-term forecasting of COVID-19 severity using routine clinical data. DNN models may balance predictive performance and generalizability better than other methods. Severity predictions by machine learning model could facilitate resource planning, like ICU arrangement and oxygen devices.","In-hospital real-time prediction of COVID-19 severity regardless of disease phase using electronic health records Coronavirus disease 2019 (COVID-19) has strained healthcare systems worldwide. Predicting COVID-19 severity could optimize resource allocation, like oxygen devices and intensive care. If machine learning model could forecast the severity of COVID-19 patients, hospital resource allocation would be more comfortable. This study evaluated machine learning models using electronic records from 3,996 COVID-19 patients to forecast mild, moderate, or severe disease up to 2 days in advance. A deep neural network (DNN) model achieved 91.8% accuracy, 0.96 AUROC, and 0.90 AUPRC for 2-day predictions, regardless of disease phase. Tree-based models like random forest achieved slightly better metrics (random forest: 94.1% of accuracy, 0.98 AUROC, 0.95 AUPRC; Gradient boost: 94.1% of accuracy, 0.98 AUROC, 0.94 AUPRC), prioritizing treatment factors like steroid use. However, the DNN relied more on fixed patient factors like demographics and symptoms in aspect to SHAP value importance. Since treatment patterns vary between hospitals, the DNN may be more generalizable than tree-based models (random forest, gradient boost model). The results demonstrate accurate short-term forecasting of COVID-19 severity using routine clinical data. DNN models may balance predictive performance and generalizability better than other methods. Severity predictions by machine learning model could facilitate resource planning, like ICU arrangement and oxygen devices.","[ 0.07372008  0.85854834  0.50663626 ...  0.48613292  0.22921628
 -0.5770603 ]",0.92189056,0.802106,Other,"neural network, machine learning model"
37806430,Assessing the effectiveness of artificial neural networks (ANN) and multiple linear regressions (MLR) in forcasting AQI and PM10 and evaluating health impacts through AirQ+ (case study: Tehran),"Shams SR, Kalantary S, Jahani A, Parsa Shams SM, Kalantari B, Singh D, Moeinnadini M, Choi Y.",Environ Pollut. 2023 Dec 1;338:122623. doi: 10.1016/j.envpol.2023.122623. Epub 2023 Oct 6.,Shams SR,Environ Pollut,2023,2023/10/08,,,10.1016/j.envpol.2023.122623,"Air pollution is one of the major concerns for the population and the environment due to its hazardous effects. PM<sub>10</sub> has affected significant scientific and regulatory interest because of its strong correlation with chronic health such as respiratory illnesses, lung cancer, and asthma. Forcasting air quality and assessing the health impacts of the air pollutants like particulate matter is crucial for protecting public health.This study incorporated weather, traffic, green space information, and time parameters, to forcst the AQI and PM<sub>10</sub>. Traffic data plays a critical role in predicting air pollution, as it significantly influences them. Therefore, including traffic data in the ANN model is necessary and valuable. Green spaces also affect air quality, and their inclusion in neural network models can improve predictive accuracy. The key factors influencing the AQI are the two-day lag time, the proximity of a park to the AQI monitoring station, the average distance between each park and AQI monitoring stations, and the air temperature. In addition, the average distance between each park, the number of parks, seasonal variations, and the total number of vehicles are the primary determinants affecting PM<sub>10</sub>.The straightforward effective Multilayer Perceptron Artificial Neural Network (MLP-ANN) demonstrated correlation coefficients (R) of 0.82 and 0.93 when forcasting AQI and PM<sub>10</sub>, respectively. This study also used the forcasted PM<sub>10</sub> values from the ANN model to assess the health effects of elevated air pollution. The results indicate that elevated levels of PM<sub>10</sub> can increase the likelihood of respiratory symptoms. Among children, there is a higher prevalence of bronchitis, while among adults, the incidence of chronic bronchitis is higher. It was estimated that the attributable proportions for children and adults were 6.87% and 9.72%, respectively. These results underscore the importance of monitoring air quality and taking action to reduce pollution to safeguard public health.","Assessing the effectiveness of artificial neural networks (ANN) and multiple linear regressions (MLR) in forcasting AQI and PM10 and evaluating health impacts through AirQ+ (case study: Tehran) Air pollution is one of the major concerns for the population and the environment due to its hazardous effects. PM<sub>10</sub> has affected significant scientific and regulatory interest because of its strong correlation with chronic health such as respiratory illnesses, lung cancer, and asthma. Forcasting air quality and assessing the health impacts of the air pollutants like particulate matter is crucial for protecting public health.This study incorporated weather, traffic, green space information, and time parameters, to forcst the AQI and PM<sub>10</sub>. Traffic data plays a critical role in predicting air pollution, as it significantly influences them. Therefore, including traffic data in the ANN model is necessary and valuable. Green spaces also affect air quality, and their inclusion in neural network models can improve predictive accuracy. The key factors influencing the AQI are the two-day lag time, the proximity of a park to the AQI monitoring station, the average distance between each park and AQI monitoring stations, and the air temperature. In addition, the average distance between each park, the number of parks, seasonal variations, and the total number of vehicles are the primary determinants affecting PM<sub>10</sub>.The straightforward effective Multilayer Perceptron Artificial Neural Network (MLP-ANN) demonstrated correlation coefficients (R) of 0.82 and 0.93 when forcasting AQI and PM<sub>10</sub>, respectively. This study also used the forcasted PM<sub>10</sub> values from the ANN model to assess the health effects of elevated air pollution. The results indicate that elevated levels of PM<sub>10</sub> can increase the likelihood of respiratory symptoms. Among children, there is a higher prevalence of bronchitis, while among adults, the incidence of chronic bronchitis is higher. It was estimated that the attributable proportions for children and adults were 6.87% and 9.72%, respectively. These results underscore the importance of monitoring air quality and taking action to reduce pollution to safeguard public health.","[-0.41017324  0.10802431  0.07636454 ...  0.72500265  0.25390697
 -0.04932882]",0.92036927,0.80534947,Other,"neural network, artificial neural network, multilayer perceptron"
37167092,NAFLDkb: A Knowledge Base and Platform for Drug Development against Nonalcoholic Fatty Liver Disease,"Xu T, Gao W, Zhu L, Chen W, Niu C, Yin W, Ma L, Zhu X, Ling Y, Gao S, Liu L, Jiao N, Chen W, Zhang G, Zhu R, Wu D.",J Chem Inf Model. 2024 Apr 8;64(7):2817-2828. doi: 10.1021/acs.jcim.3c00395. Epub 2023 May 11.,Xu T,J Chem Inf Model,2024,2023/05/11,,,10.1021/acs.jcim.3c00395,"Nonalcoholic fatty liver disease (NAFLD) is the most common chronic liver disease with a broad spectrum of histologic manifestations. The rapidly growing prevalence and the complex pathologic mechanisms of NAFLD pose great challenges for treatment development. Despite tremendous efforts devoted to drug development, there are no FDA-approved medicines yet. Here, we present NAFLDkb, a specialized knowledge base and platform for computer-aided drug design against NAFLD. With multiperspective information curated from diverse source materials and public databases, NAFLDkb presents the associations of drug-related entities as individual knowledge graphs. Practical drug discovery tools that facilitate the utilization and expansion of NAFLDkb have also been implemented in the web interface, including chemical structure search, drug-likeness screening, knowledge-based repositioning, and research article annotation. Moreover, case studies of a knowledge graph repositioning model and a generative neural network model are presented herein, where three repositioning drug candidates and 137 novel lead-like compounds were newly established as NAFLD pharmacotherapy options reusing data records and machine learning tools in NAFLDkb, suggesting its clinical reliability and great potential in identifying novel drug-disease associations of NAFLD and generating new insights to accelerate NAFLD drug development. NAFLDkb is freely accessible at https://www.biosino.org/nafldkb and will be updated periodically with the latest findings.","NAFLDkb: A Knowledge Base and Platform for Drug Development against Nonalcoholic Fatty Liver Disease Nonalcoholic fatty liver disease (NAFLD) is the most common chronic liver disease with a broad spectrum of histologic manifestations. The rapidly growing prevalence and the complex pathologic mechanisms of NAFLD pose great challenges for treatment development. Despite tremendous efforts devoted to drug development, there are no FDA-approved medicines yet. Here, we present NAFLDkb, a specialized knowledge base and platform for computer-aided drug design against NAFLD. With multiperspective information curated from diverse source materials and public databases, NAFLDkb presents the associations of drug-related entities as individual knowledge graphs. Practical drug discovery tools that facilitate the utilization and expansion of NAFLDkb have also been implemented in the web interface, including chemical structure search, drug-likeness screening, knowledge-based repositioning, and research article annotation. Moreover, case studies of a knowledge graph repositioning model and a generative neural network model are presented herein, where three repositioning drug candidates and 137 novel lead-like compounds were newly established as NAFLD pharmacotherapy options reusing data records and machine learning tools in NAFLDkb, suggesting its clinical reliability and great potential in identifying novel drug-disease associations of NAFLD and generating new insights to accelerate NAFLD drug development. NAFLDkb is freely accessible at https://www.biosino.org/nafldkb and will be updated periodically with the latest findings.","[-0.16465677  0.72842973  0.04223033 ...  0.4246801  -0.4536625
 -0.00943234]",0.91547567,0.802296,Other,neural network
36339186,"An SEM-ANN approach to evaluate public awareness about COVID, A pathway toward adaptation effective strategies for sustainable development","Sohail MT, Yang M, Maresova P, Mustafa S.",Front Public Health. 2022 Oct 19;10:1046780. doi: 10.3389/fpubh.2022.1046780. eCollection 2022.,Sohail MT,Front Public Health,2022,2022/11/07,PMC9627197,,10.3389/fpubh.2022.1046780,"This study was conducted to evaluate public awareness about COVID with aimed to check public strategies against COVID-19. A semi structured questionnaire was collected and the data was analyzed using some statistical tools (PLS-SEM) and artificial neural networks (ANN). We started by looking at the known causal linkages between the different variables to see if they matched up with the hypotheses that had been proposed. Next, for this reason, we ran a 5,000-sample bootstrapping test to assess how strongly our findings corroborated the null hypothesis. PLS-SEM direct path analysis revealed HRP -&gt; PA-COVID, HI -&gt; PA-COVID, MU -&gt; PA-COVID, PM -&gt; PA-COVID, SD -&gt; PA-COVID. These findings provide credence to the acceptance of hypotheses H1, H3, and H5, but reject hypothesis H2. We have also examined control factors such as respondents' age, gender, and level of education. Age was found to have a positive correlation with PA-COVID, while mean gender and education level were found to not correlate at all with PA-COVID. However, age can be a useful control variable, as a more seasoned individual is likely to have a better understanding of COVID and its effects on independent variables. Study results revealed a small moderation effect in the relationships between understudy independent and dependent variables. Education significantly moderates the relationship of PA-COVID associated with MU, PH, SD, RP, PM, PA-COVID, depicts the moderation role of education on the relationship between MU*Education-&gt;PA-COVID, HI*Education-&gt;PA.COVID, SD*Education-&gt;PA.COVID, HRP*Education-&gt;PA.COVID, PM*Education -&gt; PA.COVID. The artificial neural network (ANN) model we've developed for spreading information about COVID-19 (PA-COVID) follows in the footsteps of previous studies. The root means the square of the errors (RMSE). Validity measures how well a model can predict a certain result. With RMSE values of 0.424 for training and 0.394 for testing, we observed that our ANN model for public awareness of COVID-19 (PA-COVID) had a strong predictive ability. Based on the sensitivity analysis results, we determined that PA. COVID had the highest relative normalized relevance for our sample (100%). These factors were then followed by MU (54.6%), HI (11.1%), SD (100.0%), HRP (28.5%), and PM (64.6%) were likewise shown to be the least important factors for consumers in developing countries struggling with diseases caused by contaminated water. In addition, a specific approach was used to construct a goodness-of-fit coefficient to evaluate the performance of the ANN models. The study will aid in the implementation of effective monitoring and public policies to promote the health of local people.","An SEM-ANN approach to evaluate public awareness about COVID, A pathway toward adaptation effective strategies for sustainable development This study was conducted to evaluate public awareness about COVID with aimed to check public strategies against COVID-19. A semi structured questionnaire was collected and the data was analyzed using some statistical tools (PLS-SEM) and artificial neural networks (ANN). We started by looking at the known causal linkages between the different variables to see if they matched up with the hypotheses that had been proposed. Next, for this reason, we ran a 5,000-sample bootstrapping test to assess how strongly our findings corroborated the null hypothesis. PLS-SEM direct path analysis revealed HRP -&gt; PA-COVID, HI -&gt; PA-COVID, MU -&gt; PA-COVID, PM -&gt; PA-COVID, SD -&gt; PA-COVID. These findings provide credence to the acceptance of hypotheses H1, H3, and H5, but reject hypothesis H2. We have also examined control factors such as respondents' age, gender, and level of education. Age was found to have a positive correlation with PA-COVID, while mean gender and education level were found to not correlate at all with PA-COVID. However, age can be a useful control variable, as a more seasoned individual is likely to have a better understanding of COVID and its effects on independent variables. Study results revealed a small moderation effect in the relationships between understudy independent and dependent variables. Education significantly moderates the relationship of PA-COVID associated with MU, PH, SD, RP, PM, PA-COVID, depicts the moderation role of education on the relationship between MU*Education-&gt;PA-COVID, HI*Education-&gt;PA.COVID, SD*Education-&gt;PA.COVID, HRP*Education-&gt;PA.COVID, PM*Education -&gt; PA.COVID. The artificial neural network (ANN) model we've developed for spreading information about COVID-19 (PA-COVID) follows in the footsteps of previous studies. The root means the square of the errors (RMSE). Validity measures how well a model can predict a certain result. With RMSE values of 0.424 for training and 0.394 for testing, we observed that our ANN model for public awareness of COVID-19 (PA-COVID) had a strong predictive ability. Based on the sensitivity analysis results, we determined that PA. COVID had the highest relative normalized relevance for our sample (100%). These factors were then followed by MU (54.6%), HI (11.1%), SD (100.0%), HRP (28.5%), and PM (64.6%) were likewise shown to be the least important factors for consumers in developing countries struggling with diseases caused by contaminated water. In addition, a specific approach was used to construct a goodness-of-fit coefficient to evaluate the performance of the ANN models. The study will aid in the implementation of effective monitoring and public policies to promote the health of local people.","[ 0.1923645   0.0698764   0.04121148 ...  0.48503363  0.3808608
 -0.24079923]",0.90228534,0.8136819,Other,"neural network, artificial neural network"
36207072,Defining factors in hospital admissions during COVID-19 using LSTM-FCA explainable model,"Md Saleh NI, Ab Ghani H, Jilani Z.",Artif Intell Med. 2022 Oct;132:102394. doi: 10.1016/j.artmed.2022.102394. Epub 2022 Sep 5.,Md Saleh NI,Artif Intell Med,2022,2022/10/07,PMC9443659,,10.1016/j.artmed.2022.102394,"Outbreaks of the COVID-19 pandemic caused by the SARS-CoV-2 infection that started in Wuhan, China, have quickly spread worldwide. The current situation has contributed to a dynamic rate of hospital admissions. Global efforts by Artificial Intelligence (AI) and Machine Learning (ML) communities to develop solutions to assist COVID-19-related research have escalated ever since. However, despite overwhelming efforts from the AI and ML community, many machine learning-based AI systems have been designed as black boxes. This paper proposes a model that utilizes Formal Concept Analysis (FCA) to explain a machine learning technique called Long-short Term Memory (LSTM) on a dataset of hospital admissions due to COVID-19 in the United Kingdom. This paper intends to increase the transparency of decision-making in the era of ML by using the proposed LSTM-FCA explainable model. Both LSTM and FCA are able to evaluate the data and explain the model to make the results more understandable and interpretable. The results and discussions are helpful and may lead to new research to optimize the use of ML in various real-world applications and to contain the disease.","Defining factors in hospital admissions during COVID-19 using LSTM-FCA explainable model Outbreaks of the COVID-19 pandemic caused by the SARS-CoV-2 infection that started in Wuhan, China, have quickly spread worldwide. The current situation has contributed to a dynamic rate of hospital admissions. Global efforts by Artificial Intelligence (AI) and Machine Learning (ML) communities to develop solutions to assist COVID-19-related research have escalated ever since. However, despite overwhelming efforts from the AI and ML community, many machine learning-based AI systems have been designed as black boxes. This paper proposes a model that utilizes Formal Concept Analysis (FCA) to explain a machine learning technique called Long-short Term Memory (LSTM) on a dataset of hospital admissions due to COVID-19 in the United Kingdom. This paper intends to increase the transparency of decision-making in the era of ML by using the proposed LSTM-FCA explainable model. Both LSTM and FCA are able to evaluate the data and explain the model to make the results more understandable and interpretable. The results and discussions are helpful and may lead to new research to optimize the use of ML in various real-world applications and to contain the disease.","[-0.0606687   0.8352078   0.4533663  ...  0.8767213  -0.22929135
 -0.5907928 ]",0.9074983,0.8070179,Both,LSTM
35948797,Multi-region machine learning-based novel ensemble approaches for predicting COVID-19 pandemic in Africa,"Ibrahim Z, Tulay P, Abdullahi J.",Environ Sci Pollut Res Int. 2023 Jan;30(2):3621-3643. doi: 10.1007/s11356-022-22373-6. Epub 2022 Aug 11.,Ibrahim Z,Environ Sci Pollut Res Int,2023,2022/08/10,PMC9365685,,10.1007/s11356-022-22373-6,"Coronavirus disease 2019 (COVID-19) has produced a global pandemic, which has devastating effects on health, economy and social interactions. Despite the less contraction and spread of COVID-19 in Africa compared to some other continents in the world, Africa remains amongst the most vulnerable regions due to less technology and unequipped or poor health system. Recent happenings showed that COVID-19 may stay for years owing to the discoveries of new variants (such as Omicron) and new wave of infections in several countries. Therefore, accurate prediction of new cases is vital to make informed decisions and in evaluating the measures that should be implemented. Studies on COVID-19 prediction are limited in Africa despite the risks and dangers that the virus possessed. Hence, this study was performed to predict daily COVID-19 cases in 10 African countries spread across the north, south, east, west and central Africa considering countries with few and large number of daily COVID-19 cases. Machine learning (ML) models due to their nonlinearity and accurate prediction capabilities were employed for this purpose, including artificial neural network (ANN), adaptive neuro-fuzzy inference system (ANFIS), support vector machine (SVM) and conventional multiple linear regression (MLR) models. As any other natural process, the COVID-19 pandemic may contain both linear and nonlinear aspects. In such circumstances, neither nonlinear (ML) nor linear (MLR) models could be sufficient; hence, combining both ML and MLR models may produce better accuracy. Consequently, to improve the prediction efficiency of the ML models, novel ensemble approaches including ANN-E and SVM-E were employed. The advantage of using ensemble approaches is that they provide collective benefits of all the standalone models, thereby reducing their weaknesses and enhancing their prediction capabilities. The obtained results showed that ANFIS led to better prediction performance with MAD = 0.0106, MSE = 0.0003, RMSE = 0.0185 and R2 = 0.9059 in the validation step. The results of the proposed ensemble approaches demonstrated very high improvements in predicting the COVID-19 pandemic in Africa with MAD = 0.0073, MSE = 0.0002, RMSE = 0.0155 and R2 = 0.9616. The ANN-E improved the standalone models performance in the validation step up to 10%, 14%, 42%, 6%, 83%, 11%, 7%, 5%, 7% and 31% for Morocco, Sudan, Namibia, South Africa, Uganda, Rwanda, Nigeria, Senegal, Gabon and Cameroon, respectively. This study results offer a solid foundation in the application of ensemble approaches for predicting COVID-19 pandemic across all regions and countries in the world.","Multi-region machine learning-based novel ensemble approaches for predicting COVID-19 pandemic in Africa Coronavirus disease 2019 (COVID-19) has produced a global pandemic, which has devastating effects on health, economy and social interactions. Despite the less contraction and spread of COVID-19 in Africa compared to some other continents in the world, Africa remains amongst the most vulnerable regions due to less technology and unequipped or poor health system. Recent happenings showed that COVID-19 may stay for years owing to the discoveries of new variants (such as Omicron) and new wave of infections in several countries. Therefore, accurate prediction of new cases is vital to make informed decisions and in evaluating the measures that should be implemented. Studies on COVID-19 prediction are limited in Africa despite the risks and dangers that the virus possessed. Hence, this study was performed to predict daily COVID-19 cases in 10 African countries spread across the north, south, east, west and central Africa considering countries with few and large number of daily COVID-19 cases. Machine learning (ML) models due to their nonlinearity and accurate prediction capabilities were employed for this purpose, including artificial neural network (ANN), adaptive neuro-fuzzy inference system (ANFIS), support vector machine (SVM) and conventional multiple linear regression (MLR) models. As any other natural process, the COVID-19 pandemic may contain both linear and nonlinear aspects. In such circumstances, neither nonlinear (ML) nor linear (MLR) models could be sufficient; hence, combining both ML and MLR models may produce better accuracy. Consequently, to improve the prediction efficiency of the ML models, novel ensemble approaches including ANN-E and SVM-E were employed. The advantage of using ensemble approaches is that they provide collective benefits of all the standalone models, thereby reducing their weaknesses and enhancing their prediction capabilities. The obtained results showed that ANFIS led to better prediction performance with MAD = 0.0106, MSE = 0.0003, RMSE = 0.0185 and R2 = 0.9059 in the validation step. The results of the proposed ensemble approaches demonstrated very high improvements in predicting the COVID-19 pandemic in Africa with MAD = 0.0073, MSE = 0.0002, RMSE = 0.0155 and R2 = 0.9616. The ANN-E improved the standalone models performance in the validation step up to 10%, 14%, 42%, 6%, 83%, 11%, 7%, 5%, 7% and 31% for Morocco, Sudan, Namibia, South Africa, Uganda, Rwanda, Nigeria, Senegal, Gabon and Cameroon, respectively. This study results offer a solid foundation in the application of ensemble approaches for predicting COVID-19 pandemic across all regions and countries in the world.","[ 0.04965849  0.5311723   0.30368432 ...  0.57741624  0.15796284
 -0.4035672 ]",0.9205874,0.8007271,Other,"neural network, artificial neural network"
35937245,Prediction of COVID-19 Data Using Hybrid Modeling Approaches,"Zhao W, Sun Y, Li Y, Guan W.",Front Public Health. 2022 Jul 22;10:923978. doi: 10.3389/fpubh.2022.923978. eCollection 2022.,Zhao W,Front Public Health,2022,2022/08/08,PMC9354929,,10.3389/fpubh.2022.923978,"A major emphasis is the dissemination of COVID-19 across the country's many regions and provinces. Using the present COVID-19 pandemic as a guide, the researchers suggest a hybrid model architecture for analyzing and optimizing COVID-19 data during the complete country. The analysis of COVID-19's exploration and death rate uses an ARIMA model with susceptible-infectious-removed and susceptible-exposed-infectious-removed (SEIR) models. The logistic model's failure to forecast the number of confirmed diagnoses and the snags of the SEIR model's too many tuning parameters are both addressed by a hybrid model method. Logistic regression (LR), Autoregressive Integrated Moving Average Model (ARIMA), support vector regression (SVR), multilayer perceptron (MLP), Recurrent Neural Networks (RNN), Gate Recurrent Unit (GRU), and long short-term memory (LSTM) are utilized for the same purpose. Root mean square error, mean absolute error, and mean absolute percentage error are used to show these models. New COVID-19 cases, the number of quarantines, mortality rates, and the deployment of public self-protection measures to reduce the epidemic are all outlined in the study's findings. Government officials can use the findings to guide future illness prevention and control choices.","Prediction of COVID-19 Data Using Hybrid Modeling Approaches A major emphasis is the dissemination of COVID-19 across the country's many regions and provinces. Using the present COVID-19 pandemic as a guide, the researchers suggest a hybrid model architecture for analyzing and optimizing COVID-19 data during the complete country. The analysis of COVID-19's exploration and death rate uses an ARIMA model with susceptible-infectious-removed and susceptible-exposed-infectious-removed (SEIR) models. The logistic model's failure to forecast the number of confirmed diagnoses and the snags of the SEIR model's too many tuning parameters are both addressed by a hybrid model method. Logistic regression (LR), Autoregressive Integrated Moving Average Model (ARIMA), support vector regression (SVR), multilayer perceptron (MLP), Recurrent Neural Networks (RNN), Gate Recurrent Unit (GRU), and long short-term memory (LSTM) are utilized for the same purpose. Root mean square error, mean absolute error, and mean absolute percentage error are used to show these models. New COVID-19 cases, the number of quarantines, mortality rates, and the deployment of public self-protection measures to reduce the epidemic are all outlined in the study's findings. Government officials can use the findings to guide future illness prevention and control choices.","[ 0.1175115   0.4477545   0.24633102 ...  0.5577588  -0.11979136
 -0.6674156 ]",0.9103054,0.81681406,Other,"neural network, multilayer perceptron, recurrent neural network, RNN, LSTM"
35747601,Comparison of Conventional Modeling Techniques with the Neural Network Autoregressive Model (NNAR): Application to COVID-19 Data,"Daniyal M, Tawiah K, Muhammadullah S, Opoku-Ameyaw K.",J Healthc Eng. 2022 Jun 14;2022:4802743. doi: 10.1155/2022/4802743. eCollection 2022.,Daniyal M,J Healthc Eng,2022,2022/06/24,PMC9213132,,10.1155/2022/4802743,"The coronavirus disease 2019 (COVID-19) pandemic continues to destroy human life around the world. Almost every country throughout the globe suffered from this pandemic, forcing various governments to apply different restrictions to reduce its impact. In this study, we compare different time-series models with the neural network autoregressive model (NNAR). The study used COVID-19 data in Pakistan from February 26, 2020, to February 18, 2022, as a training and testing data set for modeling. Different models were applied and estimated on the training data set, and these models were assessed on the testing data set. Based on the mean absolute scaled error (MAE) and root mean square error (RMSE) for the training and testing data sets, the NNAR model outperformed the autoregressive integrated moving average (ARIMA) model and other competing models indicating that the NNAR model is the most appropriate for forecasting. Forecasts from the NNAR model showed that the cumulative confirmed COVID-19 cases will be 1,597,180 and cumulative confirmed COVID-19 deaths will be 32,628 on April 18, 2022. We encourage the Pakistan Government to boost its immunization policy.","Comparison of Conventional Modeling Techniques with the Neural Network Autoregressive Model (NNAR): Application to COVID-19 Data The coronavirus disease 2019 (COVID-19) pandemic continues to destroy human life around the world. Almost every country throughout the globe suffered from this pandemic, forcing various governments to apply different restrictions to reduce its impact. In this study, we compare different time-series models with the neural network autoregressive model (NNAR). The study used COVID-19 data in Pakistan from February 26, 2020, to February 18, 2022, as a training and testing data set for modeling. Different models were applied and estimated on the training data set, and these models were assessed on the testing data set. Based on the mean absolute scaled error (MAE) and root mean square error (RMSE) for the training and testing data sets, the NNAR model outperformed the autoregressive integrated moving average (ARIMA) model and other competing models indicating that the NNAR model is the most appropriate for forecasting. Forecasts from the NNAR model showed that the cumulative confirmed COVID-19 cases will be 1,597,180 and cumulative confirmed COVID-19 deaths will be 32,628 on April 18, 2022. We encourage the Pakistan Government to boost its immunization policy.","[-0.08073583  0.20967858  0.19049883 ...  0.4701772   0.06986372
 -0.45133445]",0.907427,0.8011874,Other,neural network
35192526,Analysis and modeling of COVID-19 epidemic dynamics in Saudi Arabia using SIR-PSO and machine learning approaches,"Zrieq R, Boubaker S, Kamel S, Alzain M, Algahtani FD.",J Infect Dev Ctries. 2022 Jan 31;16(1):90-100. doi: 10.3855/jidc.15004.,Zrieq R,J Infect Dev Ctries,2022,2022/02/22,,,10.3855/jidc.15004,"INTRODUCTION: COVID-19 has become a global concern because it has extensive damage to health, social and economic systems worldwide. Consequently, there is an urgent need to develop tools to understand, analyze, monitor and control further outbreaks of the disease.
METHODOLOGY: The Susceptible Infected Recovered-Particle SwarmOptimization model and the feed-forward artificial neural network model were separately developed to model COVID-19 dynamics based on daily time-series data reported by the Saudi authorities from March 2, 2020 to February 21, 2021. The collected data were divided into training and validation datasets. The effectiveness of the investigated models was evaluated by using various performance metrics. The Susceptible-Infected-Recovered-Particle-Swarm-Optimization model was found to well predict the cumulative infected and recovered cases and to optimally tune the contact rate and the characteristic duration of the illness. The feed-forward artificial neural network model was found to be efficient in modeling daily new and cumulative infections, recoveries and deaths.
RESULTS: The forecasts provided by the investigated models had high coefficient of determination values of more than 0.97 and low mean absolute percentage errors (around 7% on average).
CONCLUSIONS: Both the Susceptible-Infected-Recovered-Particle-Swarm-Optimization and feed-forward artificial neural network models were efficient in modeling COVID-19 dynamics in Saudi Arabia. The results produced by the models can help the Saudi health authorities to analyze the virus dynamics and prepare efficient measures to control any future occurrence of the epidemic.","Analysis and modeling of COVID-19 epidemic dynamics in Saudi Arabia using SIR-PSO and machine learning approaches INTRODUCTION: COVID-19 has become a global concern because it has extensive damage to health, social and economic systems worldwide. Consequently, there is an urgent need to develop tools to understand, analyze, monitor and control further outbreaks of the disease.
METHODOLOGY: The Susceptible Infected Recovered-Particle SwarmOptimization model and the feed-forward artificial neural network model were separately developed to model COVID-19 dynamics based on daily time-series data reported by the Saudi authorities from March 2, 2020 to February 21, 2021. The collected data were divided into training and validation datasets. The effectiveness of the investigated models was evaluated by using various performance metrics. The Susceptible-Infected-Recovered-Particle-Swarm-Optimization model was found to well predict the cumulative infected and recovered cases and to optimally tune the contact rate and the characteristic duration of the illness. The feed-forward artificial neural network model was found to be efficient in modeling daily new and cumulative infections, recoveries and deaths.
RESULTS: The forecasts provided by the investigated models had high coefficient of determination values of more than 0.97 and low mean absolute percentage errors (around 7% on average).
CONCLUSIONS: Both the Susceptible-Infected-Recovered-Particle-Swarm-Optimization and feed-forward artificial neural network models were efficient in modeling COVID-19 dynamics in Saudi Arabia. The results produced by the models can help the Saudi health authorities to analyze the virus dynamics and prepare efficient measures to control any future occurrence of the epidemic.","[ 0.26487678  0.5152063   0.2730736  ...  0.58217895  0.04536783
 -0.3921095 ]",0.9193016,0.80030763,Other,"neural network, artificial neural network"
35129088,Enabling CT-Scans for covid detection using transfer learning-based neural networks,"Dubey AK, Mohbey KK.",J Biomol Struct Dyn. 2023 Apr;41(6):2528-2539. doi: 10.1080/07391102.2022.2034668. Epub 2022 Feb 6.,Dubey AK,J Biomol Struct Dyn,2023,2022/02/07,,,10.1080/07391102.2022.2034668,"Today, we are coping with the pandemic, and the novel virus is covertly evolving day by day. Therefore, a precautionary system to deal with the issue is required as early as possible. The last few years were very challenging for doctors, vaccine makers, hospitals, and medical authorities to deal with the massive crowd to provide results for all patients and newcomers in the past months. Thus, these issues should be handled with a robust system that can accord with many people and deliver the results in a fraction of time without visiting public places and help reduce crowd gathering. So, to deal with these issues, we developed an AI model using transfer learning that can aid doctors and other people to get to know whether they were suffering from covid or not. In this paper, we have used VGG-19 (CNN-based) model with open-sourced COVID-CT (CTSI) dataset. The dataset consists of 349 images of COVID-19 of 216 patients and 463 images of NON-COVID-19. We have achieved an accuracy of 95%, precision of 96%, recall of 94%, and F1-Score of 96% from the experiments.Communicated by Ramaswamy H. Sarma.","Enabling CT-Scans for covid detection using transfer learning-based neural networks Today, we are coping with the pandemic, and the novel virus is covertly evolving day by day. Therefore, a precautionary system to deal with the issue is required as early as possible. The last few years were very challenging for doctors, vaccine makers, hospitals, and medical authorities to deal with the massive crowd to provide results for all patients and newcomers in the past months. Thus, these issues should be handled with a robust system that can accord with many people and deliver the results in a fraction of time without visiting public places and help reduce crowd gathering. So, to deal with these issues, we developed an AI model using transfer learning that can aid doctors and other people to get to know whether they were suffering from covid or not. In this paper, we have used VGG-19 (CNN-based) model with open-sourced COVID-CT (CTSI) dataset. The dataset consists of 349 images of COVID-19 of 216 patients and 463 images of NON-COVID-19. We have achieved an accuracy of 95%, precision of 96%, recall of 94%, and F1-Score of 96% from the experiments.Communicated by Ramaswamy H. Sarma.","[ 0.08930291  0.26809788  0.24436553 ...  0.59070057  0.14457893
 -0.06101765]",0.9308634,0.8081745,Computer Vision,"neural network, CNN"
35089226,An app to classify a 5-year survival in patients with breast cancer using the convolutional neural networks (CNN) in Microsoft Excel: Development and usability study,"Lin CY, Chien TW, Chen YH, Lee YL, Su SB.",Medicine (Baltimore). 2022 Jan 28;101(4):e28697. doi: 10.1097/MD.0000000000028697.,Lin CY,Medicine (Baltimore),2022,2022/01/28,PMC8797502,,10.1097/MD.0000000000028697,"BACKGROUND: Breast cancer (BC) is the most common malignant cancer in women. A predictive model is required to predict the 5-year survival in patients with BC (5YSPBC) and improve the treatment quality by increasing their survival rate. However, no reports in literature about apps developed and designed in medical practice to classify the 5YSPBC. This study aimed to build a model to develop an app for an automatically accurate classification of the 5YSPBC.
METHODS: A total of 1810 patients with BC were recruited in a hospital in Taiwan from the secondary data with codes on 53 characteristic variables that were endorsed by professional staff clerks as of December 31, 2019. Five models (i.e., revolution neural network [CNN], artificial neural network, Naïve Bayes, K-nearest Neighbors Algorithm, and Logistic regression) and 3 tasks (i.e., extraction of feature variables, model comparison in accuracy [ACC] and stability, and app development) were performed to achieve the goal of developing an app to predict the 5YSPBC. The sensitivity, specificity, and receiver operating characteristic curve (area under ROC curve) on models across 2 scenarios of training (70%) and testing (30%) sets were compared. An app predicting the 5YSPBC was developed involving the model estimated parameters for a website assessment.
RESULTS: We observed that the 15-variable CNN model yields higher ACC rates (0.87 and 0.86) with area under ROC curves of 0.80 and 0.78 (95% confidence interval 0.78-82 and 0.74-81) based on 1357 training and 540 testing cases an available app for patients predicting the 5YSPBC was successfully developed and demonstrated in this study.
CONCLUSION: The 15-variable CNN model with 38 parameters estimated using CNN for improving the ACC of the 5YSPBC has been particularly demonstrated in Microsoft Excel. An app developed for helping clinicians assess the 5YSPBC in clinical settings is required for application in the future.","An app to classify a 5-year survival in patients with breast cancer using the convolutional neural networks (CNN) in Microsoft Excel: Development and usability study BACKGROUND: Breast cancer (BC) is the most common malignant cancer in women. A predictive model is required to predict the 5-year survival in patients with BC (5YSPBC) and improve the treatment quality by increasing their survival rate. However, no reports in literature about apps developed and designed in medical practice to classify the 5YSPBC. This study aimed to build a model to develop an app for an automatically accurate classification of the 5YSPBC.
METHODS: A total of 1810 patients with BC were recruited in a hospital in Taiwan from the secondary data with codes on 53 characteristic variables that were endorsed by professional staff clerks as of December 31, 2019. Five models (i.e., revolution neural network [CNN], artificial neural network, Naïve Bayes, K-nearest Neighbors Algorithm, and Logistic regression) and 3 tasks (i.e., extraction of feature variables, model comparison in accuracy [ACC] and stability, and app development) were performed to achieve the goal of developing an app to predict the 5YSPBC. The sensitivity, specificity, and receiver operating characteristic curve (area under ROC curve) on models across 2 scenarios of training (70%) and testing (30%) sets were compared. An app predicting the 5YSPBC was developed involving the model estimated parameters for a website assessment.
RESULTS: We observed that the 15-variable CNN model yields higher ACC rates (0.87 and 0.86) with area under ROC curves of 0.80 and 0.78 (95% confidence interval 0.78-82 and 0.74-81) based on 1357 training and 540 testing cases an available app for patients predicting the 5YSPBC was successfully developed and demonstrated in this study.
CONCLUSION: The 15-variable CNN model with 38 parameters estimated using CNN for improving the ACC of the 5YSPBC has been particularly demonstrated in Microsoft Excel. An app developed for helping clinicians assess the 5YSPBC in clinical settings is required for application in the future.","[-0.05233611  0.54869413  0.15255888 ...  0.22269869  0.2002724
 -0.6002658 ]",0.9114485,0.81075364,Other,"neural network, artificial neural network, convolutional neural network, CNN"
34745565,Optimised deep neural network model to predict asthma exacerbation based on personalised weather triggers,"Haque R, Ho SB, Chai I, Abdullah A.",F1000Res. 2021 Sep 10;10:911. doi: 10.12688/f1000research.73026.1. eCollection 2021.,Haque R,F1000Res,2021,2021/11/08,PMC8543171,,10.12688/f1000research.73026.1,"Background - Recently, there have been attempts to develop mHealth applications for asthma self-management. However, there is a lack of applications that can offer accurate predictions of asthma exacerbation using the weather triggers and demographic characteristics to give tailored response to users. This paper proposes an optimised Deep Neural Network Regression (DNNR) model to predict asthma exacerbation based on personalised weather triggers. Methods - With the aim of integrating weather, demography, and asthma tracking, an mHealth application was developed where users conduct the Asthma Control Test (ACT) to identify the chances of their asthma exacerbation. The asthma dataset consists of panel data from 10 users that includes 1010 ACT scores as the target output. Moreover, the dataset contains 10 input features which include five weather features (temperature, humidity, air-pressure, UV-index, wind-speed) and five demography features (age, gender, outdoor-job, outdoor-activities, location). Results - Using the DNNR model on the asthma dataset, a score of 0.83 was achieved with Mean Absolute Error (MAE)=1.44 and Mean Squared Error (MSE)=3.62. It was recognised that, for effective asthma self-management, the prediction errors must be in the acceptable loss range (error&lt;0.5). Therefore, an optimisation process was proposed to reduce the error rates and increase the accuracy by applying standardisation and fragmented-grid-search. Consequently, the optimised-DNNR model (with 2 hidden-layers and 50 hidden-nodes) using the Adam optimiser achieved a 94% accuracy with MAE=0.20 and MSE=0.09. Conclusions - This study is the first of its kind that recognises the potentials of DNNR to identify the correlation patterns among asthma, weather, and demographic variables. The optimised-DNNR model provides predictions with a significantly higher accuracy rate than the existing predictive models and using less computing time. Thus, the optimisation process is useful to build an enhanced model that can be integrated into the asthma self-management for mHealth application.","Optimised deep neural network model to predict asthma exacerbation based on personalised weather triggers Background - Recently, there have been attempts to develop mHealth applications for asthma self-management. However, there is a lack of applications that can offer accurate predictions of asthma exacerbation using the weather triggers and demographic characteristics to give tailored response to users. This paper proposes an optimised Deep Neural Network Regression (DNNR) model to predict asthma exacerbation based on personalised weather triggers. Methods - With the aim of integrating weather, demography, and asthma tracking, an mHealth application was developed where users conduct the Asthma Control Test (ACT) to identify the chances of their asthma exacerbation. The asthma dataset consists of panel data from 10 users that includes 1010 ACT scores as the target output. Moreover, the dataset contains 10 input features which include five weather features (temperature, humidity, air-pressure, UV-index, wind-speed) and five demography features (age, gender, outdoor-job, outdoor-activities, location). Results - Using the DNNR model on the asthma dataset, a score of 0.83 was achieved with Mean Absolute Error (MAE)=1.44 and Mean Squared Error (MSE)=3.62. It was recognised that, for effective asthma self-management, the prediction errors must be in the acceptable loss range (error&lt;0.5). Therefore, an optimisation process was proposed to reduce the error rates and increase the accuracy by applying standardisation and fragmented-grid-search. Consequently, the optimised-DNNR model (with 2 hidden-layers and 50 hidden-nodes) using the Adam optimiser achieved a 94% accuracy with MAE=0.20 and MSE=0.09. Conclusions - This study is the first of its kind that recognises the potentials of DNNR to identify the correlation patterns among asthma, weather, and demographic variables. The optimised-DNNR model provides predictions with a significantly higher accuracy rate than the existing predictive models and using less computing time. Thus, the optimisation process is useful to build an enhanced model that can be integrated into the asthma self-management for mHealth application.","[-0.3245629   0.45785838  0.12217008 ...  1.110809    0.02939834
 -0.65293187]",0.9070214,0.82242763,Other,neural network
34725416,Estimating the COVID-19 prevalence and mortality using a novel data-driven hybrid model based on ensemble empirical mode decomposition,"Wang Y, Xu C, Yao S, Wang L, Zhao Y, Ren J, Li Y.",Sci Rep. 2021 Nov 1;11(1):21413. doi: 10.1038/s41598-021-00948-6.,Wang Y,Sci Rep,2021,2021/11/02,PMC8560776,,10.1038/s41598-021-00948-6,"In this study, we proposed a new data-driven hybrid technique by integrating an ensemble empirical mode decomposition (EEMD), an autoregressive integrated moving average (ARIMA), with a nonlinear autoregressive artificial neural network (NARANN), called the EEMD-ARIMA-NARANN model, to perform time series modeling and forecasting based on the COVID-19 prevalence and mortality data from 28 February 2020 to 27 June 2020 in South Africa and Nigeria. By comparing the accuracy level of forecasting measurements with the basic ARIMA and NARANN models, it was shown that this novel data-driven hybrid model did a better job of capturing the dynamic changing trends of the target data than the others used in this work. Our proposed mixture technique can be deemed as a helpful policy-supportive tool to plan and provide medical supplies effectively. The overall confirmed cases and deaths were estimated to reach around 176,570 [95% uncertainty level (UL) 173,607 to 178,476] and 3454 (95% UL 3384 to 3487), respectively, in South Africa, along with 32,136 (95% UL 31,568 to 32,641) and 788 (95% UL 775 to 804) in Nigeria on 12 July 2020 using this data-driven EEMD-ARIMA-NARANN hybrid technique. The contributions of this study include three aspects. First, the proposed hybrid model can better capture the dynamic dependency characteristics compared with the individual models. Second, this new data-driven hybrid model is constructed in a more reasonable way relative to the traditional mixture model. Third, this proposed model may be generalized to estimate the epidemic patterns of COVID-19 in other regions.","Estimating the COVID-19 prevalence and mortality using a novel data-driven hybrid model based on ensemble empirical mode decomposition In this study, we proposed a new data-driven hybrid technique by integrating an ensemble empirical mode decomposition (EEMD), an autoregressive integrated moving average (ARIMA), with a nonlinear autoregressive artificial neural network (NARANN), called the EEMD-ARIMA-NARANN model, to perform time series modeling and forecasting based on the COVID-19 prevalence and mortality data from 28 February 2020 to 27 June 2020 in South Africa and Nigeria. By comparing the accuracy level of forecasting measurements with the basic ARIMA and NARANN models, it was shown that this novel data-driven hybrid model did a better job of capturing the dynamic changing trends of the target data than the others used in this work. Our proposed mixture technique can be deemed as a helpful policy-supportive tool to plan and provide medical supplies effectively. The overall confirmed cases and deaths were estimated to reach around 176,570 [95% uncertainty level (UL) 173,607 to 178,476] and 3454 (95% UL 3384 to 3487), respectively, in South Africa, along with 32,136 (95% UL 31,568 to 32,641) and 788 (95% UL 775 to 804) in Nigeria on 12 July 2020 using this data-driven EEMD-ARIMA-NARANN hybrid technique. The contributions of this study include three aspects. First, the proposed hybrid model can better capture the dynamic dependency characteristics compared with the individual models. Second, this new data-driven hybrid model is constructed in a more reasonable way relative to the traditional mixture model. Third, this proposed model may be generalized to estimate the epidemic patterns of COVID-19 in other regions.","[-0.14765464  0.44177067  0.19475915 ...  0.48617405  0.0182912
 -0.49018386]",0.9013857,0.81107783,Other,"neural network, artificial neural network"
33917544,Predicting the Dynamics of the COVID-19 Pandemic in the United States Using Graph Theory-Based Neural Networks,"Davahli MR, Fiok K, Karwowski W, Aljuaid AM, Taiar R.",Int J Environ Res Public Health. 2021 Apr 6;18(7):3834. doi: 10.3390/ijerph18073834.,Davahli MR,Int J Environ Res Public Health,2021,2021/04/30,PMC8038789,,10.3390/ijerph18073834,"The COVID-19 pandemic has had unprecedented social and economic consequences in the United States. Therefore, accurately predicting the dynamics of the pandemic can be very beneficial. Two main elements required for developing reliable predictions include: (1) a predictive model and (2) an indicator of the current condition and status of the pandemic. As a pandemic indicator, we used the effective reproduction number (Rt), which is defined as the number of new infections transmitted by a single contagious individual in a population that may no longer be fully susceptible. To bring the pandemic under control, Rt must be less than one. To eliminate the pandemic, Rt should be close to zero. Therefore, this value may serve as a strong indicator of the current status of the pandemic. For a predictive model, we used graph neural networks (GNNs), a method that combines graphical analysis with the structure of neural networks. We developed two types of GNN models, including: (1) graph-theory-based neural networks (GTNN) and (2) neighborhood-based neural networks (NGNN). The nodes in both graphs indicated individual states in the United States. While the GTNN model's edges document functional connectivity between states, those in the NGNN model link neighboring states to one another. We trained both models with R<sub>t</sub> numbers collected over the previous four days and asked them to predict the following day for all states in the United States. The performance of these models was evaluated with the datasets that included R<sub>t</sub> values reflecting conditions from 22 January through 26 November 2020 (before the start of COVID-19 vaccination in the United States). To determine the efficiency, we compared the results of two models with each other and with those generated by a baseline Long short-term memory (LSTM) model. The results indicated that the GTNN model outperformed both the NGNN and LSTM models for predicting Rt.","Predicting the Dynamics of the COVID-19 Pandemic in the United States Using Graph Theory-Based Neural Networks The COVID-19 pandemic has had unprecedented social and economic consequences in the United States. Therefore, accurately predicting the dynamics of the pandemic can be very beneficial. Two main elements required for developing reliable predictions include: (1) a predictive model and (2) an indicator of the current condition and status of the pandemic. As a pandemic indicator, we used the effective reproduction number (Rt), which is defined as the number of new infections transmitted by a single contagious individual in a population that may no longer be fully susceptible. To bring the pandemic under control, Rt must be less than one. To eliminate the pandemic, Rt should be close to zero. Therefore, this value may serve as a strong indicator of the current status of the pandemic. For a predictive model, we used graph neural networks (GNNs), a method that combines graphical analysis with the structure of neural networks. We developed two types of GNN models, including: (1) graph-theory-based neural networks (GTNN) and (2) neighborhood-based neural networks (NGNN). The nodes in both graphs indicated individual states in the United States. While the GTNN model's edges document functional connectivity between states, those in the NGNN model link neighboring states to one another. We trained both models with R<sub>t</sub> numbers collected over the previous four days and asked them to predict the following day for all states in the United States. The performance of these models was evaluated with the datasets that included R<sub>t</sub> values reflecting conditions from 22 January through 26 November 2020 (before the start of COVID-19 vaccination in the United States). To determine the efficiency, we compared the results of two models with each other and with those generated by a baseline Long short-term memory (LSTM) model. The results indicated that the GTNN model outperformed both the NGNN and LSTM models for predicting Rt.","[-0.19924265  0.36330107  0.3652795  ...  0.717908   -0.24316007
 -0.30866644]",0.9054646,0.8068479,Other,"neural network, LSTM"
33859222,GraphCovidNet: A graph neural network based model for detecting COVID-19 from CT scans and X-rays of chest,"Saha P, Mukherjee D, Singh PK, Ahmadian A, Ferrara M, Sarkar R.",Sci Rep. 2021 Apr 15;11(1):8304. doi: 10.1038/s41598-021-87523-1.,Saha P,Sci Rep,2021,2021/04/16,PMC8050058,,10.1038/s41598-021-87523-1,"COVID-19, a viral infection originated from Wuhan, China has spread across the world and it has currently affected over 115 million people. Although vaccination process has already started, reaching sufficient availability will take time. Considering the impact of this widespread disease, many research attempts have been made by the computer scientists to screen the COVID-19 from Chest X-Rays (CXRs) or Computed Tomography (CT) scans. To this end, we have proposed GraphCovidNet, a Graph Isomorphic Network (GIN) based model which is used to detect COVID-19 from CT-scans and CXRs of the affected patients. Our proposed model only accepts input data in the form of graph as we follow a GIN based architecture. Initially, pre-processing is performed to convert an image data into an undirected graph to consider only the edges instead of the whole image. Our proposed GraphCovidNet model is evaluated on four standard datasets: SARS-COV-2 Ct-Scan dataset, COVID-CT dataset, combination of covid-chestxray-dataset, Chest X-Ray Images (Pneumonia) dataset and CMSC-678-ML-Project dataset. The model shows an impressive accuracy of 99% for all the datasets and its prediction capability becomes 100% accurate for the binary classification problem of detecting COVID-19 scans. Source code of this work can be found at GitHub-link .","GraphCovidNet: A graph neural network based model for detecting COVID-19 from CT scans and X-rays of chest COVID-19, a viral infection originated from Wuhan, China has spread across the world and it has currently affected over 115 million people. Although vaccination process has already started, reaching sufficient availability will take time. Considering the impact of this widespread disease, many research attempts have been made by the computer scientists to screen the COVID-19 from Chest X-Rays (CXRs) or Computed Tomography (CT) scans. To this end, we have proposed GraphCovidNet, a Graph Isomorphic Network (GIN) based model which is used to detect COVID-19 from CT-scans and CXRs of the affected patients. Our proposed model only accepts input data in the form of graph as we follow a GIN based architecture. Initially, pre-processing is performed to convert an image data into an undirected graph to consider only the edges instead of the whole image. Our proposed GraphCovidNet model is evaluated on four standard datasets: SARS-COV-2 Ct-Scan dataset, COVID-CT dataset, combination of covid-chestxray-dataset, Chest X-Ray Images (Pneumonia) dataset and CMSC-678-ML-Project dataset. The model shows an impressive accuracy of 99% for all the datasets and its prediction capability becomes 100% accurate for the binary classification problem of detecting COVID-19 scans. Source code of this work can be found at GitHub-link .","[ 0.04352932  0.49096864  0.15883893 ...  0.889732    0.0126374
 -0.30562252]",0.9015786,0.82366484,Other,neural network
33507932,Variational-LSTM autoencoder to forecast the spread of coronavirus across the globe,"Ibrahim MR, Haworth J, Lipani A, Aslam N, Cheng T, Christie N.",PLoS One. 2021 Jan 28;16(1):e0246120. doi: 10.1371/journal.pone.0246120. eCollection 2021.,Ibrahim MR,PLoS One,2021,2021/01/28,PMC7842932,,10.1371/journal.pone.0246120,"Modelling the spread of coronavirus globally while learning trends at global and country levels remains crucial for tackling the pandemic. We introduce a novel variational-LSTM Autoencoder model to predict the spread of coronavirus for each country across the globe. This deep Spatio-temporal model does not only rely on historical data of the virus spread but also includes factors related to urban characteristics represented in locational and demographic data (such as population density, urban population, and fertility rate), an index that represents the governmental measures and response amid toward mitigating the outbreak (includes 13 measures such as: 1) school closing, 2) workplace closing, 3) cancelling public events, 4) close public transport, 5) public information campaigns, 6) restrictions on internal movements, 7) international travel controls, 8) fiscal measures, 9) monetary measures, 10) emergency investment in health care, 11) investment in vaccines, 12) virus testing framework, and 13) contact tracing). In addition, the introduced method learns to generate a graph to adjust the spatial dependences among different countries while forecasting the spread. We trained two models for short and long-term forecasts. The first one is trained to output one step in future with three previous timestamps of all features across the globe, whereas the second model is trained to output 10 steps in future. Overall, the trained models show high validation for forecasting the spread for each country for short and long-term forecasts, which makes the introduce method a useful tool to assist decision and policymaking for the different corners of the globe.","Variational-LSTM autoencoder to forecast the spread of coronavirus across the globe Modelling the spread of coronavirus globally while learning trends at global and country levels remains crucial for tackling the pandemic. We introduce a novel variational-LSTM Autoencoder model to predict the spread of coronavirus for each country across the globe. This deep Spatio-temporal model does not only rely on historical data of the virus spread but also includes factors related to urban characteristics represented in locational and demographic data (such as population density, urban population, and fertility rate), an index that represents the governmental measures and response amid toward mitigating the outbreak (includes 13 measures such as: 1) school closing, 2) workplace closing, 3) cancelling public events, 4) close public transport, 5) public information campaigns, 6) restrictions on internal movements, 7) international travel controls, 8) fiscal measures, 9) monetary measures, 10) emergency investment in health care, 11) investment in vaccines, 12) virus testing framework, and 13) contact tracing). In addition, the introduced method learns to generate a graph to adjust the spatial dependences among different countries while forecasting the spread. We trained two models for short and long-term forecasts. The first one is trained to output one step in future with three previous timestamps of all features across the globe, whereas the second model is trained to output 10 steps in future. Overall, the trained models show high validation for forecasting the spread for each country for short and long-term forecasts, which makes the introduce method a useful tool to assist decision and policymaking for the different corners of the globe.","[-0.17191118  0.48584872  0.16312183 ...  1.0865434  -0.17115104
 -0.45675334]",0.9052508,0.81534755,Other,LSTM
32826097,"Real-time burn depth assessment using artificial networks: a large-scale, multicentre study","Wang Y, Ke Z, He Z, Chen X, Zhang Y, Xie P, Li T, Zhou J, Li F, Yang C, Zhang P, Huang C, Kai L.",Burns. 2020 Dec;46(8):1829-1838. doi: 10.1016/j.burns.2020.07.010. Epub 2020 Jul 25.,Wang Y,Burns,2020,2020/08/23,,,10.1016/j.burns.2020.07.010,"INTRODUCTION: Early judgment of the depth of burns is very important for the accurate formulation of treatment plans. In medical imaging the application of Artificial Intelligence has the potential for serving as a very experienced assistant to improve early clinical diagnosis. Due to lack of large volume of a particular feature, there has been almost no progress in burn field.
METHODS: 484 early wound images are collected on patients who discharged home after a burn injury in 48 h, from five different levels of hospitals in Hunan Province China. According to actual healing time, all images are manually annotated by five professional burn surgeons and divided into three sets which are shallow(0-10 days), moderate(11-20 days) and deep(more than 21 days or skin graft healing). These ROIs were further divided into 5637 patches sizes 224 × 224 pixels, of which 1733 shallow, 1804 moderate, and 2100 deep. We used transfer learning suing a Pre-trained ResNet50 model and the ratio of all images is 7:1.5:1.5 for training:validation:test.
RESULTS: A novel artificial burn depth recognition model based on convolutional neural network was established and the diagnostic accuracy of the three types of burns is about 80%.
DISCUSSION: The actual healing time can be used to deduce the depth of burn involvement. The artificial burn depth recognition model can accurately infer healing time and burn depth of the patient, which is expected to be used for auxiliary diagnosis improvement.","Real-time burn depth assessment using artificial networks: a large-scale, multicentre study INTRODUCTION: Early judgment of the depth of burns is very important for the accurate formulation of treatment plans. In medical imaging the application of Artificial Intelligence has the potential for serving as a very experienced assistant to improve early clinical diagnosis. Due to lack of large volume of a particular feature, there has been almost no progress in burn field.
METHODS: 484 early wound images are collected on patients who discharged home after a burn injury in 48 h, from five different levels of hospitals in Hunan Province China. According to actual healing time, all images are manually annotated by five professional burn surgeons and divided into three sets which are shallow(0-10 days), moderate(11-20 days) and deep(more than 21 days or skin graft healing). These ROIs were further divided into 5637 patches sizes 224 × 224 pixels, of which 1733 shallow, 1804 moderate, and 2100 deep. We used transfer learning suing a Pre-trained ResNet50 model and the ratio of all images is 7:1.5:1.5 for training:validation:test.
RESULTS: A novel artificial burn depth recognition model based on convolutional neural network was established and the diagnostic accuracy of the three types of burns is about 80%.
DISCUSSION: The actual healing time can be used to deduce the depth of burn involvement. The artificial burn depth recognition model can accurately infer healing time and burn depth of the patient, which is expected to be used for auxiliary diagnosis improvement.","[-0.24610558  0.5599687   0.17875418 ...  0.6511917  -0.07161064
 -0.46423903]",0.90971684,0.8026157,Computer Vision,"neural network, convolutional neural network"
32565882,Modeling the Spread of COVID-19 Infection Using a Multilayer Perceptron,"Car Z, Baressi Šegota S, Anđelić N, Lorencin I, Mrzljak V.",Comput Math Methods Med. 2020 May 29;2020:5714714. doi: 10.1155/2020/5714714. eCollection 2020.,Car Z,Comput Math Methods Med,2020,2020/06/23,PMC7260624,,10.1155/2020/5714714,"Coronavirus (COVID-19) is a highly infectious disease that has captured the attention of the worldwide public. Modeling of such diseases can be extremely important in the prediction of their impact. While classic, statistical, modeling can provide satisfactory models, it can also fail to comprehend the intricacies contained within the data. In this paper, authors use a publicly available dataset, containing information on infected, recovered, and deceased patients in 406 locations over 51 days (22nd January 2020 to 12th March 2020). This dataset, intended to be a time-series dataset, is transformed into a regression dataset and used in training a multilayer perceptron (MLP) artificial neural network (ANN). The aim of training is to achieve a worldwide model of the maximal number of patients across all locations in each time unit. Hyperparameters of the MLP are varied using a grid search algorithm, with a total of 5376 hyperparameter combinations. Using those combinations, a total of 48384 ANNs are trained (16128 for each patient group-deceased, recovered, and infected), and each model is evaluated using the coefficient of determination (R2). Cross-validation is performed using K-fold algorithm with 5-folds. Best models achieved consists of 4 hidden layers with 4 neurons in each of those layers, and use a ReLU activation function, with R2 scores of 0.98599 for confirmed, 0.99429 for deceased, and 0.97941 for recovered patient models. When cross-validation is performed, these scores drop to 0.94 for confirmed, 0.781 for recovered, and 0.986 for deceased patient models, showing high robustness of the deceased patient model, good robustness for confirmed, and low robustness for recovered patient model.","Modeling the Spread of COVID-19 Infection Using a Multilayer Perceptron Coronavirus (COVID-19) is a highly infectious disease that has captured the attention of the worldwide public. Modeling of such diseases can be extremely important in the prediction of their impact. While classic, statistical, modeling can provide satisfactory models, it can also fail to comprehend the intricacies contained within the data. In this paper, authors use a publicly available dataset, containing information on infected, recovered, and deceased patients in 406 locations over 51 days (22nd January 2020 to 12th March 2020). This dataset, intended to be a time-series dataset, is transformed into a regression dataset and used in training a multilayer perceptron (MLP) artificial neural network (ANN). The aim of training is to achieve a worldwide model of the maximal number of patients across all locations in each time unit. Hyperparameters of the MLP are varied using a grid search algorithm, with a total of 5376 hyperparameter combinations. Using those combinations, a total of 48384 ANNs are trained (16128 for each patient group-deceased, recovered, and infected), and each model is evaluated using the coefficient of determination (R2). Cross-validation is performed using K-fold algorithm with 5-folds. Best models achieved consists of 4 hidden layers with 4 neurons in each of those layers, and use a ReLU activation function, with R2 scores of 0.98599 for confirmed, 0.99429 for deceased, and 0.97941 for recovered patient models. When cross-validation is performed, these scores drop to 0.94 for confirmed, 0.781 for recovered, and 0.986 for deceased patient models, showing high robustness of the deceased patient model, good robustness for confirmed, and low robustness for recovered patient model.","[-0.10841805  0.4232635   0.2721909  ...  0.78888613  0.04411756
 -0.5371336 ]",0.9081197,0.80833113,Other,"neural network, artificial neural network, multilayer perceptron"
31868917,Broad-Spectrum Profiling of Drug Safety via Learning Complex Network,"Liu K, Ding RF, Xu H, Qin YM, He QS, Du F, Zhang Y, Yao LX, You P, Xiang YP, Ji ZL.",Clin Pharmacol Ther. 2020 Jun;107(6):1373-1382. doi: 10.1002/cpt.1750. Epub 2020 Feb 28.,Liu K,Clin Pharmacol Ther,2020,2019/12/24,PMC7325315,,10.1002/cpt.1750,"Drug safety is a severe clinical pharmacology and toxicology problem that has caused immense medical and social burdens every year. Regretfully, a reproducible method to assess drug safety systematically and quantitatively is still missing. In this study, we developed an advanced machine learning model for de novo drug safety assessment by solving the multilayer drug-gene-adverse drug reaction (ADR) interaction network. For the first time, the drug safety was assessed in a broad landscape of 1,156 distinct ADRs. We also designed a parameter ToxicityScore to quantify the overall drug safety. Moreover, we determined association strength for every 3,807,631 gene-ADR interactions, which clues mechanistic exploration of ADRs. For convenience, we deployed the model as a web service ADRAlert-gene at http://www.bio-add.org/ADRAlert/. In summary, this study offers insights into prioritizing safe drug therapy. It helps reduce the attrition rate of new drug discovery by providing a reliable ADR profile in the early preclinical stage.","Broad-Spectrum Profiling of Drug Safety via Learning Complex Network Drug safety is a severe clinical pharmacology and toxicology problem that has caused immense medical and social burdens every year. Regretfully, a reproducible method to assess drug safety systematically and quantitatively is still missing. In this study, we developed an advanced machine learning model for de novo drug safety assessment by solving the multilayer drug-gene-adverse drug reaction (ADR) interaction network. For the first time, the drug safety was assessed in a broad landscape of 1,156 distinct ADRs. We also designed a parameter ToxicityScore to quantify the overall drug safety. Moreover, we determined association strength for every 3,807,631 gene-ADR interactions, which clues mechanistic exploration of ADRs. For convenience, we deployed the model as a web service ADRAlert-gene at http://www.bio-add.org/ADRAlert/. In summary, this study offers insights into prioritizing safe drug therapy. It helps reduce the attrition rate of new drug discovery by providing a reliable ADR profile in the early preclinical stage.","[-0.39926827  0.68702793  0.25638044 ...  0.29850692 -0.31591043
  0.21555959]",0.9074193,0.80933243,Other,machine learning model
29244814,Forecasting influenza-like illness dynamics for military populations using neural networks and social media,"Volkova S, Ayton E, Porterfield K, Corley CD.",PLoS One. 2017 Dec 15;12(12):e0188941. doi: 10.1371/journal.pone.0188941. eCollection 2017.,Volkova S,PLoS One,2017,2017/12/16,PMC5731746,,10.1371/journal.pone.0188941,"This work is the first to take advantage of recurrent neural networks to predict influenza-like illness (ILI) dynamics from various linguistic signals extracted from social media data. Unlike other approaches that rely on timeseries analysis of historical ILI data and the state-of-the-art machine learning models, we build and evaluate the predictive power of neural network architectures based on Long Short Term Memory (LSTMs) units capable of nowcasting (predicting in ""real-time"") and forecasting (predicting the future) ILI dynamics in the 2011 - 2014 influenza seasons. To build our models we integrate information people post in social media e.g., topics, embeddings, word ngrams, stylistic patterns, and communication behavior using hashtags and mentions. We then quantitatively evaluate the predictive power of different social media signals and contrast the performance of the-state-of-the-art regression models with neural networks using a diverse set of evaluation metrics. Finally, we combine ILI and social media signals to build a joint neural network model for ILI dynamics prediction. Unlike the majority of the existing work, we specifically focus on developing models for local rather than national ILI surveillance, specifically for military rather than general populations in 26 U.S. and six international locations., and analyze how model performance depends on the amount of social media data available per location. Our approach demonstrates several advantages: (a) Neural network architectures that rely on LSTM units trained on social media data yield the best performance compared to previously used regression models. (b) Previously under-explored language and communication behavior features are more predictive of ILI dynamics than stylistic and topic signals expressed in social media. (c) Neural network models learned exclusively from social media signals yield comparable or better performance to the models learned from ILI historical data, thus, signals from social media can be potentially used to accurately forecast ILI dynamics for the regions where ILI historical data is not available. (d) Neural network models learned from combined ILI and social media signals significantly outperform models that rely solely on ILI historical data, which adds to a great potential of alternative public sources for ILI dynamics prediction. (e) Location-specific models outperform previously used location-independent models e.g., U.S. only. (f) Prediction results significantly vary across geolocations depending on the amount of social media data available and ILI activity patterns. (g) Model performance improves with more tweets available per geo-location e.g., the error gets lower and the Pearson score gets higher for locations with more tweets.","Forecasting influenza-like illness dynamics for military populations using neural networks and social media This work is the first to take advantage of recurrent neural networks to predict influenza-like illness (ILI) dynamics from various linguistic signals extracted from social media data. Unlike other approaches that rely on timeseries analysis of historical ILI data and the state-of-the-art machine learning models, we build and evaluate the predictive power of neural network architectures based on Long Short Term Memory (LSTMs) units capable of nowcasting (predicting in ""real-time"") and forecasting (predicting the future) ILI dynamics in the 2011 - 2014 influenza seasons. To build our models we integrate information people post in social media e.g., topics, embeddings, word ngrams, stylistic patterns, and communication behavior using hashtags and mentions. We then quantitatively evaluate the predictive power of different social media signals and contrast the performance of the-state-of-the-art regression models with neural networks using a diverse set of evaluation metrics. Finally, we combine ILI and social media signals to build a joint neural network model for ILI dynamics prediction. Unlike the majority of the existing work, we specifically focus on developing models for local rather than national ILI surveillance, specifically for military rather than general populations in 26 U.S. and six international locations., and analyze how model performance depends on the amount of social media data available per location. Our approach demonstrates several advantages: (a) Neural network architectures that rely on LSTM units trained on social media data yield the best performance compared to previously used regression models. (b) Previously under-explored language and communication behavior features are more predictive of ILI dynamics than stylistic and topic signals expressed in social media. (c) Neural network models learned exclusively from social media signals yield comparable or better performance to the models learned from ILI historical data, thus, signals from social media can be potentially used to accurately forecast ILI dynamics for the regions where ILI historical data is not available. (d) Neural network models learned from combined ILI and social media signals significantly outperform models that rely solely on ILI historical data, which adds to a great potential of alternative public sources for ILI dynamics prediction. (e) Location-specific models outperform previously used location-independent models e.g., U.S. only. (f) Prediction results significantly vary across geolocations depending on the amount of social media data available and ILI activity patterns. (g) Model performance improves with more tweets available per geo-location e.g., the error gets lower and the Pearson score gets higher for locations with more tweets.","[-0.19798942  0.29351482  0.14129852 ...  1.0172335  -0.26920298
 -0.7432806 ]",0.905585,0.81612116,Both,"neural network, machine learning model, recurrent neural network, LSTM"
25916548,Mapping chemical structure-activity information of HAART-drug cocktails over complex networks of AIDS epidemiology and socioeconomic data of U.S. counties,"Herrera-Ibatá DM, Pazos A, Orbegozo-Medina RA, Romero-Durán FJ, González-Díaz H.",Biosystems. 2015 Jun;132-133:20-34. doi: 10.1016/j.biosystems.2015.04.007. Epub 2015 Apr 24.,Herrera-Ibatá DM,Biosystems,2015,2015/04/29,,,10.1016/j.biosystems.2015.04.007,"Using computational algorithms to design tailored drug cocktails for highly active antiretroviral therapy (HAART) on specific populations is a goal of major importance for both pharmaceutical industry and public health policy institutions. New combinations of compounds need to be predicted in order to design HAART cocktails. On the one hand, there are the biomolecular factors related to the drugs in the cocktail (experimental measure, chemical structure, drug target, assay organisms, etc.); on the other hand, there are the socioeconomic factors of the specific population (income inequalities, employment levels, fiscal pressure, education, migration, population structure, etc.) to study the relationship between the socioeconomic status and the disease. In this context, machine learning algorithms, able to seek models for problems with multi-source data, have to be used. In this work, the first artificial neural network (ANN) model is proposed for the prediction of HAART cocktails, to halt AIDS on epidemic networks of U.S. counties using information indices that codify both biomolecular and several socioeconomic factors. The data was obtained from at least three major sources. The first dataset included assays of anti-HIV chemical compounds released to ChEMBL. The second dataset is the AIDSVu database of Emory University. AIDSVu compiled AIDS prevalence for >2300 U.S. counties. The third data set included socioeconomic data from the U.S. Census Bureau. Three scales or levels were employed to group the counties according to the location or population structure codes: state, rural urban continuum code (RUCC) and urban influence code (UIC). An analysis of >130,000 pairs (network links) was performed, corresponding to AIDS prevalence in 2310 counties in U.S. vs. drug cocktails made up of combinations of ChEMBL results for 21,582 unique drugs, 9 viral or human protein targets, 4856 protocols, and 10 possible experimental measures. The best model found with the original data was a linear neural network (LNN) with AUROC>0.80 and accuracy, specificity, and sensitivity≈77% in training and external validation series. The change of the spatial and population structure scale (State, UIC, or RUCC codes) does not affect the quality of the model. Unbalance was detected in all the models found comparing positive/negative cases and linear/non-linear model accuracy ratios. Using synthetic minority over-sampling technique (SMOTE), data pre-processing and machine-learning algorithms implemented into the WEKA software, more balanced models were found. In particular, a multilayer perceptron (MLP) with AUROC=97.4% and precision, recall, and F-measure >90% was found.","Mapping chemical structure-activity information of HAART-drug cocktails over complex networks of AIDS epidemiology and socioeconomic data of U.S. counties Using computational algorithms to design tailored drug cocktails for highly active antiretroviral therapy (HAART) on specific populations is a goal of major importance for both pharmaceutical industry and public health policy institutions. New combinations of compounds need to be predicted in order to design HAART cocktails. On the one hand, there are the biomolecular factors related to the drugs in the cocktail (experimental measure, chemical structure, drug target, assay organisms, etc.); on the other hand, there are the socioeconomic factors of the specific population (income inequalities, employment levels, fiscal pressure, education, migration, population structure, etc.) to study the relationship between the socioeconomic status and the disease. In this context, machine learning algorithms, able to seek models for problems with multi-source data, have to be used. In this work, the first artificial neural network (ANN) model is proposed for the prediction of HAART cocktails, to halt AIDS on epidemic networks of U.S. counties using information indices that codify both biomolecular and several socioeconomic factors. The data was obtained from at least three major sources. The first dataset included assays of anti-HIV chemical compounds released to ChEMBL. The second dataset is the AIDSVu database of Emory University. AIDSVu compiled AIDS prevalence for >2300 U.S. counties. The third data set included socioeconomic data from the U.S. Census Bureau. Three scales or levels were employed to group the counties according to the location or population structure codes: state, rural urban continuum code (RUCC) and urban influence code (UIC). An analysis of >130,000 pairs (network links) was performed, corresponding to AIDS prevalence in 2310 counties in U.S. vs. drug cocktails made up of combinations of ChEMBL results for 21,582 unique drugs, 9 viral or human protein targets, 4856 protocols, and 10 possible experimental measures. The best model found with the original data was a linear neural network (LNN) with AUROC>0.80 and accuracy, specificity, and sensitivity≈77% in training and external validation series. The change of the spatial and population structure scale (State, UIC, or RUCC codes) does not affect the quality of the model. Unbalance was detected in all the models found comparing positive/negative cases and linear/non-linear model accuracy ratios. Using synthetic minority over-sampling technique (SMOTE), data pre-processing and machine-learning algorithms implemented into the WEKA software, more balanced models were found. In particular, a multilayer perceptron (MLP) with AUROC=97.4% and precision, recall, and F-measure >90% was found.","[-0.08766464  0.54829055 -0.02266652 ...  0.5909433   0.0151979
 -0.2403119 ]",0.9243229,0.8018922,Other,"neural network, artificial neural network, multilayer perceptron"
24898862,Seminal quality prediction using data mining methods,"Sahoo AJ, Kumar Y.",Technol Health Care. 2014;22(4):531-45. doi: 10.3233/THC-140816.,Sahoo AJ,Technol Health Care,2014,2014/06/06,,,10.3233/THC-140816,"BACKGROUND: Now-a-days, some new classes of diseases have come into existences which are known as lifestyle diseases. The main reasons behind these diseases are changes in the lifestyle of people such as alcohol drinking, smoking, food habits etc. After going through the various lifestyle diseases, it has been found that the fertility rates (sperm quantity) in men has considerably been decreasing in last two decades. Lifestyle factors as well as environmental factors are mainly responsible for the change in the semen quality.
OBJECTIVE: The objective of this paper is to identify the lifestyle and environmental features that affects the seminal quality and also fertility rate in man using data mining methods.
METHOD: The five artificial intelligence techniques such as Multilayer perceptron (MLP), Decision Tree (DT), Navie Bayes (Kernel), Support vector machine+Particle swarm optimization (SVM+PSO) and Support vector machine (SVM) have been applied on fertility dataset to evaluate the seminal quality and also to predict the person is either normal or having altered fertility rate. While the eight feature selection techniques such as support vector machine (SVM), neural network (NN), evolutionary logistic regression (LR), support vector machine plus particle swarm optimization (SVM+PSO), principle component analysis (PCA), chi-square test, correlation and T-test methods have been used to identify more relevant features which affect the seminal quality. These techniques are applied on fertility dataset which contains 100 instances with nine attribute with two classes.
RESULTS: The experimental result shows that SVM+PSO provides higher accuracy and area under curve (AUC) rate (94% & 0.932) among multi-layer perceptron (MLP) (92% & 0.728), Support Vector Machines (91% & 0.758), Navie Bayes (Kernel) (89% & 0.850) and Decision Tree (89% & 0.735) for some of the seminal parameters. This paper also focuses on the feature selection process i.e. how to select the features which are more important for prediction of fertility rate. In this paper, eight feature selection methods are applied on fertility dataset to find out a set of good features. The investigational results shows that childish diseases (0.079) and high fever features (0.057) has less impact on fertility rate while age (0.8685), season (0.843), surgical intervention (0.7683), alcohol consumption (0.5992), smoking habit (0.575), number of hours spent on setting (0.4366) and accident (0.5973) features have more impact. It is also observed that feature selection methods increase the accuracy of above mentioned techniques (multilayer perceptron 92%, support vector machine 91%, SVM+PSO 94%, Navie Bayes (Kernel) 89% and decision tree 89%) as compared to without feature selection methods (multilayer perceptron 86%, support vector machine 86%, SVM+PSO 85%, Navie Bayes (Kernel) 83% and decision tree 84%) which shows the applicability of feature selection methods in prediction.
CONCLUSION: This paper lightens the application of artificial techniques in medical domain. From this paper, it can be concluded that data mining methods can be used to predict a person with or without disease based on environmental and lifestyle parameters/features rather than undergoing various medical test. In this paper, five data mining techniques are used to predict the fertility rate and among which SVM+PSO provide more accurate results than support vector machine and decision tree.","Seminal quality prediction using data mining methods BACKGROUND: Now-a-days, some new classes of diseases have come into existences which are known as lifestyle diseases. The main reasons behind these diseases are changes in the lifestyle of people such as alcohol drinking, smoking, food habits etc. After going through the various lifestyle diseases, it has been found that the fertility rates (sperm quantity) in men has considerably been decreasing in last two decades. Lifestyle factors as well as environmental factors are mainly responsible for the change in the semen quality.
OBJECTIVE: The objective of this paper is to identify the lifestyle and environmental features that affects the seminal quality and also fertility rate in man using data mining methods.
METHOD: The five artificial intelligence techniques such as Multilayer perceptron (MLP), Decision Tree (DT), Navie Bayes (Kernel), Support vector machine+Particle swarm optimization (SVM+PSO) and Support vector machine (SVM) have been applied on fertility dataset to evaluate the seminal quality and also to predict the person is either normal or having altered fertility rate. While the eight feature selection techniques such as support vector machine (SVM), neural network (NN), evolutionary logistic regression (LR), support vector machine plus particle swarm optimization (SVM+PSO), principle component analysis (PCA), chi-square test, correlation and T-test methods have been used to identify more relevant features which affect the seminal quality. These techniques are applied on fertility dataset which contains 100 instances with nine attribute with two classes.
RESULTS: The experimental result shows that SVM+PSO provides higher accuracy and area under curve (AUC) rate (94% & 0.932) among multi-layer perceptron (MLP) (92% & 0.728), Support Vector Machines (91% & 0.758), Navie Bayes (Kernel) (89% & 0.850) and Decision Tree (89% & 0.735) for some of the seminal parameters. This paper also focuses on the feature selection process i.e. how to select the features which are more important for prediction of fertility rate. In this paper, eight feature selection methods are applied on fertility dataset to find out a set of good features. The investigational results shows that childish diseases (0.079) and high fever features (0.057) has less impact on fertility rate while age (0.8685), season (0.843), surgical intervention (0.7683), alcohol consumption (0.5992), smoking habit (0.575), number of hours spent on setting (0.4366) and accident (0.5973) features have more impact. It is also observed that feature selection methods increase the accuracy of above mentioned techniques (multilayer perceptron 92%, support vector machine 91%, SVM+PSO 94%, Navie Bayes (Kernel) 89% and decision tree 89%) as compared to without feature selection methods (multilayer perceptron 86%, support vector machine 86%, SVM+PSO 85%, Navie Bayes (Kernel) 83% and decision tree 84%) which shows the applicability of feature selection methods in prediction.
CONCLUSION: This paper lightens the application of artificial techniques in medical domain. From this paper, it can be concluded that data mining methods can be used to predict a person with or without disease based on environmental and lifestyle parameters/features rather than undergoing various medical test. In this paper, five data mining techniques are used to predict the fertility rate and among which SVM+PSO provide more accurate results than support vector machine and decision tree.","[ 0.12740731  0.46007982  0.24368913 ...  0.593819    0.06236493
 -0.34304574]",0.9216571,0.8384698,Other,"neural network, multilayer perceptron"
15126220,Dental data mining: potential pitfalls and practical issues,Gansky SA.,Adv Dent Res. 2003 Dec;17:109-14. doi: 10.1177/154407370301700125.,Gansky SA,Adv Dent Res,2003,2004/05/06,,,10.1177/154407370301700125,"Knowledge Discovery and Data Mining (KDD) have become popular buzzwords. But what exactly is data mining? What are its strengths and limitations? Classic regression, artificial neural network (ANN), and classification and regression tree (CART) models are common KDD tools. Some recent reports (e.g., Kattan et al., 1998) show that ANN and CART models can perform better than classic regression models: CART models excel at covariate interactions, while ANN models excel at nonlinear covariates. Model prediction performance is examined with the use of validation procedures and evaluating concordance, sensitivity, specificity, and likelihood ratio. To aid interpretation, various plots of predicted probabilities are utilized, such as lift charts, receiver operating characteristic curves, and cumulative captured-response plots. A dental caries study is used as an illustrative example. This paper compares the performance of logistic regression with KDD methods of CART and ANN in analyzing data from the Rochester caries study. With careful analysis, such as validation with sufficient sample size and the use of proper competitors, problems of naïve KDD analyses (Schwarzer et al., 2000) can be carefully avoided.","Dental data mining: potential pitfalls and practical issues Knowledge Discovery and Data Mining (KDD) have become popular buzzwords. But what exactly is data mining? What are its strengths and limitations? Classic regression, artificial neural network (ANN), and classification and regression tree (CART) models are common KDD tools. Some recent reports (e.g., Kattan et al., 1998) show that ANN and CART models can perform better than classic regression models: CART models excel at covariate interactions, while ANN models excel at nonlinear covariates. Model prediction performance is examined with the use of validation procedures and evaluating concordance, sensitivity, specificity, and likelihood ratio. To aid interpretation, various plots of predicted probabilities are utilized, such as lift charts, receiver operating characteristic curves, and cumulative captured-response plots. A dental caries study is used as an illustrative example. This paper compares the performance of logistic regression with KDD methods of CART and ANN in analyzing data from the Rochester caries study. With careful analysis, such as validation with sufficient sample size and the use of proper competitors, problems of naïve KDD analyses (Schwarzer et al., 2000) can be carefully avoided.","[ 0.17048097  0.17602211  0.02657964 ...  0.42957932 -0.08278977
 -0.7616926 ]",0.901561,0.81686825,Both,"neural network, artificial neural network"
14687512,[Study on the application of artificial neural network on diabetes mellitus/insulin-glucose tolerance classification],"Qian L, Shi LY, Cheng MJ.",Zhonghua Liu Xing Bing Xue Za Zhi. 2003 Nov;24(11):1052-6.,Qian L,Zhonghua Liu Xing Bing Xue Za Zhi,2003,2003/12/23,,,,"OBJECTIVE: To discuss the potential application of artificial neural network (ANN) on the epidemiological classification of disease.
METHODS: Learning vector quantification neural network (LVQNN) and discriminate analysis were applied to data from epidemiological survey in a mine in 1996.
RESULTS: The structure of LVQNN was 25-->13-->3. The total veracity rates was 96.98%, and 92.45% among the abnormal blood glucose individuals. Through stepwise discriminate analysis, the discriminate equations were established including 11 variables with a total veracity rate of 87.34%, but was 85.53% in the abnormal blood glucose individuals. Further analysis on 30 cases with missing values showed that the disagreement ratio of LVQ was 1/30, lower than that of discriminate analysis of 7/30.
CONCLUSIONS: Compared to the conventional statistics method, LVQ not only showed better prediction precision, but could treat data with missing values satisfactorily plus it had no limit to the type or distribution of relevant data, thus provided a new powerful method to epidemiologic prediction.","[Study on the application of artificial neural network on diabetes mellitus/insulin-glucose tolerance classification] OBJECTIVE: To discuss the potential application of artificial neural network (ANN) on the epidemiological classification of disease.
METHODS: Learning vector quantification neural network (LVQNN) and discriminate analysis were applied to data from epidemiological survey in a mine in 1996.
RESULTS: The structure of LVQNN was 25-->13-->3. The total veracity rates was 96.98%, and 92.45% among the abnormal blood glucose individuals. Through stepwise discriminate analysis, the discriminate equations were established including 11 variables with a total veracity rate of 87.34%, but was 85.53% in the abnormal blood glucose individuals. Further analysis on 30 cases with missing values showed that the disagreement ratio of LVQ was 1/30, lower than that of discriminate analysis of 7/30.
CONCLUSIONS: Compared to the conventional statistics method, LVQ not only showed better prediction precision, but could treat data with missing values satisfactorily plus it had no limit to the type or distribution of relevant data, thus provided a new powerful method to epidemiologic prediction.","[ 0.18884198  0.47963464  0.25775996 ...  0.39690724 -0.05835545
 -0.50479054]",0.912022,0.80025035,Other,"neural network, artificial neural network"
37824311,Lung Cancer Prediction Using Electronic Claims Records: A Transformer-Based Approach,"Chen HY, Wang HM, Lin CH, Yang R, Lee CC.",IEEE J Biomed Health Inform. 2023 Dec;27(12):6062-6073. doi: 10.1109/JBHI.2023.3324191. Epub 2023 Dec 5.,Chen HY,IEEE J Biomed Health Inform,2023,2023/10/12,,,10.1109/JBHI.2023.3324191,"Electronic claims records (ECRs) are large scale and longitudinal collections of individual's medical service seeking actions. Compared to in-hospital medical records (EMRs), ECRs are more standardized and cross-sites. Recently, there has been studies showing promising results on modeling claims data for a wide range of medical applications. However, few of them address the exclusion criteria on cohort selection to extract new incidence without prior signs and also often lack of emphasis on predicting cancer in early stages. In this work, we aim to design a lung cancer prediction framework using ECRs with rigorous exclusion design using state-of-the-art sequence-based transformer. Furthermore, this work presents one of the first results by applying disease prediction model to the entire population in Taiwan. The result shows over 2.1 predictive power, 5 average positive predictive value (PPV), and 0.668 area under curve (AUC) in all-stage lung cancer and around 2.0 predictive power, 1 average PPV and 0.645 AUC in early-stage in our dataset. Sub-cohort analysis could funnel high precision selective group into prioritized clinical examination. Onset analysis validates the effect of our exclusion criteria. This work presents comprehensive analyses on lung cancer prediction, and the proposed approach can serve as a state-of-the-art disease risk prediction framework on claims data.","Lung Cancer Prediction Using Electronic Claims Records: A Transformer-Based Approach Electronic claims records (ECRs) are large scale and longitudinal collections of individual's medical service seeking actions. Compared to in-hospital medical records (EMRs), ECRs are more standardized and cross-sites. Recently, there has been studies showing promising results on modeling claims data for a wide range of medical applications. However, few of them address the exclusion criteria on cohort selection to extract new incidence without prior signs and also often lack of emphasis on predicting cancer in early stages. In this work, we aim to design a lung cancer prediction framework using ECRs with rigorous exclusion design using state-of-the-art sequence-based transformer. Furthermore, this work presents one of the first results by applying disease prediction model to the entire population in Taiwan. The result shows over 2.1 predictive power, 5 average positive predictive value (PPV), and 0.668 area under curve (AUC) in all-stage lung cancer and around 2.0 predictive power, 1 average PPV and 0.645 AUC in early-stage in our dataset. Sub-cohort analysis could funnel high precision selective group into prioritized clinical examination. Onset analysis validates the effect of our exclusion criteria. This work presents comprehensive analyses on lung cancer prediction, and the proposed approach can serve as a state-of-the-art disease risk prediction framework on claims data.","[-0.3228335   1.0111424   0.312506   ...  0.62790287 -0.03398243
 -0.7113375 ]",0.90499926,0.8085392,Computer Vision,transformer
35308991,Detecting Fine-Grained Emotions on Social Media during Major Disease Outbreaks: Health and Well-being before and during the COVID-19 Pandemic,"Aduragba OT, Yu J, Cristea AI, Shi L.",AMIA Annu Symp Proc. 2022 Feb 21;2021:187-196. eCollection 2021.,Aduragba OT,AMIA Annu Symp Proc,2022,2022/03/21,PMC8861702,,,"The COVID-19 pandemic has affected the whole world in various ways. One type of impact is that communication, work, interaction, a great part of our lives has moved online on various platforms, with some of the most popular being the social media ones. Another, arguably less visible impact, is the emotional impact. Detecting and understanding emotions is important, to better discern the emotional health and well-being of the global population. Thus, in this work, we use a social media platform (Twitter) to analyse emotions in detail. Our contribution is twofold: (1) we propose EmoBERT, a new emotion-based variant of the BERT transformer model, able to learn emotion representations and outperform the state-of-the-art; (2) we provide a fine-grained analysis of the pandemic's effect in a major location, London, comparing specific emotions (annoyed, anxious, empathetic, sad) before and during the epidemic.","Detecting Fine-Grained Emotions on Social Media during Major Disease Outbreaks: Health and Well-being before and during the COVID-19 Pandemic The COVID-19 pandemic has affected the whole world in various ways. One type of impact is that communication, work, interaction, a great part of our lives has moved online on various platforms, with some of the most popular being the social media ones. Another, arguably less visible impact, is the emotional impact. Detecting and understanding emotions is important, to better discern the emotional health and well-being of the global population. Thus, in this work, we use a social media platform (Twitter) to analyse emotions in detail. Our contribution is twofold: (1) we propose EmoBERT, a new emotion-based variant of the BERT transformer model, able to learn emotion representations and outperform the state-of-the-art; (2) we provide a fine-grained analysis of the pandemic's effect in a major location, London, comparing specific emotions (annoyed, anxious, empathetic, sad) before and during the epidemic.","[ 0.12908018  0.32659265  0.33444336 ...  0.7584467  -0.19323707
 -0.16782822]",0.9048269,0.80150443,Both,transformer
